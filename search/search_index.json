{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AIGVE!","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"contact/","title":"Contact us","text":""},{"location":"contact/#for-contributing","title":"For contributing","text":"<p>Contributing to the project, including report/fix bugs, add new features, improve documentation, and send feedbacks, can all be done via the project github issue page at https://github.com/ShaneXiangH/AIGVE_Tool/issues.</p>"},{"location":"contact/#for-licenses","title":"For Licenses","text":"<p><code>AIGVE</code> is an open-source project. For more information about the licenses, please refer to the license page.</p>"},{"location":"contact/contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contact/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at the project gitHub issue page.</p> <p>If you are reporting a bug, please also include information about:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contact/contributing/#fix-bugs-and-add-new-features","title":"Fix Bugs and Add New Features","text":"<p>You may look through the project gitHub issue page for potential problems to help with. </p> <ul> <li>Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</li> <li>Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</li> </ul>"},{"location":"contact/contributing/#write-documentation","title":"Write Documentation","text":"<p>We provide the preliminary version of the documentation at the AIGVE website. <code>aigve</code> could always use more documentation, whether as part of the official <code>aigve</code> docs,  in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contact/contributing/#send-feedbacks","title":"Send Feedbacks","text":"<p>The best way to send feedback is to file an issue at the project gitHub issue page. We will check and solve the reported issues periodically.</p> <p>If you are proposing a feature to be potentially added into <code>aigve</code>:</p> <ul> <li>Please explain in detail how it would work, and how it would be intergrated into the current framework.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome.</li> </ul>"},{"location":"contact/license/","title":"Licenses","text":"<p>Copyright \u00a9 2025 IFM Lab. All rights reserved.</p> <ul> <li><code>aigve</code> source code is published under the terms of the MIT License. </li> <li><code>aigve</code> documentation and the RPN papers are licensed under a Creative Commons Attribution-Share Alike 4.0 Unported License (CC BY-SA 4.0). </li> </ul>"},{"location":"documentations/","title":"AIGVE Documentation","text":"<p>The documentations of the <code>AIGVE</code> library will be organized as follows.</p> <ul> <li> <code>aigve</code> library for assessing AI-generated video quality</li> </ul> <ul> <li> <code>aigve.config</code> for parameter configuration and management</li> </ul> <ul> <li> <code>aigve.core</code> for video evaluation process design</li> </ul> <ul> <li> <code>aigve.datasets</code> for dataset loading design</li> </ul> <ul> <li> <code>aigve.metrics</code> for video evaluation metrics design and building</li> </ul> <ul> <li> <code>aigve.utils</code> for utility function definition</li> </ul>"},{"location":"documentations/aigve/","title":"aigve","text":"<p>This aigve library provides a comprehensive and structured evaluation framework  for assessing AI-generated video quality. It integrates multiple evaluation metrics,  covering diverse aspects of video evaluation, including neural-network-based assessment,  distribution comparison, vision-language alignment, and multi-faceted analysis.</p>"},{"location":"documentations/aigve/#aigve.CLIPSimScore","title":"<code>CLIPSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                      Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n        processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                Defaults to ``openai/clip-vit-base-patch32``.\n        model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempDataset","title":"<code>CLIPTempDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass CLIPTempDataset(Dataset):\n    def __init__(self, processor_name, prompt_dir, video_dir):\n        super(CLIPTempDataset, self).__init__()\n        self.prompt_dir = prompt_dir\n        self.video_dir = video_dir\n        self.processor_name = processor_name\n\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.video_names = self._read_videoname()\n\n    def _read_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        video_name_list = []\n        for item in read_data[\"datset_list\"]:\n            video_name = item['video_path_pd'].strip()\n            video_name_list.append(video_name)\n\n        return video_name_list\n\n    def __len__(self):\n        return len(self.video_names)-1\n\n    def __getitem__(self, index):\n        '''return video frame pairs\n        '''\n        video_name = self.video_names[index]\n        video_path = self.video_dir + video_name\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n            frames.append(resized_frame)\n\n        input_frame_tensor = self.processor(\n            images=frames,\n            padding=True,\n            truncation=True,\n            max_length=77,\n            return_tensors=\"pt\",\n        )['pixel_values']\n\n        return input_frame_tensor\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>return video frame pairs</p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>def __getitem__(self, index):\n    '''return video frame pairs\n    '''\n    video_name = self.video_names[index]\n    video_path = self.video_dir + video_name\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n        frames.append(resized_frame)\n\n    input_frame_tensor = self.processor(\n        images=frames,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    )['pixel_values']\n\n    return input_frame_tensor\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore","title":"<code>CLIPTempScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore","title":"<code>DSGScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>vqa_model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> <code>'InstructBLIP'</code> <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n        vqa_model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n        verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]]): The current evaluated frame.\n\n        Returns:\n            Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n                - 'frame_index' (int): The index of the evaluated frame.\n                - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n                - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n                - 'qid2question' (dict): Mapping of question IDs to actual questions.\n                - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n                - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n                - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n                - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n                - 'average_score_without_dependency' (float): Average score before dependency filtering.\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[int, dict, float]]</code> <p>Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys: - 'frame_index' (int): The index of the evaluated frame. - 'qid2tuple' (dict): Mapping of question IDs to tuples. - 'qid2dependency' (dict): Mapping of question IDs to dependencies. - 'qid2question' (dict): Mapping of question IDs to actual questions. - 'qid2answer' (dict): Mapping of question IDs to predicted answers. - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering. - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering. - 'average_score_with_dependency' (float): Average score considering dependency filtering. - 'average_score_without_dependency' (float): Average score before dependency filtering.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n        qid_list (List[str]): The list of DSG parse question generation results.\n        frame_index (int): The index number of the currently evaluated frame.\n        frame (List[List[float]]): The current evaluated frame.\n\n    Returns:\n        Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n            - 'frame_index' (int): The index of the evaluated frame.\n            - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n            - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n            - 'qid2question' (dict): Mapping of question IDs to actual questions.\n            - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n            - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n            - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n            - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n            - 'average_score_without_dependency' (float): Average score before dependency filtering.\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore","title":"<code>FIDScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>@METRICS.register_module()\nclass FIDScore(BaseMetric):\n\n    def __init__(self, \n                 model_name: str = 'inception_v3', \n                 input_shape: tuple = (299, 299, 3), \n                 is_gpu: str = True):\n        super(FIDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.model_name = model_name\n        self.input_shape = input_shape\n        if self.model_name == \"inception_v3\":\n            self.model = models.inception_v3(pretrained=True, transform_input=False)\n            self.model.fc = nn.Identity()  # Remove classification head\n            self.model.eval().to(self.device)\n        else:\n            raise ValueError(f\"Model '{self.model_name}' is not supported for FID computation.\")\n\n        # Define preprocessing for InceptionV3\n        self.transform = transforms.Compose([\n            transforms.Resize((self.input_shape[0], self.input_shape[1])),  # InceptionV3 input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        video_tensor = self.transform(video_tensor / 255.0)\n        return video_tensor\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate activation statistics (mean and covariance) from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple of mean and covariance matrix.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Calculate FID score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FID score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n        mu2, sigma2 = self.calculate_statistics(fake)\n\n        # Compute FID score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and compute FID.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n                fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FID_Score\": fid_score\n                })\n                print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the final FID score.\"\"\"\n        scores = np.array([res[\"FID_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FID mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FID_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FID mean score saved to {json_file_path}\")\n\n        return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore.calculate_fid","title":"<code>calculate_fid(real, fake)</code>","text":"<p>Calculate FID score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Calculate FID score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FID score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n    mu2, sigma2 = self.calculate_statistics(fake)\n\n    # Compute FID score\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Calculate activation statistics (mean and covariance) from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple of mean and covariance matrix.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate activation statistics (mean and covariance) from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple of mean and covariance matrix.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n    mu = features.mean(axis=0)\n    sigma = np.cov(features, rowvar=False)\n    return mu, sigma\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the final FID score.\"\"\"\n    scores = np.array([res[\"FID_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FID mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FID_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FID mean score saved to {json_file_path}\")\n\n    return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    video_tensor = self.transform(video_tensor / 255.0)\n    return video_tensor\n</code></pre>"},{"location":"documentations/aigve/#aigve.FIDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and compute FID.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and compute FID.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n            fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FID_Score\": fid_score\n            })\n            print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore","title":"<code>FVDScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Fr\u00e9chet Video Distance (FVD) computation using I3D model. Users can first download the pretrained I3D model from:  https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth Then put in the folder:  AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to pre-trained I3D model.</p> required <code>feature_layer</code> <code>int</code> <p>Layer to extract features from. Default is -2 (penultimate layer).</p> <code>-2</code> <code>is_gpu</code> <code>bool</code> <p>Whether to use GPU. Default is True.</p> <code>True</code> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>@METRICS.register_module()\nclass FVDScore(BaseMetric):\n    \"\"\"\n    Fr\u00e9chet Video Distance (FVD) computation using I3D model.\n    Users can first download the pretrained I3D model from: \n    https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth\n    Then put in the folder: \n    AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/\n\n    Args:\n        model_path (str): Path to pre-trained I3D model.\n        feature_layer (int): Layer to extract features from. Default is -2 (penultimate layer).\n        is_gpu (bool): Whether to use GPU. Default is True.\n    \"\"\"\n    def __init__(self, \n                 model_path: str, \n                 feature_layer: int = -2, \n                 is_gpu: bool = True):\n        super(FVDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu and torch.cuda.is_available() else \"cpu\")\n        self.model = self.load_i3d_model(model_path, feature_layer)\n        self.model.eval()\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),  # I3D input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n        \"\"\"\n        Load a pre-trained I3D model and modify it to extract features.\n\n        Args:\n            model_path (str): Path to the I3D model checkpoint.\n            feature_layer (int): The layer index from which to extract features.\n\n        Returns:\n            torch.nn.Module: I3D feature extraction model.\n        \"\"\"\n        model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n        model.fc = nn.Identity()  # Remove classification head\n\n        if os.path.exists(model_path):\n            model.load_state_dict(torch.load(model_path, map_location=self.device))\n        else:\n            print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n        return model\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        return self.transform(video_tensor / 255.0)\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Extract activation statistics from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        self.model.to(self.device)\n        # Permute to match I3D input format [B, C, T, H, W]\n        video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()\n\n        # print('features: ', features.shape)\n        mu = features.mean(axis=0)\n        # Ensure at least 2 samples to compute covariance\n        if features.shape[0] &gt; 1:\n            sigma = np.cov(features, rowvar=False)\n        else:\n            sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n        return mu, sigma\n\n    def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Compute FVD score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FVD score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n        mu2, sigma2 = self.calculate_statistics(fake)\n        # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n        # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n        # Ensure sigma matrices are at least 2D\n        if sigma1.ndim &lt; 2:\n            sigma1 = np.expand_dims(sigma1, axis=0)\n        if sigma2.ndim &lt; 2:\n            sigma2 = np.expand_dims(sigma2, axis=0)\n\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process a batch of videos and compute FVD.\n\n        Args:\n            data_batch (dict): Not used here.\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n\n                fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FVD_Score\": fvd_score\n                })\n                print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the final FVD score.\n\n        Args:\n            results (list): List of FVD scores for each batch.\n\n        Returns:\n            Dict[str, float]: Dictionary containing mean FVD score.\n        \"\"\"\n        scores = np.array([res[\"FVD_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FVD mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FVD_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FVD mean score saved to {json_file_path}\")\n\n        return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.calculate_fvd","title":"<code>calculate_fvd(real, fake)</code>","text":"<p>Compute FVD score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Compute FVD score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FVD score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n    mu2, sigma2 = self.calculate_statistics(fake)\n    # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n    # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n    # Ensure sigma matrices are at least 2D\n    if sigma1.ndim &lt; 2:\n        sigma1 = np.expand_dims(sigma1, axis=0)\n    if sigma2.ndim &lt; 2:\n        sigma2 = np.expand_dims(sigma2, axis=0)\n\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Extract activation statistics from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract activation statistics from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    self.model.to(self.device)\n    # Permute to match I3D input format [B, C, T, H, W]\n    video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()\n\n    # print('features: ', features.shape)\n    mu = features.mean(axis=0)\n    # Ensure at least 2 samples to compute covariance\n    if features.shape[0] &gt; 1:\n        sigma = np.cov(features, rowvar=False)\n    else:\n        sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n    return mu, sigma\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FVD score.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>List of FVD scores for each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing mean FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the final FVD score.\n\n    Args:\n        results (list): List of FVD scores for each batch.\n\n    Returns:\n        Dict[str, float]: Dictionary containing mean FVD score.\n    \"\"\"\n    scores = np.array([res[\"FVD_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FVD mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FVD_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FVD mean score saved to {json_file_path}\")\n\n    return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.load_i3d_model","title":"<code>load_i3d_model(model_path, feature_layer)</code>","text":"<p>Load a pre-trained I3D model and modify it to extract features.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the I3D model checkpoint.</p> required <code>feature_layer</code> <code>int</code> <p>The layer index from which to extract features.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: I3D feature extraction model.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n    \"\"\"\n    Load a pre-trained I3D model and modify it to extract features.\n\n    Args:\n        model_path (str): Path to the I3D model checkpoint.\n        feature_layer (int): The layer index from which to extract features.\n\n    Returns:\n        torch.nn.Module: I3D feature extraction model.\n    \"\"\"\n    model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n    model.fc = nn.Identity()  # Remove classification head\n\n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\n    else:\n        print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n    return model\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    return self.transform(video_tensor / 255.0)\n</code></pre>"},{"location":"documentations/aigve/#aigve.FVDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of videos and compute FVD.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>Not used here.</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process a batch of videos and compute FVD.\n\n    Args:\n        data_batch (dict): Not used here.\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n\n            fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FVD_Score\": fvd_score\n            })\n            print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.FidDataset","title":"<code>FidDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for Fr\u00e9chet Inception Distance (FID) evaluation.</p> <p>For each sample, this dataset:     - Loads both the ground-truth (real) and generated (predicted) videos.     - Converts each video into a tensor of shape [T, C, H, W] using OpenCV.     - Optionally pads or truncates videos to a fixed number of frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_dir</code> <code>str</code> <p>Directory containing video files.</p> required <code>prompt_dir</code> <code>str</code> <p>Path to JSON file that lists ground-truth and generated video filenames.</p> required <code>max_len</code> <code>int</code> <p>Maximum number of frames to load per video. Default: 500.</p> <code>500</code> <code>if_pad</code> <code>bool</code> <p>Whether to pad videos to exactly <code>max_len</code> frames. If False, videos can have variable lengths.</p> <code>False</code> Source code in <code>aigve/datasets/fid_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass FidDataset(Dataset):\n    \"\"\"\n    Dataset for Fr\u00e9chet Inception Distance (FID) evaluation.\n\n    For each sample, this dataset:\n        - Loads both the ground-truth (real) and generated (predicted) videos.\n        - Converts each video into a tensor of shape [T, C, H, W] using OpenCV.\n        - Optionally pads or truncates videos to a fixed number of frames.\n\n    Args:\n        video_dir (str): Directory containing video files.\n        prompt_dir (str): Path to JSON file that lists ground-truth and generated video filenames.\n        max_len (int): Maximum number of frames to load per video. Default: 500.\n        if_pad (bool): Whether to pad videos to exactly `max_len` frames. If False, videos can have variable lengths.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, max_len=500, if_pad=False):\n        super(FidDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.max_len = max_len\n        self.if_pad = if_pad\n\n        self.gt_video_names, self.gen_video_names = self._read_video_names()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n            gt_video_names = [item['video_path_gt'].strip() for item in read_data[\"data_list\"]]\n            gen_video_names = [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n        return gt_video_names, gen_video_names\n\n    def _load_video_tensor(self, video_path: str) -&gt; torch.Tensor:\n        \"\"\"Load a video and return its tensor of shape [T, C, H, W].\"\"\"\n        assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n        cap = cv2.VideoCapture(video_path)\n        input_frames = []\n        frame_count = 0\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        if len(input_frames) == 0:\n            raise RuntimeError(f\"No valid frames found in {video_path}\")\n\n        if self.if_pad:\n            num_frames = len(input_frames)\n            if num_frames &lt; self.max_len:\n                pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n                video_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            else:\n                video_tensor = torch.stack(input_frames[:self.max_len])\n        else:\n            video_tensor = torch.stack(input_frames)\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        return video_tensor.permute(0, 3, 1, 2)\n\n    def __len__(self):\n        return len(self.gt_video_names)\n\n    def __getitem__(self, index) -&gt; tuple[torch.Tensor, torch.Tensor, str, str]:\n        \"\"\"\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, str, str]: \n                - Ground-truth (Real) video tensor of shape [T, C, H, W].\n                - Generated video tensor of shape [T, C, H, W].\n                - Ground-truth video name.\n                - Generated video name.\n        \"\"\"\n        gt_video_name = self.gt_video_names[index]\n        gt_video_path = os.path.join(self.video_dir, gt_video_name)\n        gen_video_name = self.gen_video_names[index]\n        gen_video_path = os.path.join(self.video_dir, gen_video_name) \n\n        gt_video_tensor = self._load_video_tensor(gt_video_path)\n        gen_video_tensor = self._load_video_tensor(gen_video_path)\n\n        return gt_video_tensor, gen_video_tensor, gt_video_name, gen_video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.FidDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, str, str]</code> <p>Tuple[torch.Tensor, torch.Tensor, str, str]:  - Ground-truth (Real) video tensor of shape [T, C, H, W]. - Generated video tensor of shape [T, C, H, W]. - Ground-truth video name. - Generated video name.</p> Source code in <code>aigve/datasets/fid_dataset.py</code> <pre><code>def __getitem__(self, index) -&gt; tuple[torch.Tensor, torch.Tensor, str, str]:\n    \"\"\"\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, str, str]: \n            - Ground-truth (Real) video tensor of shape [T, C, H, W].\n            - Generated video tensor of shape [T, C, H, W].\n            - Ground-truth video name.\n            - Generated video name.\n    \"\"\"\n    gt_video_name = self.gt_video_names[index]\n    gt_video_path = os.path.join(self.video_dir, gt_video_name)\n    gen_video_name = self.gen_video_names[index]\n    gen_video_path = os.path.join(self.video_dir, gen_video_name) \n\n    gt_video_tensor = self._load_video_tensor(gt_video_path)\n    gen_video_tensor = self._load_video_tensor(gen_video_path)\n\n    return gt_video_tensor, gen_video_tensor, gt_video_name, gen_video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQADataset","title":"<code>GSTVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass GSTVQADataset(Dataset):\n    \"\"\"Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.\"\"\"\n\n    def __init__(self, video_dir, prompt_dir, model_name='vgg16', max_len=500):\n        super(GSTVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.model_name = model_name\n        self.max_len = max_len\n        self.feature_extractor = FeatureExtractor(model_name=model_name)\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, index) -&gt; tuple[torch.Tensor, int, str]:\n        \"\"\"\n        Returns a tuple of:\n            deep_features (torch.Tensor): Shape [max_len, 2944]\n                Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n                Padded to self.max_len if the number of frames is less.\n            num_frames (int): The number of frames in the video.\n            video_name (str): The file name for the video.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n        input_frames = []\n\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            # frame = cv2.resize(frame, self.frame_size)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        # Pad or truncate frames to max_len\n        num_frames = len(input_frames)\n        # print('num_frames: ', num_frames)\n        if num_frames &lt; 30:\n            pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n        elif num_frames &lt; self.max_len:\n            pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        else:\n            input_frames_tensor = torch.stack(input_frames[:self.max_len])\n        # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n        # Extract features using the chosen model (VGG16 or ResNet)\n        with torch.no_grad():\n            mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n        # Concatenate to match GSTVQA expected 2944-dim features\n        deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n        # Ensure output shape [max_len, 2944] (pad if needed)\n        if deep_features.shape[0] &lt; self.max_len:\n            pad_size = self.max_len - deep_features.shape[0]\n            padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n            deep_features = torch.cat((deep_features, padding), dim=0)\n\n        return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Returns a tuple of <p>deep_features (torch.Tensor): Shape [max_len, 2944]     Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).     Padded to self.max_len if the number of frames is less. num_frames (int): The number of frames in the video. video_name (str): The file name for the video.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>def __getitem__(self, index) -&gt; tuple[torch.Tensor, int, str]:\n    \"\"\"\n    Returns a tuple of:\n        deep_features (torch.Tensor): Shape [max_len, 2944]\n            Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n            Padded to self.max_len if the number of frames is less.\n        num_frames (int): The number of frames in the video.\n        video_name (str): The file name for the video.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n    input_frames = []\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n\n    while cap.isOpened() and frame_count &lt; self.max_len:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # frame = cv2.resize(frame, self.frame_size)\n        input_frames.append(torch.tensor(frame).float())\n        frame_count += 1\n\n    cap.release()\n\n    # Pad or truncate frames to max_len\n    num_frames = len(input_frames)\n    # print('num_frames: ', num_frames)\n    if num_frames &lt; 30:\n        pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n    elif num_frames &lt; self.max_len:\n        pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n    else:\n        input_frames_tensor = torch.stack(input_frames[:self.max_len])\n    # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n    # Convert from [T, H, W, C] to [T, C, H, W]\n    input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n    # Extract features using the chosen model (VGG16 or ResNet)\n    with torch.no_grad():\n        mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n    # Concatenate to match GSTVQA expected 2944-dim features\n    deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n    # Ensure output shape [max_len, 2944] (pad if needed)\n    if deep_features.shape[0] &lt; self.max_len:\n        pad_size = self.max_len - deep_features.shape[0]\n        padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n        deep_features = torch.cat((deep_features, padding), dim=0)\n\n    return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa","title":"<code>GstVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of each three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of each three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of each three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus","title":"<code>LightVQAPlus</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                A list containing five tuples:\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         A list containing five tuples:         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.         The len of each tuples are the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            A list containing five tuples:\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset","title":"<code>LightVQAPlusDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for LightVQA+. Extracts:     - spatial_features (torch.Tensor): Extracted key frames.     - temporal_features (torch.Tensor): SlowFast motion features.     - BNS_features (torch.Tensor): Brightness &amp; Noise features.     - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass LightVQAPlusDataset(Dataset):\n    \"\"\"\n    Dataset for LightVQA+.\n    Extracts:\n        - spatial_features (torch.Tensor): Extracted key frames.\n        - temporal_features (torch.Tensor): SlowFast motion features.\n        - BNS_features (torch.Tensor): Brightness &amp; Noise features.\n        - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(LightVQAPlusDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.video_names = self._read_video_names()\n\n        # Load CLIP model for BNS and BC features\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n        self.preprocess = transforms.Normalize(\n            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n        )\n        self.to_tensor = transforms.ToTensor()\n\n        # CLIP text prompts\n        self.text_B = clip.tokenize([  # brightness (B)\n            \"an underexposed photo\", \"a slightly underexposed photo\",\n            \"a well-exposed photo\", \"a slightly overexposed photo\", \"an overexposed photo\"\n        ])\n\n        self.text_N = clip.tokenize([ # noise (N)\n            \"a photo with no noise\", \"a photo with little noise\",\n            \"a photo with considerable noise\", \"a photo with serious noise\", \"a photo with extreme noise\"\n        ])\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        # original_path = os.path.join(self.submodel_path, \"Light-VQA-plus\")\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        # if os.path.exists(original_path) and not os.path.exists(lightvqa_path):\n        #     os.rename(original_path, lightvqa_path)\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n        # print(sys.path)\n\n        # Load SlowFast model\n        slowfast, _ = lazy_import()\n        self.slowfast_model = slowfast()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n        return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def extract_key_frames(self, video_path):\n        \"\"\"\n        Extracts 8 evenly spaced key frames across the entire video duration.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path).split('.')[0]\n\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        if video_length &gt;= 8:\n            # Select 8 unique frame indices evenly spaced across the entire video\n            frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n        else:\n            # Select all available frames and repeat the last one to reach 8\n            frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n        spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n        transform = transforms.Compose([\n            transforms.Resize([672, 1120]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        last_valid_frame = None\n        for idx, frame_idx in enumerate(frame_indices):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                spatial_features[idx] = transform(frame)\n                last_valid_frame = spatial_features[idx]\n            elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n                spatial_features[idx] = last_valid_frame\n\n        cap.release()\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n        return spatial_features\n\n    def get_global_sf(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts global brightness &amp; noise features across full video.\n\n        Args:\n            video_path (str): Path to video file.\n\n        Returns:\n            torch.Tensor: Extracted global features (Shape: [8, 150]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        # print('video_length: ', video_length)  # 16\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        length = len(frames)\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        while now + interval - 1 &lt; length:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n\n            # Step 1: Convert to tensor batch\n            images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n            # Step 2: Unfold into patches (Strictly following GET_SF)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n            images = self.preprocess(images)  # Normalize for CLIP\n            # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n            # Step 3: Extract features using CLIP\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N)\n                logits_B, _ = self.clip_model(images, self.text_B)\n\n            tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n            tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n            # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n            # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n            now += interval\n\n        # Handle remaining frames\n        if length &gt; now:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n\n            images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n            images = self.preprocess(images)\n\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n                logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n                # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n                # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n            tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n            # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 150]\n        # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n        # Step 4: Aggregate into 8 time slots\n        chunk_size = length // 8\n        final_res = [\n            torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n            for i in range(8)\n        ]\n\n        return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n\n    def extract_bns_features(self, video_path):\n        \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n        Local Feature Extraction (res1) \u2192 Uses 8 key frames\n        Global Feature Extraction (res2) \u2192 Uses all frames\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n                Shape [8, 3, 672, 1120] containing 8 key frames.\n            final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n        \"\"\"\n        # Local Feature Extraction Step 1: Extract key frames\n        spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n        # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n        images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n        # print(images.device)\n        # print(self.text_N.device)\n\n        # Step 3: Pass through CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n        # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n        res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n        # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n        res1 = torch.cat((res_N, res_B), dim=1)\n        # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n        # Global Feature Extraction (GET_SF Equivalent)\n        res2 = self.get_global_sf(video_path)\n        # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n        # Split &amp; Combine Features\n        Nl, Bl = torch.split(res1, 75, dim=1)\n        Ng, Bg = torch.split(res2, 75, dim=1)\n        final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n        # print('final_res: ', final_res.shape)\n\n        return spatial_features, final_res  # Shape: [8, 300]\n\n    def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n        Returns:\n            torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        length = len(frames)\n\n        # Step 1: Extract CLIP Features at Fixed Intervals\n        while now + interval - 1 &lt; length:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n            now += interval\n\n        # Handle Remaining Frames\n        if length &gt; now:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 5]\n        # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n        # Step 2: Multi-Scale Variance Computation: downsample frames steps\n        # smaller step: Captures fast, fine-grained changes.\n        # larger step:  Captures slow, long-term trends.\n        final_res = []\n        for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n            chunk_number = 8 // step\n            chunk_size = length // chunk_number\n            chunks = []\n            for i in range(chunk_number):\n                if i &lt; chunk_number - 1:\n                    chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n                else:\n                    chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n                tmp = []\n                for j in range(step):\n                    temp = chunk[j::step, :]  \n                    tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n                chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n            final_res.append(chunks) # final final_res len: 4\n\n        # Step 3: Aggregate Multi-Scale Features\n        temp = []\n        for i in range(8):  # Aggregate temporal information across 8 time slots\n            temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                                + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                                + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                                + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                                , dim=0))\n\n        final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n        # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n        return final_res\n\n    def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),  # Match SlowFast input size\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n        ])\n\n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                frames.append(transform(frame))  # Resize &amp; normalize\n        cap.release()\n\n        if len(frames) &lt; 8:\n            raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n        video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n        # Prepare for SlowFast input\n        video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n        video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n        # Pack pathways for SlowFast model\n        _, pack_pathway_output = lazy_import()\n        inputs = pack_pathway_output(video_tensor, device='cpu')\n        # print('inputs len: ', len(inputs))\n        # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n        # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n        # Extract features using SlowFast\n        with torch.no_grad():\n            slow_feature, fast_feature = self.slowfast_model(inputs)\n\n        # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n        # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n        # Concatenate slow and fast features\n        features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n        # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n        return features\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n            bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n            temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n            video_name (str): Video filename.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, bns_features = self.extract_bns_features(video_path)\n        bc_features = self.extract_bc_features(video_path)\n        temporal_features = self.extract_temporal_features(video_path)\n\n        return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Spatial features. Shape: [8, 3, 672, 1120].</p> <code>bns_features</code> <code>Tensor</code> <p>Brightness &amp; Noise features. Shape: [8, 300].</p> <code>bc_features (torch.Tensor</code> <p>Temporal brightness contrast features. Shape: [8, final_dim].)</p> <code>temporal_features</code> <code>Tensor</code> <p>SlowFast motion features. Shape: [1, feature_dim(2304)]</p> <code>video_name</code> <code>str</code> <p>Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n        bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n        (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n        temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n        video_name (str): Video filename.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, bns_features = self.extract_bns_features(video_path)\n    bc_features = self.extract_bc_features(video_path)\n    temporal_features = self.extract_temporal_features(video_path)\n\n    return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_bc_features","title":"<code>extract_bc_features(video_path)</code>","text":"<p>Extracts Brightness Consistency features using CLIP-based temporal processing.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n    Returns:\n        torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    length = len(frames)\n\n    # Step 1: Extract CLIP Features at Fixed Intervals\n    while now + interval - 1 &lt; length:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n        now += interval\n\n    # Handle Remaining Frames\n    if length &gt; now:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 5]\n    # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n    # Step 2: Multi-Scale Variance Computation: downsample frames steps\n    # smaller step: Captures fast, fine-grained changes.\n    # larger step:  Captures slow, long-term trends.\n    final_res = []\n    for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n        chunk_number = 8 // step\n        chunk_size = length // chunk_number\n        chunks = []\n        for i in range(chunk_number):\n            if i &lt; chunk_number - 1:\n                chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n            else:\n                chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n            tmp = []\n            for j in range(step):\n                temp = chunk[j::step, :]  \n                tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n            chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n        final_res.append(chunks) # final final_res len: 4\n\n    # Step 3: Aggregate Multi-Scale Features\n    temp = []\n    for i in range(8):  # Aggregate temporal information across 8 time slots\n        temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                            + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                            + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                            + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                            , dim=0))\n\n    final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n    # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n    return final_res\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_bns_features","title":"<code>extract_bns_features(video_path)</code>","text":"<p>Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP. Local Feature Extraction (res1) \u2192 Uses 8 key frames Global Feature Extraction (res2) \u2192 Uses all frames</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Extracted 8 evenly spaced key frames across the entire video duration. Shape [8, 3, 672, 1120] containing 8 key frames.</p> <code>final_res</code> <code>Tensor</code> <p>Extracted BNS feature (Shape: [8, 300]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bns_features(self, video_path):\n    \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n    Local Feature Extraction (res1) \u2192 Uses 8 key frames\n    Global Feature Extraction (res2) \u2192 Uses all frames\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n            Shape [8, 3, 672, 1120] containing 8 key frames.\n        final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n    \"\"\"\n    # Local Feature Extraction Step 1: Extract key frames\n    spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n    # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n    images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n    images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n    images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n    images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n    images = self.preprocess(images)  # Normalize for CLIP\n    # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n    # print(images.device)\n    # print(self.text_N.device)\n\n    # Step 3: Pass through CLIP\n    with torch.no_grad():\n        logits_N, _ = self.clip_model(images, self.text_N)\n        logits_B, _ = self.clip_model(images, self.text_B)\n\n    res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n    # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n    res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n    # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n    res1 = torch.cat((res_N, res_B), dim=1)\n    # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n    # Global Feature Extraction (GET_SF Equivalent)\n    res2 = self.get_global_sf(video_path)\n    # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n    # Split &amp; Combine Features\n    Nl, Bl = torch.split(res1, 75, dim=1)\n    Ng, Bg = torch.split(res2, 75, dim=1)\n    final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n    # print('final_res: ', final_res.shape)\n\n    return spatial_features, final_res  # Shape: [8, 300]\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_key_frames","title":"<code>extract_key_frames(video_path)</code>","text":"<p>Extracts 8 evenly spaced key frames across the entire video duration.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [8, 3, 672, 1120] containing 8 key frames.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_key_frames(self, video_path):\n    \"\"\"\n    Extracts 8 evenly spaced key frames across the entire video duration.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path).split('.')[0]\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if video_length &gt;= 8:\n        # Select 8 unique frame indices evenly spaced across the entire video\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n    else:\n        # Select all available frames and repeat the last one to reach 8\n        frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n    spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n    transform = transforms.Compose([\n        transforms.Resize([672, 1120]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    last_valid_frame = None\n    for idx, frame_idx in enumerate(frame_indices):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            spatial_features[idx] = transform(frame)\n            last_valid_frame = spatial_features[idx]\n        elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n            spatial_features[idx] = last_valid_frame\n\n    cap.release()\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n    return spatial_features\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_temporal_features","title":"<code>extract_temporal_features(video_path)</code>","text":"<p>Extracts SlowFast motion features on the entire video segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),  # Match SlowFast input size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n    ])\n\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frames.append(transform(frame))  # Resize &amp; normalize\n    cap.release()\n\n    if len(frames) &lt; 8:\n        raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n    video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n    # Prepare for SlowFast input\n    video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n    video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n    # Pack pathways for SlowFast model\n    _, pack_pathway_output = lazy_import()\n    inputs = pack_pathway_output(video_tensor, device='cpu')\n    # print('inputs len: ', len(inputs))\n    # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n    # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n    # Extract features using SlowFast\n    with torch.no_grad():\n        slow_feature, fast_feature = self.slowfast_model(inputs)\n\n    # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n    # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n    # Concatenate slow and fast features\n    features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n    # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n    return features\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.get_global_sf","title":"<code>get_global_sf(video_path)</code>","text":"<p>Extracts global brightness &amp; noise features across full video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted global features (Shape: [8, 150]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def get_global_sf(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts global brightness &amp; noise features across full video.\n\n    Args:\n        video_path (str): Path to video file.\n\n    Returns:\n        torch.Tensor: Extracted global features (Shape: [8, 150]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print('video_length: ', video_length)  # 16\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    length = len(frames)\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    while now + interval - 1 &lt; length:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n\n        # Step 1: Convert to tensor batch\n        images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n        # Step 2: Unfold into patches (Strictly following GET_SF)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n        # Step 3: Extract features using CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n        tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n        # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n        # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n        now += interval\n\n    # Handle remaining frames\n    if length &gt; now:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n\n        images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n        images = self.preprocess(images)\n\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n            logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n            # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n            # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n        tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n        # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 150]\n    # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n    # Step 4: Aggregate into 8 time slots\n    chunk_size = length // 8\n    final_res = [\n        torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n        for i in range(8)\n    ]\n\n    return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset","title":"<code>SimpleVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimpleVQA. Each sample returns:     - spatial_features (torch.Tensor): Extracted spatial frames.     - motion_features (torch.Tensor): Extracted motion-based clips.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass SimpleVQADataset(Dataset):\n    \"\"\"\n    Dataset for SimpleVQA.\n    Each sample returns:\n        - spatial_features (torch.Tensor): Extracted spatial frames.\n        - motion_features (torch.Tensor): Extracted motion-based clips.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(SimpleVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def video_processing_spatial(self, video_path):\n        \"\"\"\n        Extracts spatial frames with proper resizing and normalization.\n            - Key frame extraction: It selects 1 frame per second.\n            - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n        Return:\n            transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n                `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        # Compute the number of total seconds of the video\n        video_length_read = int(video_length/video_frame_rate) # math.ceil()\n        # print('video_length_read (s): ', video_length_read)\n        transformations = transforms.Compose([\n            transforms.Resize(520),\n            transforms.CenterCrop(448),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n        ])\n        transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n        video_read_index = 0\n        frame_idx = 0\n        for i in range(video_length):\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                # Key frames extraction\n                if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                    read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                    read_frame = transformations(read_frame)\n                    transformed_video[video_read_index] = read_frame\n                    video_read_index += 1\n                frame_idx += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; self.min_video_seconds:\n            for i in range(video_read_index, self.min_video_seconds):\n                transformed_video[i] = transformed_video[video_read_index - 1]\n\n        video_capture.release()\n        return transformed_video, video_name\n\n    def video_processing_motion(self, video_path):\n        \"\"\"\n        Extracts motion-based clips suitable for SlowFast.\n            - Standard input size: It resizes frames to 224 * 224.\n            - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n        Return:\n            transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n                Len(List) is total seconds of the video, with minium 8.\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n        ])\n        transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n        video_read_index = 0\n        for i in range(video_length): # All frames extraction\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transform(read_frame)\n                transformed_frame_all[video_read_index] = read_frame\n                video_read_index += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; video_length: \n            for i in range(video_read_index, video_length):\n                transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n        video_capture.release()\n\n        # Compute the number of total seconds of the video\n        video_clip = int(video_length/video_frame_rate)\n        # print('video_clip (s): ', video_clip)\n        video_length_clip = 32\n        transformed_video_all = []\n\n        # Extract motion-based clips: select 32 consecutive frames from each second\n        for i in range(video_clip):\n            transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n            if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n                transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n            else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n                transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n                for j in range((video_length - i * video_frame_rate), video_length_clip):\n                    transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n            transformed_video_all.append(transformed_video)\n\n        if video_clip &lt; self.min_video_seconds:\n            for i in range(video_clip, self.min_video_seconds):\n                transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n        return transformed_video_all, video_name\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            motion_features (List[torch.Tensor]): List of motion feature tensors.\n                Each tensor has shape [32, 3, 224, 224].\n                Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n            video_name (str): Video filename\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, video_name = self.video_processing_spatial(video_path)\n        motion_features, video_name = self.video_processing_motion(video_path)\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n        # print('motion_features len: ', len(motion_features)) # 8\n        # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n        return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [v_len_second, 3, 448, 448] <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).</p> <code>motion_features</code> <code>List[Tensor]</code> <p>List of motion feature tensors. Each tensor has shape [32, 3, 224, 224]. Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).</p> <code>video_name</code> <code>str</code> <p>Video filename</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n            `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        motion_features (List[torch.Tensor]): List of motion feature tensors.\n            Each tensor has shape [32, 3, 224, 224].\n            Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n        video_name (str): Video filename\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, video_name = self.video_processing_spatial(video_path)\n    motion_features, video_name = self.video_processing_motion(video_path)\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n    # print('motion_features len: ', len(motion_features)) # 8\n    # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n    return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.video_processing_motion","title":"<code>video_processing_motion(video_path)</code>","text":"<p>Extracts motion-based clips suitable for SlowFast.     - Standard input size: It resizes frames to 224 * 224.     - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second. Return:     transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224].          Len(List) is total seconds of the video, with minium 8.     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_motion(self, video_path):\n    \"\"\"\n    Extracts motion-based clips suitable for SlowFast.\n        - Standard input size: It resizes frames to 224 * 224.\n        - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n    Return:\n        transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n            Len(List) is total seconds of the video, with minium 8.\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n    ])\n    transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n    video_read_index = 0\n    for i in range(video_length): # All frames extraction\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            read_frame = transform(read_frame)\n            transformed_frame_all[video_read_index] = read_frame\n            video_read_index += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; video_length: \n        for i in range(video_read_index, video_length):\n            transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n    video_capture.release()\n\n    # Compute the number of total seconds of the video\n    video_clip = int(video_length/video_frame_rate)\n    # print('video_clip (s): ', video_clip)\n    video_length_clip = 32\n    transformed_video_all = []\n\n    # Extract motion-based clips: select 32 consecutive frames from each second\n    for i in range(video_clip):\n        transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n        if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n            transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n        else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n            transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n            for j in range((video_length - i * video_frame_rate), video_length_clip):\n                transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n        transformed_video_all.append(transformed_video)\n\n    if video_clip &lt; self.min_video_seconds:\n        for i in range(video_clip, self.min_video_seconds):\n            transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n    return transformed_video_all, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.video_processing_spatial","title":"<code>video_processing_spatial(video_path)</code>","text":"<p>Extracts spatial frames with proper resizing and normalization.     - Key frame extraction: It selects 1 frame per second.     - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520). Return:     transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448].          <code>video_length_read</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_spatial(self, video_path):\n    \"\"\"\n    Extracts spatial frames with proper resizing and normalization.\n        - Key frame extraction: It selects 1 frame per second.\n        - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n    Return:\n        transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n            `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    # Compute the number of total seconds of the video\n    video_length_read = int(video_length/video_frame_rate) # math.ceil()\n    # print('video_length_read (s): ', video_length_read)\n    transformations = transforms.Compose([\n        transforms.Resize(520),\n        transforms.CenterCrop(448),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n    ])\n    transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n    video_read_index = 0\n    frame_idx = 0\n    for i in range(video_length):\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            # Key frames extraction\n            if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transformations(read_frame)\n                transformed_video[video_read_index] = read_frame\n                video_read_index += 1\n            frame_idx += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; self.min_video_seconds:\n        for i in range(video_read_index, self.min_video_seconds):\n            transformed_video[i] = transformed_video[video_read_index - 1]\n\n    video_capture.release()\n    return transformed_video, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa","title":"<code>SimpleVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore","title":"<code>TIFAScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>AIGVEModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n        openai_key (str): The user's api key of the LLM models openai provides.\n        llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n        unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n        vqa_model_name (str): The name of the ``AIGVEModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.AIGVEModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.ToyDataset","title":"<code>ToyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>ToyDataset for testing.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory for data.</p> <code>None</code> <code>ann_file</code> <code>str</code> <p>Annotation file path.</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>Metadata information.</p> <code>None</code> <code>data_prefix</code> <code>dict</code> <p>Prefix paths for different modalities.</p> <code>None</code> <code>pipeline</code> <code>List[Union[Callable, dict]]</code> <p>Data transformation pipeline.</p> <code>[]</code> <code>modality</code> <code>dict</code> <p>Specifies which modalities are used (video, text, image).</p> <code>dict(use_video=True, use_text=True, use_image=False)</code> <code>image_frame</code> <code>int</code> <p>Number of frames for images.</p> <code>None</code> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass ToyDataset(BaseDataset):\n    \"\"\"ToyDataset for testing.\n\n    Args:\n        data_root (str, optional): Root directory for data.\n        ann_file (str): Annotation file path.\n        metainfo (dict, optional): Metadata information.\n        data_prefix (dict): Prefix paths for different modalities.\n        pipeline (List[Union[Callable, dict]]): Data transformation pipeline.\n        modality (dict): Specifies which modalities are used (video, text, image).\n        image_frame (int, optional): Number of frames for images.\n    \"\"\"\n\n    def __init__(self,\n                 data_root: Optional[str] = None,\n                 ann_file: str = '',\n                 metainfo: Optional[dict] = None,\n                 data_prefix: dict = None,\n                 pipeline: List[Union[Callable, dict]] = [],\n                 modality: dict = dict(use_video=True, use_text=True, use_image=False),\n                 image_frame: int = None,\n                 **kwargs) -&gt; None:\n        super().__init__(\n            data_root=data_root,\n            ann_file=ann_file,\n            metainfo=metainfo,\n            data_prefix=data_prefix,\n            pipeline=pipeline,\n            **kwargs\n        )\n        self.modality = modality\n        self.image_frame = image_frame\n        assert self.modality['use_video'] or self.modality['use_text'], (\n            'Please specify the `modality` (`use_video` '\n            f', `use_text`) for {self.__class__.__name__}')\n\n    def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n        \"\"\"Parse raw data info.\"\"\"\n        info = {}\n        info['img_frame'] = None\n        if self.modality['use_text']:\n            info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                         raw_data_info['prompt_gt'])\n\n        if self.modality['use_video'] or self.modality['use_image']:\n            info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['video_path_pd'])\n            if self.modality['use_image']:\n                info['img_frame'] = self.image_frame\n\n        return info\n</code></pre>"},{"location":"documentations/aigve/#aigve.ToyDataset.parse_data_info","title":"<code>parse_data_info(raw_data_info)</code>","text":"<p>Parse raw data info.</p> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n    \"\"\"Parse raw data info.\"\"\"\n    info = {}\n    info['img_frame'] = None\n    if self.modality['use_text']:\n        info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['prompt_gt'])\n\n    if self.modality['use_video'] or self.modality['use_image']:\n        info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                 raw_data_info['video_path_pd'])\n        if self.modality['use_image']:\n            info['img_frame'] = self.image_frame\n\n    return info\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore","title":"<code>VIEEvalScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>VIEEvalScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_backbone</code> <code>str</code> <p>The name of the LLM model used in the VIEEvalScore evaluator. Defaults to <code>got4o</code>.</p> <code>'gpt4o'</code> <code>api_key_path</code> <code>str</code> <p>The user's api key path to initialize LLM models provides by openai.</p> <code>'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt'</code> <code>task</code> <code>str</code> <p>The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.</p> <code>'t2v'</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>@METRICS.register_module()\nclass VIEEvalScore(BaseMetric):\n    \"\"\" Initialize the ``VIEEvalScore`` evaluator.\n\n    Args:\n        llm_backbone (str): The name of the LLM model used in the VIEEvalScore evaluator. Defaults to ``got4o``.\n        api_key_path (str): The user's api key path to initialize LLM models provides by openai.\n        task (str): The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.\n    \"\"\"\n    def __init__(self,\n                 llm_backbone: str = \"gpt4o\",\n                 api_key_path: str = 'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt',\n                 task: str = 't2v',\n                 ):\n        super().__init__()\n\n        self.api_key_path = api_key_path\n        self.llm_backbone = llm_backbone\n        self.task = task\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/VIE'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/TIGER-AI-Lab/VIEScore.git', \n                submodule_path=self.submodel_path\n            )  \n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )  \n        from .VIEScore.viescore import VIEScore \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n\n        self.vie_score = VIEScore(backbone=self.llm_backbone, task=self.task, key_path=self.api_key_path)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"VIEScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_vie_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            vie_score_list = []\n            for index, frame_path in enumerate(input_video):\n                pil_image = Image.open(frame_path)\n                score_list = self.vie_score.evaluate(pil_image, input_prompt)\n                sementics_score, quality_score, overall_score = score_list\n                vie_score_list.append(overall_score)\n            average_vie_score = sum(vie_score_list)/len(vie_score_list)\n            average_vie_score_list.append(average_vie_score)\n\n        result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        vie_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            vie_score_np[i] = result['vie_score']\n\n        vie_score_np_mean = np.mean(vie_score_np) \n\n        print(\"Test results: vie score with dependency={:.4f}\"\n              .format(vie_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    vie_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        vie_score_np[i] = result['vie_score']\n\n    vie_score_np_mean = np.mean(vie_score_np) \n\n    print(\"Test results: vie score with dependency={:.4f}\"\n          .format(vie_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>VIEScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"VIEScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_vie_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        vie_score_list = []\n        for index, frame_path in enumerate(input_video):\n            pil_image = Image.open(frame_path)\n            score_list = self.vie_score.evaluate(pil_image, input_prompt)\n            sementics_score, quality_score, overall_score = score_list\n            vie_score_list.append(overall_score)\n        average_vie_score = sum(vie_score_list)/len(vie_score_list)\n        average_vie_score_list.append(average_vie_score)\n\n    result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchDataset","title":"<code>VbenchDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VbenchDataset(BaseDataset):\n    def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n                 serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000,\n                 ):\n        \"\"\"\n        Args:\n            ann_file (str): annotation file path\n            metainfo (dict): meta information about the dataset\n            data_root (str): the root path of the data\n            data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n            filter_cfg (dict): the filter configuration\n            indices (list): the indices of the data\n            serialize_data (bool): whether to serialize the data\n            pipeline (list): the pipeline of the data\n            test_mode (bool): whether in test mode\n            lazy_init (bool): whether to lazy initialize the dataset\n            max_refetch (int): the maximum number of refetching data\n            model_name (str): the name of the model\n\n        \"\"\"\n        super(VbenchDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: the length of the dataset\n        \"\"\"\n        return self.metainfo['length']\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): the index of the data\n        \"\"\"\n        anno_info = self.get_data_info(idx)\n        video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n        prompt = anno_info['prompt_gt']\n\n        inputs = {\n            'video_path': video_path,\n            'prompt': prompt,\n        }\n        return inputs\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the data</p> required Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): the index of the data\n    \"\"\"\n    anno_info = self.get_data_info(idx)\n    video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n    prompt = anno_info['prompt_gt']\n\n    inputs = {\n        'video_path': video_path,\n        'prompt': prompt,\n    }\n    return inputs\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchDataset.__init__","title":"<code>__init__(ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None, serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ann_file</code> <code>str</code> <p>annotation file path</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>meta information about the dataset</p> <code>None</code> <code>data_root</code> <code>str</code> <p>the root path of the data</p> <code>''</code> <code>data_prefix</code> <code>dict</code> <p>the prefix of the data, for example, the prefix of the image path</p> <code>{'video_path_pd': ''}</code> <code>filter_cfg</code> <code>dict</code> <p>the filter configuration</p> <code>None</code> <code>indices</code> <code>list</code> <p>the indices of the data</p> <code>None</code> <code>serialize_data</code> <code>bool</code> <p>whether to serialize the data</p> <code>True</code> <code>pipeline</code> <code>list</code> <p>the pipeline of the data</p> <code>[]</code> <code>test_mode</code> <code>bool</code> <p>whether in test mode</p> <code>False</code> <code>lazy_init</code> <code>bool</code> <p>whether to lazy initialize the dataset</p> <code>False</code> <code>max_refetch</code> <code>int</code> <p>the maximum number of refetching data</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>the name of the model</p> required Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n             serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000,\n             ):\n    \"\"\"\n    Args:\n        ann_file (str): annotation file path\n        metainfo (dict): meta information about the dataset\n        data_root (str): the root path of the data\n        data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n        filter_cfg (dict): the filter configuration\n        indices (list): the indices of the data\n        serialize_data (bool): whether to serialize the data\n        pipeline (list): the pipeline of the data\n        test_mode (bool): whether in test mode\n        lazy_init (bool): whether to lazy initialize the dataset\n        max_refetch (int): the maximum number of refetching data\n        model_name (str): the name of the model\n\n    \"\"\"\n    super(VbenchDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the length of the dataset</p> Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: the length of the dataset\n    \"\"\"\n    return self.metainfo['length']\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchMetric","title":"<code>VbenchMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>@METRICS.register_module()\nclass VbenchMetric(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n                local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n            eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n            ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n            'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n            'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n            eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n            local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n            read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n            category(str): The category to evaluate on, usage: --category=animal.\n            imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n            3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            Then the center 512 x 512 after resized is used for evaluation.\n            4. 'None': no preprocessing\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.results = []\n        self.vbench_prompt_json_path = vbench_prompt_json_path\n        self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n        self.eval_aspects = eval_aspects\n        self.eval_mode = eval_mode\n        self.local = local\n        self.read_frame = read_frame\n        self.category = category\n        self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n        if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n            video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n            if len(video_roots) &gt; 1:\n                raise ValueError('The video paths should be in the same directory.')\n            else:\n                video_path = video_roots.pop()\n        elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n            video_path = data_batch['video_path'][0]\n        elif type(data_batch['video_path']) == str:\n            video_path = data_batch['video_path']\n        else:\n            raise ValueError('The video paths should be a list or a string.')\n\n\n\n        kwargs = {}\n\n        if self.category != '':\n            kwargs['category'] = self.category\n\n        kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n        result = self.vbench.evaluate(\n            videos_path = video_path,\n            name = f'results_{self.eval_mode}',\n            prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n            dimension_list = self.eval_aspects,\n            local=self.local,\n            read_frame=self.read_frame,\n            mode=self.eval_mode, **kwargs)\n\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        print('results:', results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchMetric.__init__","title":"<code>__init__(collect_device=None, prefix=None, vbench_prompt_json_path=None, eval_aspects=None, eval_mode='vbench_standard', local=False, read_frame=False, category='', imaging_quality_preprocessing_mode='longer', **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>vbench_prompt_json_path</code> <code>str</code> <p>The path to the vbench prompt JSON file.</p> <code>None</code> <code>eval_aspects</code> <code>list</code> <p>the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are</p> <code>None</code> <code>eval_mode</code> <code>str</code> <p>the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']</p> <code>'vbench_standard'</code> <code>local</code> <code>bool</code> <p>whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet</p> <code>False</code> <code>read_frame</code> <code>bool</code> <p>whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video</p> <code>False</code> <code>category(str)</code> <p>The category to evaluate on, usage: --category=animal.</p> required <code>imaging_quality_preprocessing_mode(str)</code> <ol> <li>'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.</li> </ol> required <code>2.</code> <code>longer</code> <p>if the longer side is more than 512, the image is resized so that the longer side is 512.</p> required <code>3.</code> <code>shorter_centercrop</code> <p>if the shorter side is more than 512, the image is resized so that the shorter side is 512.</p> required <code>4.</code> <code>None</code> <p>no preprocessing</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n            local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n        eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n        ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n        'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n        'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n        eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n        local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n        read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n        category(str): The category to evaluate on, usage: --category=animal.\n        imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n        3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        Then the center 512 x 512 after resized is used for evaluation.\n        4. 'None': no preprocessing\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.results = []\n    self.vbench_prompt_json_path = vbench_prompt_json_path\n    self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n    self.eval_aspects = eval_aspects\n    self.eval_mode = eval_mode\n    self.local = local\n    self.read_frame = read_frame\n    self.category = category\n    self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchMetric.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    print('results:', results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VbenchMetric.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n    if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n        video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n        if len(video_roots) &gt; 1:\n            raise ValueError('The video paths should be in the same directory.')\n        else:\n            video_path = video_roots.pop()\n    elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n        video_path = data_batch['video_path'][0]\n    elif type(data_batch['video_path']) == str:\n        video_path = data_batch['video_path']\n    else:\n        raise ValueError('The video paths should be a list or a string.')\n\n\n\n    kwargs = {}\n\n    if self.category != '':\n        kwargs['category'] = self.category\n\n    kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n    result = self.vbench.evaluate(\n        videos_path = video_path,\n        name = f'results_{self.eval_mode}',\n        prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n        dimension_list = self.eval_aspects,\n        local=self.local,\n        read_frame=self.read_frame,\n        mode=self.eval_mode, **kwargs)\n\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy","title":"<code>VideoPhy</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset","title":"<code>VideoPhyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoPhyDataset(Dataset):\n    def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n        \"\"\"\n        Args:\n            data_path (str): Path to the data folder, it should be a json file\n            tokenizer (Tokenizer): Tokenizer object\n            processor (Processor): Processor object\n            max_length (int): Maximum length of the input sequence\n            media_tokens (list): List of media tokens\n        \"\"\"\n        self.dataset = json.load(open(data_path))\n        self.video_root_path = video_root_path\n\n        self.hf_token = hf_token\n        self.hf_checkpoint = hf_checkpoint\n        self.max_length = max_length\n        self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n        self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n        self.bucket = {}\n\n\n        # initialize tokenizer\n        if tokenizer is not None:\n            self.tokenizer = tokenizer\n        else:\n            self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n        MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n        # initialize processor\n        if processor is not None:\n            self.processor = processor\n        else:\n            self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: Length of the dataset\n        \"\"\"\n        return self.dataset['metainfo']['length']\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the dataset\n        Returns:\n            dict: Dictionary containing the video, text, video path and caption\n        \"\"\"\n        data = self.dataset['dataset_list'][idx]\n        videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n        caption = data['prompt_gt']\n        # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n        video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n        text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n        item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n        return item\n\n    def _extract_text_token_from_conversation(self, data, max_length, index):\n        \"\"\"\n        Extracts the text tokens from the conversation\n        Args:\n            data (str): Conversation\n            max_length (int): Maximum length of the input sequence\n            index (int): Index of the dataset\n        \"\"\"\n        # output enc_chunk\n        enc_chunk = []\n\n        if self.tokenizer.bos_token_id &gt; 0:\n            prompt_chunk = [self.tokenizer.bos_token_id]\n        else:\n            prompt_chunk = []\n\n        # conversation = data[\"completion\"]\n        conversation = data\n\n        # For Text only data\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            prompt_length = -1\n            stop_flag = False\n            for idx, chunk_str in enumerate(chunk_strs):\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                            stop_flag = True\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                            stop_flag = True\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n                    if stop_flag:\n                        break\n\n        # For Image-Text Data\n        else:\n            enc_length = 0\n            prompt_length = -2\n            pattern = '|'.join(\n                map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n            for idx, chunk_str in enumerate(chunk_strs):\n                if enc_length &gt;= max_length + 1:\n                    break\n\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_str in self.media_tokens:\n                        # [CLS] + 256 + [EOS]\n                        if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                            break\n                        else:\n                            enc_chunk += [self.media_tokens[chunk_str]\n                                          ] * self.media_lengths[chunk_str]\n                            enc_length += self.media_lengths[chunk_str]\n                            label_chunk += [0] * self.media_lengths[chunk_str]\n                    else:\n\n                        if chunk_strs[idx - 1] == 'AI: ':\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length:\n                                curr_chunk = curr_chunk[:max_length - enc_length]\n                            curr_chunk += [self.tokenizer.eos_token_id]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [1] * len(curr_chunk)\n                        else:\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                                curr_chunk = curr_chunk[:max_length +\n                                                         1 - enc_length]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [0] * len(curr_chunk)\n\n        if enc_length &lt; max_length + 1:\n            padding_chunk = [self.tokenizer.pad_token_id] * \\\n                            (max_length + 1 - enc_length)\n            padding_length = len(padding_chunk)\n            label_chunk += [0] * (max_length + 1 - enc_length)\n            enc_chunk = enc_chunk + padding_chunk\n        else:\n            padding_length = 0\n\n        assert enc_length + padding_length == max_length + \\\n               1, (index, prompt_length, enc_length,\n                   padding_length, max_length + 1)\n        assert len(label_chunk) == max_length + \\\n               1, (len(label_chunk), max_length + 1)\n        non_padding_mask = [1 if i &lt; enc_length -\n                                 1 else 0 for i in range(max_length)]\n\n        enc_chunk = torch.tensor(enc_chunk).long()\n        non_padding_mask = torch.tensor(non_padding_mask).long()\n        prompt_mask = torch.tensor(label_chunk)[1:].long()\n        prompt_length = torch.tensor([prompt_length]).long()\n\n        # Create loss mask\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            non_media_mask = torch.ones_like(non_padding_mask).long()\n        else:\n            tmp_enc_chunk = enc_chunk.clone()\n            tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n            tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n            non_media_mask = torch.tensor(tmp_enc_chunk).long()\n            non_media_mask = non_media_mask[1:].long()\n        return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n                \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the dataset</p> required <p>Returns:     dict: Dictionary containing the video, text, video path and caption</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): Index of the dataset\n    Returns:\n        dict: Dictionary containing the video, text, video path and caption\n    \"\"\"\n    data = self.dataset['dataset_list'][idx]\n    videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n    caption = data['prompt_gt']\n    # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n    video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n    text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n    item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n    return item\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__init__","title":"<code>__init__(data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data folder, it should be a json file</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>None</code> <code>processor</code> <code>Processor</code> <p>Processor object</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence</p> <code>2048</code> <code>media_tokens</code> <code>list</code> <p>List of media tokens</p> <code>['&lt;image&gt;', '&lt;|video|&gt;']</code> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n    \"\"\"\n    Args:\n        data_path (str): Path to the data folder, it should be a json file\n        tokenizer (Tokenizer): Tokenizer object\n        processor (Processor): Processor object\n        max_length (int): Maximum length of the input sequence\n        media_tokens (list): List of media tokens\n    \"\"\"\n    self.dataset = json.load(open(data_path))\n    self.video_root_path = video_root_path\n\n    self.hf_token = hf_token\n    self.hf_checkpoint = hf_checkpoint\n    self.max_length = max_length\n    self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n    self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n    self.bucket = {}\n\n\n    # initialize tokenizer\n    if tokenizer is not None:\n        self.tokenizer = tokenizer\n    else:\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n    MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n    # initialize processor\n    if processor is not None:\n        self.processor = processor\n    else:\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Length of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: Length of the dataset\n    \"\"\"\n    return self.dataset['metainfo']['length']\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore","title":"<code>VideoScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScoreDataset","title":"<code>VideoScoreDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoScoreDataset(BaseDataset):\n    def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n                 serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n                max_num_frames: int = None):\n        \"\"\"\n        Args:\n            ann_file (str): annotation file path\n            metainfo (dict): meta information about the dataset\n            data_root (str): the root path of the data\n            data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n            filter_cfg (dict): the filter configuration\n            indices (list): the indices of the data\n            serialize_data (bool): whether to serialize the data\n            pipeline (list): the pipeline of the data\n            test_mode (bool): whether in test mode\n            lazy_init (bool): whether to lazy initialize the dataset\n            max_refetch (int): the maximum number of refetching data\n            model_name (str): the name of the model\n            regression_query_prompt (str): the prompt for the regression query\n            max_num_frames (int): the maximum number of frames\n        \"\"\"\n        super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n        if model_name is None:\n            self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n        else:\n            self.model_name = model_name\n\n        self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n        if regression_query_prompt is not None:\n            self.regression_query_prompt = regression_query_prompt\n        else:\n            self.regression_query_prompt = '''\n                Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n                please watch the following frames of a given video and see the text prompt for generating the video,\n                then give scores from 5 different dimensions:\n                (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n                (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n                (3) dynamic degree, the degree of dynamic changes\n                (4) text-to-video alignment, the alignment between the text prompt and the video content\n                (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n                for each dimension, output a float number from 1.0 to 4.0,\n                the higher the number is, the better the video performs in that sub-score, \n                the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n                Here is an output example:\n                visual quality: 3.2\n                temporal consistency: 2.7\n                dynamic degree: 4.0\n                text-to-video alignment: 2.3\n                factual consistency: 1.8\n                For this video, the text prompt is \"{text_prompt}\",\n                all the frames of video are as follows:\n            '''\n        if max_num_frames is not None:\n            self.max_num_frames = max_num_frames\n        else:\n            self.max_num_frames = 48\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: the length of the dataset\n        \"\"\"\n        return self.metainfo['length']\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): the index of the data\n        \"\"\"\n        anno_info = self.get_data_info(idx)\n        video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n        container = av.open(video_path)\n\n        total_frames = container.streams.video[0].frames\n        if total_frames &gt; self.max_num_frames:\n            indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n        else:\n            indices = np.arange(total_frames)\n\n        frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n        eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n        num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n        if num_image_token &lt; len(frames):\n            eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n        flatten_images = []\n        for x in [frames]:\n            if isinstance(x, list):\n                flatten_images.extend(x)\n            else:\n                flatten_images.append(x)\n        flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n        inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n        return inputs\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScoreDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the data</p> required Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): the index of the data\n    \"\"\"\n    anno_info = self.get_data_info(idx)\n    video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n    container = av.open(video_path)\n\n    total_frames = container.streams.video[0].frames\n    if total_frames &gt; self.max_num_frames:\n        indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n    else:\n        indices = np.arange(total_frames)\n\n    frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n    eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n    num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n    if num_image_token &lt; len(frames):\n        eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n    flatten_images = []\n    for x in [frames]:\n        if isinstance(x, list):\n            flatten_images.extend(x)\n        else:\n            flatten_images.append(x)\n    flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n    inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n    return inputs\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScoreDataset.__init__","title":"<code>__init__(ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None, serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name=None, regression_query_prompt=None, max_num_frames=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ann_file</code> <code>str</code> <p>annotation file path</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>meta information about the dataset</p> <code>None</code> <code>data_root</code> <code>str</code> <p>the root path of the data</p> <code>''</code> <code>data_prefix</code> <code>dict</code> <p>the prefix of the data, for example, the prefix of the image path</p> <code>{'video_path_pd': ''}</code> <code>filter_cfg</code> <code>dict</code> <p>the filter configuration</p> <code>None</code> <code>indices</code> <code>list</code> <p>the indices of the data</p> <code>None</code> <code>serialize_data</code> <code>bool</code> <p>whether to serialize the data</p> <code>True</code> <code>pipeline</code> <code>list</code> <p>the pipeline of the data</p> <code>[]</code> <code>test_mode</code> <code>bool</code> <p>whether in test mode</p> <code>False</code> <code>lazy_init</code> <code>bool</code> <p>whether to lazy initialize the dataset</p> <code>False</code> <code>max_refetch</code> <code>int</code> <p>the maximum number of refetching data</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>the name of the model</p> <code>None</code> <code>regression_query_prompt</code> <code>str</code> <p>the prompt for the regression query</p> <code>None</code> <code>max_num_frames</code> <code>int</code> <p>the maximum number of frames</p> <code>None</code> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n             serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n            max_num_frames: int = None):\n    \"\"\"\n    Args:\n        ann_file (str): annotation file path\n        metainfo (dict): meta information about the dataset\n        data_root (str): the root path of the data\n        data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n        filter_cfg (dict): the filter configuration\n        indices (list): the indices of the data\n        serialize_data (bool): whether to serialize the data\n        pipeline (list): the pipeline of the data\n        test_mode (bool): whether in test mode\n        lazy_init (bool): whether to lazy initialize the dataset\n        max_refetch (int): the maximum number of refetching data\n        model_name (str): the name of the model\n        regression_query_prompt (str): the prompt for the regression query\n        max_num_frames (int): the maximum number of frames\n    \"\"\"\n    super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n    if model_name is None:\n        self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n    else:\n        self.model_name = model_name\n\n    self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n    if regression_query_prompt is not None:\n        self.regression_query_prompt = regression_query_prompt\n    else:\n        self.regression_query_prompt = '''\n            Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n            please watch the following frames of a given video and see the text prompt for generating the video,\n            then give scores from 5 different dimensions:\n            (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n            (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n            (3) dynamic degree, the degree of dynamic changes\n            (4) text-to-video alignment, the alignment between the text prompt and the video content\n            (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n            for each dimension, output a float number from 1.0 to 4.0,\n            the higher the number is, the better the video performs in that sub-score, \n            the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n            Here is an output example:\n            visual quality: 3.2\n            temporal consistency: 2.7\n            dynamic degree: 4.0\n            text-to-video alignment: 2.3\n            factual consistency: 1.8\n            For this video, the text prompt is \"{text_prompt}\",\n            all the frames of video are as follows:\n        '''\n    if max_num_frames is not None:\n        self.max_num_frames = max_num_frames\n    else:\n        self.max_num_frames = 48\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScoreDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the length of the dataset</p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: the length of the dataset\n    \"\"\"\n    return self.metainfo['length']\n</code></pre>"},{"location":"documentations/aigve/#organization-of-this-library","title":"Organization of this Library","text":"<ul> <li>aigve.configs</li> <li>aigve.core</li> <li>aigve.datasets</li> <li>aigve.metrics</li> <li>aigve.utils</li> </ul>"},{"location":"documentations/configs/","title":"aigve.configs","text":"<p>In <code>AIGVE</code>, configuration management is handled using MMEngine's configuration  system, which provides a modular, hierarchical, and flexible approach to defining  experiment settings. The config system allows users to efficiently configure  video evaluation metrics, datasets, dataloaders, etc., making  benchmarking and experimentation more streamlined in a structured manner.</p>"},{"location":"documentations/configs/#aigve.configs--key-features-of-aigve-config-system","title":"Key Features of AIGVE Config System","text":"<ul> <li>Modular Design: Uses <code>_base_</code> configurations to reduce redundancy.</li> <li>Customizable Pipelines: Define different evaluation metrics and datasets easily.</li> <li>Flexible Overriding: Modify parameters dynamically via command-line arguments.</li> <li>Scalability: Supports large-scale video evaluation with efficient data loading.</li> </ul>"},{"location":"documentations/configs/#aigve.configs--aigve-configuration-example","title":"AIGVE Configuration Example","text":"<p>AIGVE uses structured configuration files to define evaluation settings.  Below is an example of a CLIPSim metric configuration file:</p> <pre><code># Copyright (c) IFM Lab. All rights reserved.\nfrom mmengine.config import read_base\nfrom metrics.text_video_alignment.similarity_based import CLIPSimScore\n\nwith read_base():\n    from ._base_.datasets.clipsim_dataset import *\n    from ._base_.default import *\n\nval_evaluator = dict(\n    type=CLIPSimScore,\n    model_name='openai/clip-vit-base-patch32',\n    logit_scale=False,\n)\n\nval_dataloader = dict(\n    batch_size=2, \n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type=DefaultSampler, shuffle=False),\n    dataset=dict(\n        type=CLIPSimDataset,\n        processor_name='openai/clip-vit-base-patch32',\n        video_dir='AIGVE_Tool/data/toy/evaluate/',\n        prompt_dir='AIGVE_Tool/data/toy/annotations/evaluate.json',\n    )\n)\n</code></pre>"},{"location":"documentations/core/","title":"aigve.core","text":""},{"location":"documentations/core/#aigve.core.AIGVELoop","title":"<code>AIGVELoop</code>","text":"<p>               Bases: <code>BaseLoop</code></p> <p>Loop for AIGVE metric evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>Runner</code> <p>A reference of runner.</p> required <code>dataloader</code> <code>Dataloader or dict</code> <p>A dataloader object or a dict to build a dataloader.</p> required <code>evaluator</code> <code>Evaluator or dict or list</code> <p>Used for computing metrics.</p> required <code>fp16</code> <code>bool</code> <p>Whether to enable fp16 validation. Defaults to False.</p> <code>False</code> Source code in <code>aigve/core/loops.py</code> <pre><code>@LOOPS.register_module()\nclass AIGVELoop(BaseLoop):\n    \"\"\"Loop for AIGVE metric evaluation.\n\n    Args:\n        runner (Runner): A reference of runner.\n        dataloader (Dataloader or dict): A dataloader object or a dict to\n            build a dataloader.\n        evaluator (Evaluator or dict or list): Used for computing metrics.\n        fp16 (bool): Whether to enable fp16 validation. Defaults to\n            False.\n    \"\"\"\n\n    def __init__(self,\n                 runner,\n                 dataloader: Union[DataLoader, Dict],\n                 evaluator: Union[Evaluator, Dict, List],\n                 fp16: bool = False) -&gt; None:\n        super().__init__(runner, dataloader)\n        if isinstance(evaluator, (dict, list)):\n            self.evaluator = runner.build_evaluator(evaluator)  # type: ignore\n        else:\n            assert isinstance(evaluator, Evaluator), (\n                'evaluator must be one of dict, list or Evaluator instance, '\n                f'but got {type(evaluator)}.')\n            self.evaluator = evaluator  # type: ignore\n        if hasattr(self.dataloader.dataset, 'metainfo'):\n            self.evaluator.dataset_meta = self.dataloader.dataset.metainfo\n            self.runner.visualizer.dataset_meta = \\\n                self.dataloader.dataset.metainfo\n        else:\n            print_log(\n                f'Dataset {self.dataloader.dataset.__class__.__name__} has no '\n                'metainfo. ``dataset_meta`` in evaluator, metric and '\n                'visualizer will be None.',\n                logger='current',\n                level=logging.WARNING)\n        self.fp16 = fp16\n        self.val_loss: Dict[str, HistoryBuffer] = dict()\n\n    def run(self) -&gt; dict:\n        \"\"\"Launch validation.\"\"\"\n        self.runner.call_hook('before_val')\n        self.runner.call_hook('before_val_epoch')\n        self.runner.model.eval()\n\n        for idx, data_batch in enumerate(self.dataloader):\n            self.run_iter(idx, data_batch)\n\n        metrics = self.evaluator.evaluate(len(self.dataloader.dataset))\n\n        self.runner.call_hook('after_val_epoch', metrics=metrics)\n        self.runner.call_hook('after_val')\n        return metrics\n\n    def run_iter(self, idx, data_batch: Sequence[dict]):\n        \"\"\"Iterate one mini-batch.\n\n        Args:\n            data_batch (Sequence[dict]): Batch of data\n                from dataloader.\n        \"\"\"\n        self.runner.call_hook(\n            'before_val_iter', batch_idx=idx, data_batch=data_batch)\n        outputs = data_batch\n        self.evaluator.process(data_batch=data_batch, data_samples=outputs)\n        self.runner.call_hook(\n            'after_val_iter',\n            batch_idx=idx,\n            data_batch=data_batch,\n            outputs=outputs)\n</code></pre>"},{"location":"documentations/core/#aigve.core.AIGVELoop.run","title":"<code>run()</code>","text":"<p>Launch validation.</p> Source code in <code>aigve/core/loops.py</code> <pre><code>def run(self) -&gt; dict:\n    \"\"\"Launch validation.\"\"\"\n    self.runner.call_hook('before_val')\n    self.runner.call_hook('before_val_epoch')\n    self.runner.model.eval()\n\n    for idx, data_batch in enumerate(self.dataloader):\n        self.run_iter(idx, data_batch)\n\n    metrics = self.evaluator.evaluate(len(self.dataloader.dataset))\n\n    self.runner.call_hook('after_val_epoch', metrics=metrics)\n    self.runner.call_hook('after_val')\n    return metrics\n</code></pre>"},{"location":"documentations/core/#aigve.core.AIGVELoop.run_iter","title":"<code>run_iter(idx, data_batch)</code>","text":"<p>Iterate one mini-batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence[dict]</code> <p>Batch of data from dataloader.</p> required Source code in <code>aigve/core/loops.py</code> <pre><code>def run_iter(self, idx, data_batch: Sequence[dict]):\n    \"\"\"Iterate one mini-batch.\n\n    Args:\n        data_batch (Sequence[dict]): Batch of data\n            from dataloader.\n    \"\"\"\n    self.runner.call_hook(\n        'before_val_iter', batch_idx=idx, data_batch=data_batch)\n    outputs = data_batch\n    self.evaluator.process(data_batch=data_batch, data_samples=outputs)\n    self.runner.call_hook(\n        'after_val_iter',\n        batch_idx=idx,\n        data_batch=data_batch,\n        outputs=outputs)\n</code></pre>"},{"location":"documentations/datasets/","title":"aigve.datasets","text":""},{"location":"documentations/datasets/#aigve.datasets.CLIPTempDataset","title":"<code>CLIPTempDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass CLIPTempDataset(Dataset):\n    def __init__(self, processor_name, prompt_dir, video_dir):\n        super(CLIPTempDataset, self).__init__()\n        self.prompt_dir = prompt_dir\n        self.video_dir = video_dir\n        self.processor_name = processor_name\n\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.video_names = self._read_videoname()\n\n    def _read_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        video_name_list = []\n        for item in read_data[\"datset_list\"]:\n            video_name = item['video_path_pd'].strip()\n            video_name_list.append(video_name)\n\n        return video_name_list\n\n    def __len__(self):\n        return len(self.video_names)-1\n\n    def __getitem__(self, index):\n        '''return video frame pairs\n        '''\n        video_name = self.video_names[index]\n        video_path = self.video_dir + video_name\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n            frames.append(resized_frame)\n\n        input_frame_tensor = self.processor(\n            images=frames,\n            padding=True,\n            truncation=True,\n            max_length=77,\n            return_tensors=\"pt\",\n        )['pixel_values']\n\n        return input_frame_tensor\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.CLIPTempDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>return video frame pairs</p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>def __getitem__(self, index):\n    '''return video frame pairs\n    '''\n    video_name = self.video_names[index]\n    video_path = self.video_dir + video_name\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n        frames.append(resized_frame)\n\n    input_frame_tensor = self.processor(\n        images=frames,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    )['pixel_values']\n\n    return input_frame_tensor\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.FidDataset","title":"<code>FidDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for Fr\u00e9chet Inception Distance (FID) evaluation.</p> <p>For each sample, this dataset:     - Loads both the ground-truth (real) and generated (predicted) videos.     - Converts each video into a tensor of shape [T, C, H, W] using OpenCV.     - Optionally pads or truncates videos to a fixed number of frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_dir</code> <code>str</code> <p>Directory containing video files.</p> required <code>prompt_dir</code> <code>str</code> <p>Path to JSON file that lists ground-truth and generated video filenames.</p> required <code>max_len</code> <code>int</code> <p>Maximum number of frames to load per video. Default: 500.</p> <code>500</code> <code>if_pad</code> <code>bool</code> <p>Whether to pad videos to exactly <code>max_len</code> frames. If False, videos can have variable lengths.</p> <code>False</code> Source code in <code>aigve/datasets/fid_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass FidDataset(Dataset):\n    \"\"\"\n    Dataset for Fr\u00e9chet Inception Distance (FID) evaluation.\n\n    For each sample, this dataset:\n        - Loads both the ground-truth (real) and generated (predicted) videos.\n        - Converts each video into a tensor of shape [T, C, H, W] using OpenCV.\n        - Optionally pads or truncates videos to a fixed number of frames.\n\n    Args:\n        video_dir (str): Directory containing video files.\n        prompt_dir (str): Path to JSON file that lists ground-truth and generated video filenames.\n        max_len (int): Maximum number of frames to load per video. Default: 500.\n        if_pad (bool): Whether to pad videos to exactly `max_len` frames. If False, videos can have variable lengths.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, max_len=500, if_pad=False):\n        super(FidDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.max_len = max_len\n        self.if_pad = if_pad\n\n        self.gt_video_names, self.gen_video_names = self._read_video_names()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n            gt_video_names = [item['video_path_gt'].strip() for item in read_data[\"data_list\"]]\n            gen_video_names = [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n        return gt_video_names, gen_video_names\n\n    def _load_video_tensor(self, video_path: str) -&gt; torch.Tensor:\n        \"\"\"Load a video and return its tensor of shape [T, C, H, W].\"\"\"\n        assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n        cap = cv2.VideoCapture(video_path)\n        input_frames = []\n        frame_count = 0\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        if len(input_frames) == 0:\n            raise RuntimeError(f\"No valid frames found in {video_path}\")\n\n        if self.if_pad:\n            num_frames = len(input_frames)\n            if num_frames &lt; self.max_len:\n                pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n                video_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            else:\n                video_tensor = torch.stack(input_frames[:self.max_len])\n        else:\n            video_tensor = torch.stack(input_frames)\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        return video_tensor.permute(0, 3, 1, 2)\n\n    def __len__(self):\n        return len(self.gt_video_names)\n\n    def __getitem__(self, index) -&gt; tuple[torch.Tensor, torch.Tensor, str, str]:\n        \"\"\"\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, str, str]: \n                - Ground-truth (Real) video tensor of shape [T, C, H, W].\n                - Generated video tensor of shape [T, C, H, W].\n                - Ground-truth video name.\n                - Generated video name.\n        \"\"\"\n        gt_video_name = self.gt_video_names[index]\n        gt_video_path = os.path.join(self.video_dir, gt_video_name)\n        gen_video_name = self.gen_video_names[index]\n        gen_video_path = os.path.join(self.video_dir, gen_video_name) \n\n        gt_video_tensor = self._load_video_tensor(gt_video_path)\n        gen_video_tensor = self._load_video_tensor(gen_video_path)\n\n        return gt_video_tensor, gen_video_tensor, gt_video_name, gen_video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.FidDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, str, str]</code> <p>Tuple[torch.Tensor, torch.Tensor, str, str]:  - Ground-truth (Real) video tensor of shape [T, C, H, W]. - Generated video tensor of shape [T, C, H, W]. - Ground-truth video name. - Generated video name.</p> Source code in <code>aigve/datasets/fid_dataset.py</code> <pre><code>def __getitem__(self, index) -&gt; tuple[torch.Tensor, torch.Tensor, str, str]:\n    \"\"\"\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, str, str]: \n            - Ground-truth (Real) video tensor of shape [T, C, H, W].\n            - Generated video tensor of shape [T, C, H, W].\n            - Ground-truth video name.\n            - Generated video name.\n    \"\"\"\n    gt_video_name = self.gt_video_names[index]\n    gt_video_path = os.path.join(self.video_dir, gt_video_name)\n    gen_video_name = self.gen_video_names[index]\n    gen_video_path = os.path.join(self.video_dir, gen_video_name) \n\n    gt_video_tensor = self._load_video_tensor(gt_video_path)\n    gen_video_tensor = self._load_video_tensor(gen_video_path)\n\n    return gt_video_tensor, gen_video_tensor, gt_video_name, gen_video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.GSTVQADataset","title":"<code>GSTVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass GSTVQADataset(Dataset):\n    \"\"\"Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.\"\"\"\n\n    def __init__(self, video_dir, prompt_dir, model_name='vgg16', max_len=500):\n        super(GSTVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.model_name = model_name\n        self.max_len = max_len\n        self.feature_extractor = FeatureExtractor(model_name=model_name)\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, index) -&gt; tuple[torch.Tensor, int, str]:\n        \"\"\"\n        Returns a tuple of:\n            deep_features (torch.Tensor): Shape [max_len, 2944]\n                Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n                Padded to self.max_len if the number of frames is less.\n            num_frames (int): The number of frames in the video.\n            video_name (str): The file name for the video.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n        input_frames = []\n\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            # frame = cv2.resize(frame, self.frame_size)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        # Pad or truncate frames to max_len\n        num_frames = len(input_frames)\n        # print('num_frames: ', num_frames)\n        if num_frames &lt; 30:\n            pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n        elif num_frames &lt; self.max_len:\n            pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        else:\n            input_frames_tensor = torch.stack(input_frames[:self.max_len])\n        # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n        # Extract features using the chosen model (VGG16 or ResNet)\n        with torch.no_grad():\n            mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n        # Concatenate to match GSTVQA expected 2944-dim features\n        deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n        # Ensure output shape [max_len, 2944] (pad if needed)\n        if deep_features.shape[0] &lt; self.max_len:\n            pad_size = self.max_len - deep_features.shape[0]\n            padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n            deep_features = torch.cat((deep_features, padding), dim=0)\n\n        return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.GSTVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Returns a tuple of <p>deep_features (torch.Tensor): Shape [max_len, 2944]     Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).     Padded to self.max_len if the number of frames is less. num_frames (int): The number of frames in the video. video_name (str): The file name for the video.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>def __getitem__(self, index) -&gt; tuple[torch.Tensor, int, str]:\n    \"\"\"\n    Returns a tuple of:\n        deep_features (torch.Tensor): Shape [max_len, 2944]\n            Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n            Padded to self.max_len if the number of frames is less.\n        num_frames (int): The number of frames in the video.\n        video_name (str): The file name for the video.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n    input_frames = []\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n\n    while cap.isOpened() and frame_count &lt; self.max_len:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # frame = cv2.resize(frame, self.frame_size)\n        input_frames.append(torch.tensor(frame).float())\n        frame_count += 1\n\n    cap.release()\n\n    # Pad or truncate frames to max_len\n    num_frames = len(input_frames)\n    # print('num_frames: ', num_frames)\n    if num_frames &lt; 30:\n        pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n    elif num_frames &lt; self.max_len:\n        pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n    else:\n        input_frames_tensor = torch.stack(input_frames[:self.max_len])\n    # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n    # Convert from [T, H, W, C] to [T, C, H, W]\n    input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n    # Extract features using the chosen model (VGG16 or ResNet)\n    with torch.no_grad():\n        mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n    # Concatenate to match GSTVQA expected 2944-dim features\n    deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n    # Ensure output shape [max_len, 2944] (pad if needed)\n    if deep_features.shape[0] &lt; self.max_len:\n        pad_size = self.max_len - deep_features.shape[0]\n        padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n        deep_features = torch.cat((deep_features, padding), dim=0)\n\n    return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset","title":"<code>LightVQAPlusDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for LightVQA+. Extracts:     - spatial_features (torch.Tensor): Extracted key frames.     - temporal_features (torch.Tensor): SlowFast motion features.     - BNS_features (torch.Tensor): Brightness &amp; Noise features.     - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass LightVQAPlusDataset(Dataset):\n    \"\"\"\n    Dataset for LightVQA+.\n    Extracts:\n        - spatial_features (torch.Tensor): Extracted key frames.\n        - temporal_features (torch.Tensor): SlowFast motion features.\n        - BNS_features (torch.Tensor): Brightness &amp; Noise features.\n        - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(LightVQAPlusDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.video_names = self._read_video_names()\n\n        # Load CLIP model for BNS and BC features\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n        self.preprocess = transforms.Normalize(\n            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n        )\n        self.to_tensor = transforms.ToTensor()\n\n        # CLIP text prompts\n        self.text_B = clip.tokenize([  # brightness (B)\n            \"an underexposed photo\", \"a slightly underexposed photo\",\n            \"a well-exposed photo\", \"a slightly overexposed photo\", \"an overexposed photo\"\n        ])\n\n        self.text_N = clip.tokenize([ # noise (N)\n            \"a photo with no noise\", \"a photo with little noise\",\n            \"a photo with considerable noise\", \"a photo with serious noise\", \"a photo with extreme noise\"\n        ])\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        # original_path = os.path.join(self.submodel_path, \"Light-VQA-plus\")\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        # if os.path.exists(original_path) and not os.path.exists(lightvqa_path):\n        #     os.rename(original_path, lightvqa_path)\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n        # print(sys.path)\n\n        # Load SlowFast model\n        slowfast, _ = lazy_import()\n        self.slowfast_model = slowfast()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n        return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def extract_key_frames(self, video_path):\n        \"\"\"\n        Extracts 8 evenly spaced key frames across the entire video duration.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path).split('.')[0]\n\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        if video_length &gt;= 8:\n            # Select 8 unique frame indices evenly spaced across the entire video\n            frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n        else:\n            # Select all available frames and repeat the last one to reach 8\n            frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n        spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n        transform = transforms.Compose([\n            transforms.Resize([672, 1120]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        last_valid_frame = None\n        for idx, frame_idx in enumerate(frame_indices):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                spatial_features[idx] = transform(frame)\n                last_valid_frame = spatial_features[idx]\n            elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n                spatial_features[idx] = last_valid_frame\n\n        cap.release()\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n        return spatial_features\n\n    def get_global_sf(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts global brightness &amp; noise features across full video.\n\n        Args:\n            video_path (str): Path to video file.\n\n        Returns:\n            torch.Tensor: Extracted global features (Shape: [8, 150]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        # print('video_length: ', video_length)  # 16\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        length = len(frames)\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        while now + interval - 1 &lt; length:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n\n            # Step 1: Convert to tensor batch\n            images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n            # Step 2: Unfold into patches (Strictly following GET_SF)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n            images = self.preprocess(images)  # Normalize for CLIP\n            # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n            # Step 3: Extract features using CLIP\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N)\n                logits_B, _ = self.clip_model(images, self.text_B)\n\n            tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n            tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n            # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n            # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n            now += interval\n\n        # Handle remaining frames\n        if length &gt; now:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n\n            images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n            images = self.preprocess(images)\n\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n                logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n                # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n                # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n            tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n            # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 150]\n        # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n        # Step 4: Aggregate into 8 time slots\n        chunk_size = length // 8\n        final_res = [\n            torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n            for i in range(8)\n        ]\n\n        return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n\n    def extract_bns_features(self, video_path):\n        \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n        Local Feature Extraction (res1) \u2192 Uses 8 key frames\n        Global Feature Extraction (res2) \u2192 Uses all frames\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n                Shape [8, 3, 672, 1120] containing 8 key frames.\n            final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n        \"\"\"\n        # Local Feature Extraction Step 1: Extract key frames\n        spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n        # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n        images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n        # print(images.device)\n        # print(self.text_N.device)\n\n        # Step 3: Pass through CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n        # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n        res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n        # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n        res1 = torch.cat((res_N, res_B), dim=1)\n        # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n        # Global Feature Extraction (GET_SF Equivalent)\n        res2 = self.get_global_sf(video_path)\n        # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n        # Split &amp; Combine Features\n        Nl, Bl = torch.split(res1, 75, dim=1)\n        Ng, Bg = torch.split(res2, 75, dim=1)\n        final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n        # print('final_res: ', final_res.shape)\n\n        return spatial_features, final_res  # Shape: [8, 300]\n\n    def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n        Returns:\n            torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        length = len(frames)\n\n        # Step 1: Extract CLIP Features at Fixed Intervals\n        while now + interval - 1 &lt; length:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n            now += interval\n\n        # Handle Remaining Frames\n        if length &gt; now:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 5]\n        # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n        # Step 2: Multi-Scale Variance Computation: downsample frames steps\n        # smaller step: Captures fast, fine-grained changes.\n        # larger step:  Captures slow, long-term trends.\n        final_res = []\n        for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n            chunk_number = 8 // step\n            chunk_size = length // chunk_number\n            chunks = []\n            for i in range(chunk_number):\n                if i &lt; chunk_number - 1:\n                    chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n                else:\n                    chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n                tmp = []\n                for j in range(step):\n                    temp = chunk[j::step, :]  \n                    tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n                chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n            final_res.append(chunks) # final final_res len: 4\n\n        # Step 3: Aggregate Multi-Scale Features\n        temp = []\n        for i in range(8):  # Aggregate temporal information across 8 time slots\n            temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                                + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                                + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                                + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                                , dim=0))\n\n        final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n        # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n        return final_res\n\n    def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),  # Match SlowFast input size\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n        ])\n\n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                frames.append(transform(frame))  # Resize &amp; normalize\n        cap.release()\n\n        if len(frames) &lt; 8:\n            raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n        video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n        # Prepare for SlowFast input\n        video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n        video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n        # Pack pathways for SlowFast model\n        _, pack_pathway_output = lazy_import()\n        inputs = pack_pathway_output(video_tensor, device='cpu')\n        # print('inputs len: ', len(inputs))\n        # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n        # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n        # Extract features using SlowFast\n        with torch.no_grad():\n            slow_feature, fast_feature = self.slowfast_model(inputs)\n\n        # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n        # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n        # Concatenate slow and fast features\n        features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n        # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n        return features\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n            bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n            temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n            video_name (str): Video filename.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, bns_features = self.extract_bns_features(video_path)\n        bc_features = self.extract_bc_features(video_path)\n        temporal_features = self.extract_temporal_features(video_path)\n\n        return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Spatial features. Shape: [8, 3, 672, 1120].</p> <code>bns_features</code> <code>Tensor</code> <p>Brightness &amp; Noise features. Shape: [8, 300].</p> <code>bc_features (torch.Tensor</code> <p>Temporal brightness contrast features. Shape: [8, final_dim].)</p> <code>temporal_features</code> <code>Tensor</code> <p>SlowFast motion features. Shape: [1, feature_dim(2304)]</p> <code>video_name</code> <code>str</code> <p>Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n        bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n        (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n        temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n        video_name (str): Video filename.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, bns_features = self.extract_bns_features(video_path)\n    bc_features = self.extract_bc_features(video_path)\n    temporal_features = self.extract_temporal_features(video_path)\n\n    return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_bc_features","title":"<code>extract_bc_features(video_path)</code>","text":"<p>Extracts Brightness Consistency features using CLIP-based temporal processing.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n    Returns:\n        torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    length = len(frames)\n\n    # Step 1: Extract CLIP Features at Fixed Intervals\n    while now + interval - 1 &lt; length:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n        now += interval\n\n    # Handle Remaining Frames\n    if length &gt; now:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 5]\n    # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n    # Step 2: Multi-Scale Variance Computation: downsample frames steps\n    # smaller step: Captures fast, fine-grained changes.\n    # larger step:  Captures slow, long-term trends.\n    final_res = []\n    for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n        chunk_number = 8 // step\n        chunk_size = length // chunk_number\n        chunks = []\n        for i in range(chunk_number):\n            if i &lt; chunk_number - 1:\n                chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n            else:\n                chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n            tmp = []\n            for j in range(step):\n                temp = chunk[j::step, :]  \n                tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n            chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n        final_res.append(chunks) # final final_res len: 4\n\n    # Step 3: Aggregate Multi-Scale Features\n    temp = []\n    for i in range(8):  # Aggregate temporal information across 8 time slots\n        temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                            + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                            + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                            + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                            , dim=0))\n\n    final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n    # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n    return final_res\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_bns_features","title":"<code>extract_bns_features(video_path)</code>","text":"<p>Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP. Local Feature Extraction (res1) \u2192 Uses 8 key frames Global Feature Extraction (res2) \u2192 Uses all frames</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Extracted 8 evenly spaced key frames across the entire video duration. Shape [8, 3, 672, 1120] containing 8 key frames.</p> <code>final_res</code> <code>Tensor</code> <p>Extracted BNS feature (Shape: [8, 300]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bns_features(self, video_path):\n    \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n    Local Feature Extraction (res1) \u2192 Uses 8 key frames\n    Global Feature Extraction (res2) \u2192 Uses all frames\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n            Shape [8, 3, 672, 1120] containing 8 key frames.\n        final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n    \"\"\"\n    # Local Feature Extraction Step 1: Extract key frames\n    spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n    # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n    images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n    images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n    images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n    images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n    images = self.preprocess(images)  # Normalize for CLIP\n    # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n    # print(images.device)\n    # print(self.text_N.device)\n\n    # Step 3: Pass through CLIP\n    with torch.no_grad():\n        logits_N, _ = self.clip_model(images, self.text_N)\n        logits_B, _ = self.clip_model(images, self.text_B)\n\n    res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n    # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n    res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n    # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n    res1 = torch.cat((res_N, res_B), dim=1)\n    # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n    # Global Feature Extraction (GET_SF Equivalent)\n    res2 = self.get_global_sf(video_path)\n    # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n    # Split &amp; Combine Features\n    Nl, Bl = torch.split(res1, 75, dim=1)\n    Ng, Bg = torch.split(res2, 75, dim=1)\n    final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n    # print('final_res: ', final_res.shape)\n\n    return spatial_features, final_res  # Shape: [8, 300]\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_key_frames","title":"<code>extract_key_frames(video_path)</code>","text":"<p>Extracts 8 evenly spaced key frames across the entire video duration.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [8, 3, 672, 1120] containing 8 key frames.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_key_frames(self, video_path):\n    \"\"\"\n    Extracts 8 evenly spaced key frames across the entire video duration.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path).split('.')[0]\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if video_length &gt;= 8:\n        # Select 8 unique frame indices evenly spaced across the entire video\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n    else:\n        # Select all available frames and repeat the last one to reach 8\n        frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n    spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n    transform = transforms.Compose([\n        transforms.Resize([672, 1120]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    last_valid_frame = None\n    for idx, frame_idx in enumerate(frame_indices):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            spatial_features[idx] = transform(frame)\n            last_valid_frame = spatial_features[idx]\n        elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n            spatial_features[idx] = last_valid_frame\n\n    cap.release()\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n    return spatial_features\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_temporal_features","title":"<code>extract_temporal_features(video_path)</code>","text":"<p>Extracts SlowFast motion features on the entire video segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),  # Match SlowFast input size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n    ])\n\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frames.append(transform(frame))  # Resize &amp; normalize\n    cap.release()\n\n    if len(frames) &lt; 8:\n        raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n    video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n    # Prepare for SlowFast input\n    video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n    video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n    # Pack pathways for SlowFast model\n    _, pack_pathway_output = lazy_import()\n    inputs = pack_pathway_output(video_tensor, device='cpu')\n    # print('inputs len: ', len(inputs))\n    # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n    # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n    # Extract features using SlowFast\n    with torch.no_grad():\n        slow_feature, fast_feature = self.slowfast_model(inputs)\n\n    # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n    # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n    # Concatenate slow and fast features\n    features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n    # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n    return features\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.get_global_sf","title":"<code>get_global_sf(video_path)</code>","text":"<p>Extracts global brightness &amp; noise features across full video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted global features (Shape: [8, 150]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def get_global_sf(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts global brightness &amp; noise features across full video.\n\n    Args:\n        video_path (str): Path to video file.\n\n    Returns:\n        torch.Tensor: Extracted global features (Shape: [8, 150]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print('video_length: ', video_length)  # 16\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    length = len(frames)\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    while now + interval - 1 &lt; length:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n\n        # Step 1: Convert to tensor batch\n        images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n        # Step 2: Unfold into patches (Strictly following GET_SF)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n        # Step 3: Extract features using CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n        tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n        # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n        # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n        now += interval\n\n    # Handle remaining frames\n    if length &gt; now:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n\n        images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n        images = self.preprocess(images)\n\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n            logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n            # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n            # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n        tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n        # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 150]\n    # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n    # Step 4: Aggregate into 8 time slots\n    chunk_size = length // 8\n    final_res = [\n        torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n        for i in range(8)\n    ]\n\n    return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset","title":"<code>SimpleVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimpleVQA. Each sample returns:     - spatial_features (torch.Tensor): Extracted spatial frames.     - motion_features (torch.Tensor): Extracted motion-based clips.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass SimpleVQADataset(Dataset):\n    \"\"\"\n    Dataset for SimpleVQA.\n    Each sample returns:\n        - spatial_features (torch.Tensor): Extracted spatial frames.\n        - motion_features (torch.Tensor): Extracted motion-based clips.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(SimpleVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def video_processing_spatial(self, video_path):\n        \"\"\"\n        Extracts spatial frames with proper resizing and normalization.\n            - Key frame extraction: It selects 1 frame per second.\n            - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n        Return:\n            transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n                `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        # Compute the number of total seconds of the video\n        video_length_read = int(video_length/video_frame_rate) # math.ceil()\n        # print('video_length_read (s): ', video_length_read)\n        transformations = transforms.Compose([\n            transforms.Resize(520),\n            transforms.CenterCrop(448),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n        ])\n        transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n        video_read_index = 0\n        frame_idx = 0\n        for i in range(video_length):\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                # Key frames extraction\n                if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                    read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                    read_frame = transformations(read_frame)\n                    transformed_video[video_read_index] = read_frame\n                    video_read_index += 1\n                frame_idx += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; self.min_video_seconds:\n            for i in range(video_read_index, self.min_video_seconds):\n                transformed_video[i] = transformed_video[video_read_index - 1]\n\n        video_capture.release()\n        return transformed_video, video_name\n\n    def video_processing_motion(self, video_path):\n        \"\"\"\n        Extracts motion-based clips suitable for SlowFast.\n            - Standard input size: It resizes frames to 224 * 224.\n            - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n        Return:\n            transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n                Len(List) is total seconds of the video, with minium 8.\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n        ])\n        transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n        video_read_index = 0\n        for i in range(video_length): # All frames extraction\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transform(read_frame)\n                transformed_frame_all[video_read_index] = read_frame\n                video_read_index += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; video_length: \n            for i in range(video_read_index, video_length):\n                transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n        video_capture.release()\n\n        # Compute the number of total seconds of the video\n        video_clip = int(video_length/video_frame_rate)\n        # print('video_clip (s): ', video_clip)\n        video_length_clip = 32\n        transformed_video_all = []\n\n        # Extract motion-based clips: select 32 consecutive frames from each second\n        for i in range(video_clip):\n            transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n            if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n                transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n            else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n                transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n                for j in range((video_length - i * video_frame_rate), video_length_clip):\n                    transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n            transformed_video_all.append(transformed_video)\n\n        if video_clip &lt; self.min_video_seconds:\n            for i in range(video_clip, self.min_video_seconds):\n                transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n        return transformed_video_all, video_name\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            motion_features (List[torch.Tensor]): List of motion feature tensors.\n                Each tensor has shape [32, 3, 224, 224].\n                Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n            video_name (str): Video filename\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, video_name = self.video_processing_spatial(video_path)\n        motion_features, video_name = self.video_processing_motion(video_path)\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n        # print('motion_features len: ', len(motion_features)) # 8\n        # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n        return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [v_len_second, 3, 448, 448] <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).</p> <code>motion_features</code> <code>List[Tensor]</code> <p>List of motion feature tensors. Each tensor has shape [32, 3, 224, 224]. Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).</p> <code>video_name</code> <code>str</code> <p>Video filename</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n            `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        motion_features (List[torch.Tensor]): List of motion feature tensors.\n            Each tensor has shape [32, 3, 224, 224].\n            Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n        video_name (str): Video filename\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, video_name = self.video_processing_spatial(video_path)\n    motion_features, video_name = self.video_processing_motion(video_path)\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n    # print('motion_features len: ', len(motion_features)) # 8\n    # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n    return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.video_processing_motion","title":"<code>video_processing_motion(video_path)</code>","text":"<p>Extracts motion-based clips suitable for SlowFast.     - Standard input size: It resizes frames to 224 * 224.     - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second. Return:     transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224].          Len(List) is total seconds of the video, with minium 8.     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_motion(self, video_path):\n    \"\"\"\n    Extracts motion-based clips suitable for SlowFast.\n        - Standard input size: It resizes frames to 224 * 224.\n        - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n    Return:\n        transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n            Len(List) is total seconds of the video, with minium 8.\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n    ])\n    transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n    video_read_index = 0\n    for i in range(video_length): # All frames extraction\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            read_frame = transform(read_frame)\n            transformed_frame_all[video_read_index] = read_frame\n            video_read_index += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; video_length: \n        for i in range(video_read_index, video_length):\n            transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n    video_capture.release()\n\n    # Compute the number of total seconds of the video\n    video_clip = int(video_length/video_frame_rate)\n    # print('video_clip (s): ', video_clip)\n    video_length_clip = 32\n    transformed_video_all = []\n\n    # Extract motion-based clips: select 32 consecutive frames from each second\n    for i in range(video_clip):\n        transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n        if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n            transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n        else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n            transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n            for j in range((video_length - i * video_frame_rate), video_length_clip):\n                transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n        transformed_video_all.append(transformed_video)\n\n    if video_clip &lt; self.min_video_seconds:\n        for i in range(video_clip, self.min_video_seconds):\n            transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n    return transformed_video_all, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.video_processing_spatial","title":"<code>video_processing_spatial(video_path)</code>","text":"<p>Extracts spatial frames with proper resizing and normalization.     - Key frame extraction: It selects 1 frame per second.     - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520). Return:     transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448].          <code>video_length_read</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_spatial(self, video_path):\n    \"\"\"\n    Extracts spatial frames with proper resizing and normalization.\n        - Key frame extraction: It selects 1 frame per second.\n        - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n    Return:\n        transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n            `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    # Compute the number of total seconds of the video\n    video_length_read = int(video_length/video_frame_rate) # math.ceil()\n    # print('video_length_read (s): ', video_length_read)\n    transformations = transforms.Compose([\n        transforms.Resize(520),\n        transforms.CenterCrop(448),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n    ])\n    transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n    video_read_index = 0\n    frame_idx = 0\n    for i in range(video_length):\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            # Key frames extraction\n            if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transformations(read_frame)\n                transformed_video[video_read_index] = read_frame\n                video_read_index += 1\n            frame_idx += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; self.min_video_seconds:\n        for i in range(video_read_index, self.min_video_seconds):\n            transformed_video[i] = transformed_video[video_read_index - 1]\n\n    video_capture.release()\n    return transformed_video, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.ToyDataset","title":"<code>ToyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>ToyDataset for testing.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory for data.</p> <code>None</code> <code>ann_file</code> <code>str</code> <p>Annotation file path.</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>Metadata information.</p> <code>None</code> <code>data_prefix</code> <code>dict</code> <p>Prefix paths for different modalities.</p> <code>None</code> <code>pipeline</code> <code>List[Union[Callable, dict]]</code> <p>Data transformation pipeline.</p> <code>[]</code> <code>modality</code> <code>dict</code> <p>Specifies which modalities are used (video, text, image).</p> <code>dict(use_video=True, use_text=True, use_image=False)</code> <code>image_frame</code> <code>int</code> <p>Number of frames for images.</p> <code>None</code> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass ToyDataset(BaseDataset):\n    \"\"\"ToyDataset for testing.\n\n    Args:\n        data_root (str, optional): Root directory for data.\n        ann_file (str): Annotation file path.\n        metainfo (dict, optional): Metadata information.\n        data_prefix (dict): Prefix paths for different modalities.\n        pipeline (List[Union[Callable, dict]]): Data transformation pipeline.\n        modality (dict): Specifies which modalities are used (video, text, image).\n        image_frame (int, optional): Number of frames for images.\n    \"\"\"\n\n    def __init__(self,\n                 data_root: Optional[str] = None,\n                 ann_file: str = '',\n                 metainfo: Optional[dict] = None,\n                 data_prefix: dict = None,\n                 pipeline: List[Union[Callable, dict]] = [],\n                 modality: dict = dict(use_video=True, use_text=True, use_image=False),\n                 image_frame: int = None,\n                 **kwargs) -&gt; None:\n        super().__init__(\n            data_root=data_root,\n            ann_file=ann_file,\n            metainfo=metainfo,\n            data_prefix=data_prefix,\n            pipeline=pipeline,\n            **kwargs\n        )\n        self.modality = modality\n        self.image_frame = image_frame\n        assert self.modality['use_video'] or self.modality['use_text'], (\n            'Please specify the `modality` (`use_video` '\n            f', `use_text`) for {self.__class__.__name__}')\n\n    def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n        \"\"\"Parse raw data info.\"\"\"\n        info = {}\n        info['img_frame'] = None\n        if self.modality['use_text']:\n            info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                         raw_data_info['prompt_gt'])\n\n        if self.modality['use_video'] or self.modality['use_image']:\n            info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['video_path_pd'])\n            if self.modality['use_image']:\n                info['img_frame'] = self.image_frame\n\n        return info\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.ToyDataset.parse_data_info","title":"<code>parse_data_info(raw_data_info)</code>","text":"<p>Parse raw data info.</p> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n    \"\"\"Parse raw data info.\"\"\"\n    info = {}\n    info['img_frame'] = None\n    if self.modality['use_text']:\n        info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['prompt_gt'])\n\n    if self.modality['use_video'] or self.modality['use_image']:\n        info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                 raw_data_info['video_path_pd'])\n        if self.modality['use_image']:\n            info['img_frame'] = self.image_frame\n\n    return info\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VbenchDataset","title":"<code>VbenchDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VbenchDataset(BaseDataset):\n    def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n                 serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000,\n                 ):\n        \"\"\"\n        Args:\n            ann_file (str): annotation file path\n            metainfo (dict): meta information about the dataset\n            data_root (str): the root path of the data\n            data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n            filter_cfg (dict): the filter configuration\n            indices (list): the indices of the data\n            serialize_data (bool): whether to serialize the data\n            pipeline (list): the pipeline of the data\n            test_mode (bool): whether in test mode\n            lazy_init (bool): whether to lazy initialize the dataset\n            max_refetch (int): the maximum number of refetching data\n            model_name (str): the name of the model\n\n        \"\"\"\n        super(VbenchDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: the length of the dataset\n        \"\"\"\n        return self.metainfo['length']\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): the index of the data\n        \"\"\"\n        anno_info = self.get_data_info(idx)\n        video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n        prompt = anno_info['prompt_gt']\n\n        inputs = {\n            'video_path': video_path,\n            'prompt': prompt,\n        }\n        return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VbenchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the data</p> required Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): the index of the data\n    \"\"\"\n    anno_info = self.get_data_info(idx)\n    video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n    prompt = anno_info['prompt_gt']\n\n    inputs = {\n        'video_path': video_path,\n        'prompt': prompt,\n    }\n    return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VbenchDataset.__init__","title":"<code>__init__(ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None, serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ann_file</code> <code>str</code> <p>annotation file path</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>meta information about the dataset</p> <code>None</code> <code>data_root</code> <code>str</code> <p>the root path of the data</p> <code>''</code> <code>data_prefix</code> <code>dict</code> <p>the prefix of the data, for example, the prefix of the image path</p> <code>{'video_path_pd': ''}</code> <code>filter_cfg</code> <code>dict</code> <p>the filter configuration</p> <code>None</code> <code>indices</code> <code>list</code> <p>the indices of the data</p> <code>None</code> <code>serialize_data</code> <code>bool</code> <p>whether to serialize the data</p> <code>True</code> <code>pipeline</code> <code>list</code> <p>the pipeline of the data</p> <code>[]</code> <code>test_mode</code> <code>bool</code> <p>whether in test mode</p> <code>False</code> <code>lazy_init</code> <code>bool</code> <p>whether to lazy initialize the dataset</p> <code>False</code> <code>max_refetch</code> <code>int</code> <p>the maximum number of refetching data</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>the name of the model</p> required Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n             serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000,\n             ):\n    \"\"\"\n    Args:\n        ann_file (str): annotation file path\n        metainfo (dict): meta information about the dataset\n        data_root (str): the root path of the data\n        data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n        filter_cfg (dict): the filter configuration\n        indices (list): the indices of the data\n        serialize_data (bool): whether to serialize the data\n        pipeline (list): the pipeline of the data\n        test_mode (bool): whether in test mode\n        lazy_init (bool): whether to lazy initialize the dataset\n        max_refetch (int): the maximum number of refetching data\n        model_name (str): the name of the model\n\n    \"\"\"\n    super(VbenchDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VbenchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the length of the dataset</p> Source code in <code>aigve/datasets/vbench_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: the length of the dataset\n    \"\"\"\n    return self.metainfo['length']\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset","title":"<code>VideoPhyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoPhyDataset(Dataset):\n    def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n        \"\"\"\n        Args:\n            data_path (str): Path to the data folder, it should be a json file\n            tokenizer (Tokenizer): Tokenizer object\n            processor (Processor): Processor object\n            max_length (int): Maximum length of the input sequence\n            media_tokens (list): List of media tokens\n        \"\"\"\n        self.dataset = json.load(open(data_path))\n        self.video_root_path = video_root_path\n\n        self.hf_token = hf_token\n        self.hf_checkpoint = hf_checkpoint\n        self.max_length = max_length\n        self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n        self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n        self.bucket = {}\n\n\n        # initialize tokenizer\n        if tokenizer is not None:\n            self.tokenizer = tokenizer\n        else:\n            self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n        MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n        # initialize processor\n        if processor is not None:\n            self.processor = processor\n        else:\n            self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: Length of the dataset\n        \"\"\"\n        return self.dataset['metainfo']['length']\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the dataset\n        Returns:\n            dict: Dictionary containing the video, text, video path and caption\n        \"\"\"\n        data = self.dataset['dataset_list'][idx]\n        videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n        caption = data['prompt_gt']\n        # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n        video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n        text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n        item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n        return item\n\n    def _extract_text_token_from_conversation(self, data, max_length, index):\n        \"\"\"\n        Extracts the text tokens from the conversation\n        Args:\n            data (str): Conversation\n            max_length (int): Maximum length of the input sequence\n            index (int): Index of the dataset\n        \"\"\"\n        # output enc_chunk\n        enc_chunk = []\n\n        if self.tokenizer.bos_token_id &gt; 0:\n            prompt_chunk = [self.tokenizer.bos_token_id]\n        else:\n            prompt_chunk = []\n\n        # conversation = data[\"completion\"]\n        conversation = data\n\n        # For Text only data\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            prompt_length = -1\n            stop_flag = False\n            for idx, chunk_str in enumerate(chunk_strs):\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                            stop_flag = True\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                            stop_flag = True\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n                    if stop_flag:\n                        break\n\n        # For Image-Text Data\n        else:\n            enc_length = 0\n            prompt_length = -2\n            pattern = '|'.join(\n                map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n            for idx, chunk_str in enumerate(chunk_strs):\n                if enc_length &gt;= max_length + 1:\n                    break\n\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_str in self.media_tokens:\n                        # [CLS] + 256 + [EOS]\n                        if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                            break\n                        else:\n                            enc_chunk += [self.media_tokens[chunk_str]\n                                          ] * self.media_lengths[chunk_str]\n                            enc_length += self.media_lengths[chunk_str]\n                            label_chunk += [0] * self.media_lengths[chunk_str]\n                    else:\n\n                        if chunk_strs[idx - 1] == 'AI: ':\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length:\n                                curr_chunk = curr_chunk[:max_length - enc_length]\n                            curr_chunk += [self.tokenizer.eos_token_id]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [1] * len(curr_chunk)\n                        else:\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                                curr_chunk = curr_chunk[:max_length +\n                                                         1 - enc_length]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [0] * len(curr_chunk)\n\n        if enc_length &lt; max_length + 1:\n            padding_chunk = [self.tokenizer.pad_token_id] * \\\n                            (max_length + 1 - enc_length)\n            padding_length = len(padding_chunk)\n            label_chunk += [0] * (max_length + 1 - enc_length)\n            enc_chunk = enc_chunk + padding_chunk\n        else:\n            padding_length = 0\n\n        assert enc_length + padding_length == max_length + \\\n               1, (index, prompt_length, enc_length,\n                   padding_length, max_length + 1)\n        assert len(label_chunk) == max_length + \\\n               1, (len(label_chunk), max_length + 1)\n        non_padding_mask = [1 if i &lt; enc_length -\n                                 1 else 0 for i in range(max_length)]\n\n        enc_chunk = torch.tensor(enc_chunk).long()\n        non_padding_mask = torch.tensor(non_padding_mask).long()\n        prompt_mask = torch.tensor(label_chunk)[1:].long()\n        prompt_length = torch.tensor([prompt_length]).long()\n\n        # Create loss mask\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            non_media_mask = torch.ones_like(non_padding_mask).long()\n        else:\n            tmp_enc_chunk = enc_chunk.clone()\n            tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n            tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n            non_media_mask = torch.tensor(tmp_enc_chunk).long()\n            non_media_mask = non_media_mask[1:].long()\n        return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n                \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the dataset</p> required <p>Returns:     dict: Dictionary containing the video, text, video path and caption</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): Index of the dataset\n    Returns:\n        dict: Dictionary containing the video, text, video path and caption\n    \"\"\"\n    data = self.dataset['dataset_list'][idx]\n    videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n    caption = data['prompt_gt']\n    # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n    video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n    text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n    item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n    return item\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__init__","title":"<code>__init__(data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data folder, it should be a json file</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>None</code> <code>processor</code> <code>Processor</code> <p>Processor object</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence</p> <code>2048</code> <code>media_tokens</code> <code>list</code> <p>List of media tokens</p> <code>['&lt;image&gt;', '&lt;|video|&gt;']</code> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n    \"\"\"\n    Args:\n        data_path (str): Path to the data folder, it should be a json file\n        tokenizer (Tokenizer): Tokenizer object\n        processor (Processor): Processor object\n        max_length (int): Maximum length of the input sequence\n        media_tokens (list): List of media tokens\n    \"\"\"\n    self.dataset = json.load(open(data_path))\n    self.video_root_path = video_root_path\n\n    self.hf_token = hf_token\n    self.hf_checkpoint = hf_checkpoint\n    self.max_length = max_length\n    self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n    self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n    self.bucket = {}\n\n\n    # initialize tokenizer\n    if tokenizer is not None:\n        self.tokenizer = tokenizer\n    else:\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n    MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n    # initialize processor\n    if processor is not None:\n        self.processor = processor\n    else:\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Length of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: Length of the dataset\n    \"\"\"\n    return self.dataset['metainfo']['length']\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset","title":"<code>VideoScoreDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoScoreDataset(BaseDataset):\n    def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n                 serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n                max_num_frames: int = None):\n        \"\"\"\n        Args:\n            ann_file (str): annotation file path\n            metainfo (dict): meta information about the dataset\n            data_root (str): the root path of the data\n            data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n            filter_cfg (dict): the filter configuration\n            indices (list): the indices of the data\n            serialize_data (bool): whether to serialize the data\n            pipeline (list): the pipeline of the data\n            test_mode (bool): whether in test mode\n            lazy_init (bool): whether to lazy initialize the dataset\n            max_refetch (int): the maximum number of refetching data\n            model_name (str): the name of the model\n            regression_query_prompt (str): the prompt for the regression query\n            max_num_frames (int): the maximum number of frames\n        \"\"\"\n        super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n        if model_name is None:\n            self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n        else:\n            self.model_name = model_name\n\n        self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n        if regression_query_prompt is not None:\n            self.regression_query_prompt = regression_query_prompt\n        else:\n            self.regression_query_prompt = '''\n                Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n                please watch the following frames of a given video and see the text prompt for generating the video,\n                then give scores from 5 different dimensions:\n                (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n                (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n                (3) dynamic degree, the degree of dynamic changes\n                (4) text-to-video alignment, the alignment between the text prompt and the video content\n                (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n                for each dimension, output a float number from 1.0 to 4.0,\n                the higher the number is, the better the video performs in that sub-score, \n                the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n                Here is an output example:\n                visual quality: 3.2\n                temporal consistency: 2.7\n                dynamic degree: 4.0\n                text-to-video alignment: 2.3\n                factual consistency: 1.8\n                For this video, the text prompt is \"{text_prompt}\",\n                all the frames of video are as follows:\n            '''\n        if max_num_frames is not None:\n            self.max_num_frames = max_num_frames\n        else:\n            self.max_num_frames = 48\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: the length of the dataset\n        \"\"\"\n        return self.metainfo['length']\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): the index of the data\n        \"\"\"\n        anno_info = self.get_data_info(idx)\n        video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n        container = av.open(video_path)\n\n        total_frames = container.streams.video[0].frames\n        if total_frames &gt; self.max_num_frames:\n            indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n        else:\n            indices = np.arange(total_frames)\n\n        frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n        eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n        num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n        if num_image_token &lt; len(frames):\n            eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n        flatten_images = []\n        for x in [frames]:\n            if isinstance(x, list):\n                flatten_images.extend(x)\n            else:\n                flatten_images.append(x)\n        flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n        inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n        return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the data</p> required Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): the index of the data\n    \"\"\"\n    anno_info = self.get_data_info(idx)\n    video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n    container = av.open(video_path)\n\n    total_frames = container.streams.video[0].frames\n    if total_frames &gt; self.max_num_frames:\n        indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n    else:\n        indices = np.arange(total_frames)\n\n    frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n    eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n    num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n    if num_image_token &lt; len(frames):\n        eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n    flatten_images = []\n    for x in [frames]:\n        if isinstance(x, list):\n            flatten_images.extend(x)\n        else:\n            flatten_images.append(x)\n    flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n    inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n    return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__init__","title":"<code>__init__(ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None, serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name=None, regression_query_prompt=None, max_num_frames=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ann_file</code> <code>str</code> <p>annotation file path</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>meta information about the dataset</p> <code>None</code> <code>data_root</code> <code>str</code> <p>the root path of the data</p> <code>''</code> <code>data_prefix</code> <code>dict</code> <p>the prefix of the data, for example, the prefix of the image path</p> <code>{'video_path_pd': ''}</code> <code>filter_cfg</code> <code>dict</code> <p>the filter configuration</p> <code>None</code> <code>indices</code> <code>list</code> <p>the indices of the data</p> <code>None</code> <code>serialize_data</code> <code>bool</code> <p>whether to serialize the data</p> <code>True</code> <code>pipeline</code> <code>list</code> <p>the pipeline of the data</p> <code>[]</code> <code>test_mode</code> <code>bool</code> <p>whether in test mode</p> <code>False</code> <code>lazy_init</code> <code>bool</code> <p>whether to lazy initialize the dataset</p> <code>False</code> <code>max_refetch</code> <code>int</code> <p>the maximum number of refetching data</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>the name of the model</p> <code>None</code> <code>regression_query_prompt</code> <code>str</code> <p>the prompt for the regression query</p> <code>None</code> <code>max_num_frames</code> <code>int</code> <p>the maximum number of frames</p> <code>None</code> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n             serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n            max_num_frames: int = None):\n    \"\"\"\n    Args:\n        ann_file (str): annotation file path\n        metainfo (dict): meta information about the dataset\n        data_root (str): the root path of the data\n        data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n        filter_cfg (dict): the filter configuration\n        indices (list): the indices of the data\n        serialize_data (bool): whether to serialize the data\n        pipeline (list): the pipeline of the data\n        test_mode (bool): whether in test mode\n        lazy_init (bool): whether to lazy initialize the dataset\n        max_refetch (int): the maximum number of refetching data\n        model_name (str): the name of the model\n        regression_query_prompt (str): the prompt for the regression query\n        max_num_frames (int): the maximum number of frames\n    \"\"\"\n    super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n    if model_name is None:\n        self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n    else:\n        self.model_name = model_name\n\n    self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n    if regression_query_prompt is not None:\n        self.regression_query_prompt = regression_query_prompt\n    else:\n        self.regression_query_prompt = '''\n            Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n            please watch the following frames of a given video and see the text prompt for generating the video,\n            then give scores from 5 different dimensions:\n            (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n            (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n            (3) dynamic degree, the degree of dynamic changes\n            (4) text-to-video alignment, the alignment between the text prompt and the video content\n            (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n            for each dimension, output a float number from 1.0 to 4.0,\n            the higher the number is, the better the video performs in that sub-score, \n            the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n            Here is an output example:\n            visual quality: 3.2\n            temporal consistency: 2.7\n            dynamic degree: 4.0\n            text-to-video alignment: 2.3\n            factual consistency: 1.8\n            For this video, the text prompt is \"{text_prompt}\",\n            all the frames of video are as follows:\n        '''\n    if max_num_frames is not None:\n        self.max_num_frames = max_num_frames\n    else:\n        self.max_num_frames = 48\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the length of the dataset</p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: the length of the dataset\n    \"\"\"\n    return self.metainfo['length']\n</code></pre>"},{"location":"documentations/metrics/","title":"aigve.metrics","text":"<p>This module provides the videos evaluation metrics that can be used within the AIGVE toolkit.</p>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore","title":"<code>BlipSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>BLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the BLIP model. Defaults to <code>Salesforce/blip-itm-base-coco</code>.</p> <code>'Salesforce/blip-itm-base-coco'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>@METRICS.register_module()\nclass BlipSimScore(BaseMetric):\n    \"\"\" Initialize the ``BLIPSimScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the BLIP model. Defaults to ``Salesforce/blip-itm-base-coco``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"Salesforce/blip-itm-base-coco\",\n                 logit_scale: bool = False,\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = BlipForImageTextRetrieval.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"BLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples  \n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n\n        # Initialize an empty tensor to store the concatenated features\n        blip_score_sum, blip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n                # if isinstance(input_prompt_frame_pair, tuple):\n                #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n                # for key, value in input_prompt_frame_pair.items():\n                #     if isinstance(value, list):\n                #         input_prompt_frame_pair[key] = value[0]\n\n                # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n                # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n                # blip_scores.append(blip_cosine_sim_score)\n                input_prompt = input_prompt.to(self.device)\n                input_frames = input_frames.to(self.device)\n                blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n                blip_cosine_sim_score *= logit_scale\n                print('current blip cosine similarity score', blip_cosine_sim_score)\n                blip_score_sum += blip_cosine_sim_score\n                blip_score_cnt += 1\n\n        # Calculate the average BLIP score across all frames\n        blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n        result['blip_sim_score'] = blip_score_frames_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        blip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            blip_score_np[i] = result['blip_sim_score']\n\n        blip_sim_mean = np.mean(blip_score_np) \n\n        print(\"Test results: blip similarity score={:.4f}\"\n              .format(blip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    blip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        blip_score_np[i] = result['blip_sim_score']\n\n    blip_sim_mean = np.mean(blip_score_np) \n\n    print(\"Test results: blip similarity score={:.4f}\"\n          .format(blip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>BLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"BLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples  \n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n\n    # Initialize an empty tensor to store the concatenated features\n    blip_score_sum, blip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n            # if isinstance(input_prompt_frame_pair, tuple):\n            #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n            # for key, value in input_prompt_frame_pair.items():\n            #     if isinstance(value, list):\n            #         input_prompt_frame_pair[key] = value[0]\n\n            # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n            # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n            # blip_scores.append(blip_cosine_sim_score)\n            input_prompt = input_prompt.to(self.device)\n            input_frames = input_frames.to(self.device)\n            blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n            blip_cosine_sim_score *= logit_scale\n            print('current blip cosine similarity score', blip_cosine_sim_score)\n            blip_score_sum += blip_cosine_sim_score\n            blip_score_cnt += 1\n\n    # Calculate the average BLIP score across all frames\n    blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n    result['blip_sim_score'] = blip_score_frames_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore","title":"<code>CLIPSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                      Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n        processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                Defaults to ``openai/clip-vit-base-patch32``.\n        model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore","title":"<code>CLIPTempScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore","title":"<code>DSGScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>vqa_model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> <code>'InstructBLIP'</code> <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n        vqa_model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n        verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]]): The current evaluated frame.\n\n        Returns:\n            Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n                - 'frame_index' (int): The index of the evaluated frame.\n                - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n                - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n                - 'qid2question' (dict): Mapping of question IDs to actual questions.\n                - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n                - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n                - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n                - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n                - 'average_score_without_dependency' (float): Average score before dependency filtering.\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[int, dict, float]]</code> <p>Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys: - 'frame_index' (int): The index of the evaluated frame. - 'qid2tuple' (dict): Mapping of question IDs to tuples. - 'qid2dependency' (dict): Mapping of question IDs to dependencies. - 'qid2question' (dict): Mapping of question IDs to actual questions. - 'qid2answer' (dict): Mapping of question IDs to predicted answers. - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering. - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering. - 'average_score_with_dependency' (float): Average score considering dependency filtering. - 'average_score_without_dependency' (float): Average score before dependency filtering.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n        qid_list (List[str]): The list of DSG parse question generation results.\n        frame_index (int): The index number of the currently evaluated frame.\n        frame (List[List[float]]): The current evaluated frame.\n\n    Returns:\n        Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n            - 'frame_index' (int): The index of the evaluated frame.\n            - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n            - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n            - 'qid2question' (dict): Mapping of question IDs to actual questions.\n            - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n            - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n            - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n            - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n            - 'average_score_without_dependency' (float): Average score before dependency filtering.\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore","title":"<code>FIDScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>@METRICS.register_module()\nclass FIDScore(BaseMetric):\n\n    def __init__(self, \n                 model_name: str = 'inception_v3', \n                 input_shape: tuple = (299, 299, 3), \n                 is_gpu: str = True):\n        super(FIDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.model_name = model_name\n        self.input_shape = input_shape\n        if self.model_name == \"inception_v3\":\n            self.model = models.inception_v3(pretrained=True, transform_input=False)\n            self.model.fc = nn.Identity()  # Remove classification head\n            self.model.eval().to(self.device)\n        else:\n            raise ValueError(f\"Model '{self.model_name}' is not supported for FID computation.\")\n\n        # Define preprocessing for InceptionV3\n        self.transform = transforms.Compose([\n            transforms.Resize((self.input_shape[0], self.input_shape[1])),  # InceptionV3 input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        video_tensor = self.transform(video_tensor / 255.0)\n        return video_tensor\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate activation statistics (mean and covariance) from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple of mean and covariance matrix.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Calculate FID score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FID score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n        mu2, sigma2 = self.calculate_statistics(fake)\n\n        # Compute FID score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and compute FID.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n                fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FID_Score\": fid_score\n                })\n                print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the final FID score.\"\"\"\n        scores = np.array([res[\"FID_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FID mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FID_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FID mean score saved to {json_file_path}\")\n\n        return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.calculate_fid","title":"<code>calculate_fid(real, fake)</code>","text":"<p>Calculate FID score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Calculate FID score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FID score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n    mu2, sigma2 = self.calculate_statistics(fake)\n\n    # Compute FID score\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Calculate activation statistics (mean and covariance) from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple of mean and covariance matrix.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate activation statistics (mean and covariance) from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple of mean and covariance matrix.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n    mu = features.mean(axis=0)\n    sigma = np.cov(features, rowvar=False)\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the final FID score.\"\"\"\n    scores = np.array([res[\"FID_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FID mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FID_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FID mean score saved to {json_file_path}\")\n\n    return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    video_tensor = self.transform(video_tensor / 255.0)\n    return video_tensor\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and compute FID.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and compute FID.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n            fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FID_Score\": fid_score\n            })\n            print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore","title":"<code>FVDScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Fr\u00e9chet Video Distance (FVD) computation using I3D model. Users can first download the pretrained I3D model from:  https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth Then put in the folder:  AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to pre-trained I3D model.</p> required <code>feature_layer</code> <code>int</code> <p>Layer to extract features from. Default is -2 (penultimate layer).</p> <code>-2</code> <code>is_gpu</code> <code>bool</code> <p>Whether to use GPU. Default is True.</p> <code>True</code> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>@METRICS.register_module()\nclass FVDScore(BaseMetric):\n    \"\"\"\n    Fr\u00e9chet Video Distance (FVD) computation using I3D model.\n    Users can first download the pretrained I3D model from: \n    https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth\n    Then put in the folder: \n    AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/\n\n    Args:\n        model_path (str): Path to pre-trained I3D model.\n        feature_layer (int): Layer to extract features from. Default is -2 (penultimate layer).\n        is_gpu (bool): Whether to use GPU. Default is True.\n    \"\"\"\n    def __init__(self, \n                 model_path: str, \n                 feature_layer: int = -2, \n                 is_gpu: bool = True):\n        super(FVDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu and torch.cuda.is_available() else \"cpu\")\n        self.model = self.load_i3d_model(model_path, feature_layer)\n        self.model.eval()\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),  # I3D input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n        \"\"\"\n        Load a pre-trained I3D model and modify it to extract features.\n\n        Args:\n            model_path (str): Path to the I3D model checkpoint.\n            feature_layer (int): The layer index from which to extract features.\n\n        Returns:\n            torch.nn.Module: I3D feature extraction model.\n        \"\"\"\n        model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n        model.fc = nn.Identity()  # Remove classification head\n\n        if os.path.exists(model_path):\n            model.load_state_dict(torch.load(model_path, map_location=self.device))\n        else:\n            print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n        return model\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        return self.transform(video_tensor / 255.0)\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Extract activation statistics from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        self.model.to(self.device)\n        # Permute to match I3D input format [B, C, T, H, W]\n        video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()\n\n        # print('features: ', features.shape)\n        mu = features.mean(axis=0)\n        # Ensure at least 2 samples to compute covariance\n        if features.shape[0] &gt; 1:\n            sigma = np.cov(features, rowvar=False)\n        else:\n            sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n        return mu, sigma\n\n    def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Compute FVD score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FVD score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n        mu2, sigma2 = self.calculate_statistics(fake)\n        # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n        # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n        # Ensure sigma matrices are at least 2D\n        if sigma1.ndim &lt; 2:\n            sigma1 = np.expand_dims(sigma1, axis=0)\n        if sigma2.ndim &lt; 2:\n            sigma2 = np.expand_dims(sigma2, axis=0)\n\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process a batch of videos and compute FVD.\n\n        Args:\n            data_batch (dict): Not used here.\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n\n                fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FVD_Score\": fvd_score\n                })\n                print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the final FVD score.\n\n        Args:\n            results (list): List of FVD scores for each batch.\n\n        Returns:\n            Dict[str, float]: Dictionary containing mean FVD score.\n        \"\"\"\n        scores = np.array([res[\"FVD_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FVD mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FVD_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FVD mean score saved to {json_file_path}\")\n\n        return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.calculate_fvd","title":"<code>calculate_fvd(real, fake)</code>","text":"<p>Compute FVD score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Compute FVD score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FVD score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n    mu2, sigma2 = self.calculate_statistics(fake)\n    # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n    # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n    # Ensure sigma matrices are at least 2D\n    if sigma1.ndim &lt; 2:\n        sigma1 = np.expand_dims(sigma1, axis=0)\n    if sigma2.ndim &lt; 2:\n        sigma2 = np.expand_dims(sigma2, axis=0)\n\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Extract activation statistics from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract activation statistics from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    self.model.to(self.device)\n    # Permute to match I3D input format [B, C, T, H, W]\n    video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()\n\n    # print('features: ', features.shape)\n    mu = features.mean(axis=0)\n    # Ensure at least 2 samples to compute covariance\n    if features.shape[0] &gt; 1:\n        sigma = np.cov(features, rowvar=False)\n    else:\n        sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FVD score.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>List of FVD scores for each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing mean FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the final FVD score.\n\n    Args:\n        results (list): List of FVD scores for each batch.\n\n    Returns:\n        Dict[str, float]: Dictionary containing mean FVD score.\n    \"\"\"\n    scores = np.array([res[\"FVD_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FVD mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FVD_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FVD mean score saved to {json_file_path}\")\n\n    return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.load_i3d_model","title":"<code>load_i3d_model(model_path, feature_layer)</code>","text":"<p>Load a pre-trained I3D model and modify it to extract features.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the I3D model checkpoint.</p> required <code>feature_layer</code> <code>int</code> <p>The layer index from which to extract features.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: I3D feature extraction model.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n    \"\"\"\n    Load a pre-trained I3D model and modify it to extract features.\n\n    Args:\n        model_path (str): Path to the I3D model checkpoint.\n        feature_layer (int): The layer index from which to extract features.\n\n    Returns:\n        torch.nn.Module: I3D feature extraction model.\n    \"\"\"\n    model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n    model.fc = nn.Identity()  # Remove classification head\n\n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\n    else:\n        print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n    return model\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    return self.transform(video_tensor / 255.0)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FVDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of videos and compute FVD.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>Not used here.</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process a batch of videos and compute FVD.\n\n    Args:\n        data_batch (dict): Not used here.\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n\n            fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FVD_Score\": fvd_score\n            })\n            print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa","title":"<code>GstVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of each three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of each three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of each three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore","title":"<code>ISScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Inception Score (IS) implementation.</p> <p>The Inception Score measures the quality and diversity of generated images by evaluating the KL divergence between the conditional class distribution and the marginal class distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use. Currently only 'inception_v3' is supported.</p> <code>'inception_v3'</code> <code>input_shape</code> <code>tuple</code> <p>Input shape for the model (height, width, channels).</p> <code>(299, 299, 3)</code> <code>splits</code> <code>int</code> <p>Number of splits to use when calculating the score.</p> <code>10</code> <code>is_gpu</code> <code>bool</code> <p>Whether to use GPU. Defaults to True.</p> <code>True</code> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>@METRICS.register_module()\nclass ISScore(BaseMetric):\n    \"\"\"\n    Inception Score (IS) implementation.\n\n    The Inception Score measures the quality and diversity of generated images\n    by evaluating the KL divergence between the conditional class distribution\n    and the marginal class distribution.\n\n    Args:\n        model_name (str): Name of the model to use. Currently only 'inception_v3' is supported.\n        input_shape (tuple): Input shape for the model (height, width, channels).\n        splits (int): Number of splits to use when calculating the score.\n        is_gpu (bool): Whether to use GPU. Defaults to True.\n    \"\"\"\n\n    def __init__(\n            self, \n            model_name: str = 'inception_v3', \n            input_shape: tuple = (299, 299, 3), \n            splits: int = 10,\n            is_gpu: bool = True):\n        super(ISScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu and torch.cuda.is_available() else \"cpu\")\n        self.splits = splits\n\n        if model_name == 'inception_v3':\n            self.model = models.inception_v3(pretrained=True, transform_input=False, aux_logits=True)\n            self.model.eval().to(self.device)\n        else:\n            raise ValueError(f\"Model '{model_name}' is not supported for Inception Score computation.\")\n\n    def preprocess_tensor(self, images: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize images.\n\n        Args:\n            images (torch.Tensor): Tensor of shape [B, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed images.\n        \"\"\"\n        images = nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n        mean = torch.tensor([0.485, 0.456, 0.406], device=images.device).view(1, -1, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225], device=images.device).view(1, -1, 1, 1)\n        images = (images - mean) / std\n        return images\n\n    def compute_inception_features(self, images: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute Inception features for a batch of images.\n\n        Args:\n            images (torch.Tensor): Preprocessed image tensor.\n\n        Returns:\n            torch.Tensor: Feature activations from InceptionV3.\n        \"\"\"\n        images = self.preprocess_tensor(images).to(self.device)\n        with torch.no_grad():\n            output = self.model(images)\n            if isinstance(output, tuple):\n                output = output[0]\n        return output.cpu()\n\n    def calculate_is(self, preds: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate the Inception Score (IS) for a set of predicted class probabilities.\n\n        Args:\n            preds (np.ndarray): Array of predicted softmax probabilities with shape [N, num_classes].\n\n        Returns:\n            (float): Inception Score.\n        \"\"\"\n        kl = preds * (np.log(preds + 1e-10) - np.log(np.expand_dims(np.mean(preds, axis=0), 0) + 1e-10))\n        kl_mean = np.mean(np.sum(kl, axis=1))\n        return float(np.exp(kl_mean))\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and compute IS.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(gen_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                gen_video_name = gen_video_name_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n\n                logits = self.compute_inception_features(gen_tensor)\n                preds = torch.nn.functional.softmax(logits, dim=1).numpy()\n                is_score = self.calculate_is(preds)\n\n                results.append({\n                    \"Generated video_name\": gen_video_name, \n                    \"IS_Score\": is_score,\n                })\n                print(f\"Processed IS score {is_score:.4f} for {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the final IS score.\n\n        Args:\n            results (list): List of IS scores for each batch.\n\n        Returns:\n            Dict[str, float]: Dictionary containing mean IS score and standard deviation.\n        \"\"\"\n        scores = np.array([res[\"IS_Score\"] for res in self.results])\n\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n\n        print(f\"IS mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"is_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"IS_Mean_Score\": mean_score, \n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"IS mean score saved to {json_file_path}\")\n\n        return {\"IS_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.calculate_is","title":"<code>calculate_is(preds)</code>","text":"<p>Calculate the Inception Score (IS) for a set of predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>ndarray</code> <p>Array of predicted softmax probabilities with shape [N, num_classes].</p> required <p>Returns:</p> Type Description <code>float</code> <p>Inception Score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>def calculate_is(self, preds: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the Inception Score (IS) for a set of predicted class probabilities.\n\n    Args:\n        preds (np.ndarray): Array of predicted softmax probabilities with shape [N, num_classes].\n\n    Returns:\n        (float): Inception Score.\n    \"\"\"\n    kl = preds * (np.log(preds + 1e-10) - np.log(np.expand_dims(np.mean(preds, axis=0), 0) + 1e-10))\n    kl_mean = np.mean(np.sum(kl, axis=1))\n    return float(np.exp(kl_mean))\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.compute_inception_features","title":"<code>compute_inception_features(images)</code>","text":"<p>Compute Inception features for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Tensor</code> <p>Preprocessed image tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Feature activations from InceptionV3.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>def compute_inception_features(self, images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute Inception features for a batch of images.\n\n    Args:\n        images (torch.Tensor): Preprocessed image tensor.\n\n    Returns:\n        torch.Tensor: Feature activations from InceptionV3.\n    \"\"\"\n    images = self.preprocess_tensor(images).to(self.device)\n    with torch.no_grad():\n        output = self.model(images)\n        if isinstance(output, tuple):\n            output = output[0]\n    return output.cpu()\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final IS score.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>List of IS scores for each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing mean IS score and standard deviation.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the final IS score.\n\n    Args:\n        results (list): List of IS scores for each batch.\n\n    Returns:\n        Dict[str, float]: Dictionary containing mean IS score and standard deviation.\n    \"\"\"\n    scores = np.array([res[\"IS_Score\"] for res in self.results])\n\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n\n    print(f\"IS mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"is_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"IS_Mean_Score\": mean_score, \n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"IS mean score saved to {json_file_path}\")\n\n    return {\"IS_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.preprocess_tensor","title":"<code>preprocess_tensor(images)</code>","text":"<p>Resize and normalize images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed images.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>def preprocess_tensor(self, images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize images.\n\n    Args:\n        images (torch.Tensor): Tensor of shape [B, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed images.\n    \"\"\"\n    images = nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n    mean = torch.tensor([0.485, 0.456, 0.406], device=images.device).view(1, -1, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225], device=images.device).view(1, -1, 1, 1)\n    images = (images - mean) / std\n    return images\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and compute IS.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and compute IS.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(gen_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            gen_video_name = gen_video_name_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n\n            logits = self.compute_inception_features(gen_tensor)\n            preds = torch.nn.functional.softmax(logits, dim=1).numpy()\n            is_score = self.calculate_is(preds)\n\n            results.append({\n                \"Generated video_name\": gen_video_name, \n                \"IS_Score\": is_score,\n            })\n            print(f\"Processed IS score {is_score:.4f} for {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus","title":"<code>LightVQAPlus</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                A list containing five tuples:\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         A list containing five tuples:         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.         The len of each tuples are the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            A list containing five tuples:\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore","title":"<code>PickScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>PickScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the PickScore model. Defaults to <code>yuvalkirstain/PickScore_v1</code>.</p> <code>'yuvalkirstain/PickScore_v1'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>@METRICS.register_module()\nclass PickScore(BaseMetric):\n    \"\"\" Initialize the ``PickScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the PickScore model. Defaults to ``yuvalkirstain/PickScore_v1``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self, \n                 model_name: str = \"yuvalkirstain/PickScore_v1\", \n                 logit_scale: bool = False) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model =AutoModel.from_pretrained(self.model_name).eval().to(self.device)\n        self.model.eval()\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"PickScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        pickscore_sum, pickscore_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt)\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_features = self.model.get_image_features(input_frames)\n                frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n                pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n                print('current pickscore', pick_score)\n                pickscore_sum += pick_score\n                pickscore_cnt += 1\n\n        # get probabilities if you have multiple images to choose from\n        # probs = torch.softmax(scores, dim=-1)\n        pickscore_total_avg = pickscore_sum/pickscore_cnt\n        result['pick_score'] = pickscore_total_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        pickscore_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            pickscore_np[i] = result['pick_score']\n\n        pickscore_sim_mean = np.mean(pickscore_np) \n\n        print(\"Test results: PickScore={:.4f}\"\n              .format(pickscore_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    pickscore_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        pickscore_np[i] = result['pick_score']\n\n    pickscore_sim_mean = np.mean(pickscore_np) \n\n    print(\"Test results: PickScore={:.4f}\"\n          .format(pickscore_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>PickScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"PickScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    pickscore_sum, pickscore_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt)\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_features = self.model.get_image_features(input_frames)\n            frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n            pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n            print('current pickscore', pick_score)\n            pickscore_sum += pick_score\n            pickscore_cnt += 1\n\n    # get probabilities if you have multiple images to choose from\n    # probs = torch.softmax(scores, dim=-1)\n    pickscore_total_avg = pickscore_sum/pickscore_cnt\n    result['pick_score'] = pickscore_total_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa","title":"<code>SimpleVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore","title":"<code>TIFAScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>AIGVEModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n        openai_key (str): The user's api key of the LLM models openai provides.\n        llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n        unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n        vqa_model_name (str): The name of the ``AIGVEModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.AIGVEModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore","title":"<code>VIEEvalScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>VIEEvalScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_backbone</code> <code>str</code> <p>The name of the LLM model used in the VIEEvalScore evaluator. Defaults to <code>got4o</code>.</p> <code>'gpt4o'</code> <code>api_key_path</code> <code>str</code> <p>The user's api key path to initialize LLM models provides by openai.</p> <code>'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt'</code> <code>task</code> <code>str</code> <p>The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.</p> <code>'t2v'</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>@METRICS.register_module()\nclass VIEEvalScore(BaseMetric):\n    \"\"\" Initialize the ``VIEEvalScore`` evaluator.\n\n    Args:\n        llm_backbone (str): The name of the LLM model used in the VIEEvalScore evaluator. Defaults to ``got4o``.\n        api_key_path (str): The user's api key path to initialize LLM models provides by openai.\n        task (str): The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.\n    \"\"\"\n    def __init__(self,\n                 llm_backbone: str = \"gpt4o\",\n                 api_key_path: str = 'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt',\n                 task: str = 't2v',\n                 ):\n        super().__init__()\n\n        self.api_key_path = api_key_path\n        self.llm_backbone = llm_backbone\n        self.task = task\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/VIE'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/TIGER-AI-Lab/VIEScore.git', \n                submodule_path=self.submodel_path\n            )  \n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )  \n        from .VIEScore.viescore import VIEScore \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n\n        self.vie_score = VIEScore(backbone=self.llm_backbone, task=self.task, key_path=self.api_key_path)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"VIEScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_vie_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            vie_score_list = []\n            for index, frame_path in enumerate(input_video):\n                pil_image = Image.open(frame_path)\n                score_list = self.vie_score.evaluate(pil_image, input_prompt)\n                sementics_score, quality_score, overall_score = score_list\n                vie_score_list.append(overall_score)\n            average_vie_score = sum(vie_score_list)/len(vie_score_list)\n            average_vie_score_list.append(average_vie_score)\n\n        result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        vie_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            vie_score_np[i] = result['vie_score']\n\n        vie_score_np_mean = np.mean(vie_score_np) \n\n        print(\"Test results: vie score with dependency={:.4f}\"\n              .format(vie_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    vie_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        vie_score_np[i] = result['vie_score']\n\n    vie_score_np_mean = np.mean(vie_score_np) \n\n    print(\"Test results: vie score with dependency={:.4f}\"\n          .format(vie_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>VIEScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"VIEScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_vie_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        vie_score_list = []\n        for index, frame_path in enumerate(input_video):\n            pil_image = Image.open(frame_path)\n            score_list = self.vie_score.evaluate(pil_image, input_prompt)\n            sementics_score, quality_score, overall_score = score_list\n            vie_score_list.append(overall_score)\n        average_vie_score = sum(vie_score_list)/len(vie_score_list)\n        average_vie_score_list.append(average_vie_score)\n\n    result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VbenchMetric","title":"<code>VbenchMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>@METRICS.register_module()\nclass VbenchMetric(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n                local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n            eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n            ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n            'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n            'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n            eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n            local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n            read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n            category(str): The category to evaluate on, usage: --category=animal.\n            imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n            3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            Then the center 512 x 512 after resized is used for evaluation.\n            4. 'None': no preprocessing\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.results = []\n        self.vbench_prompt_json_path = vbench_prompt_json_path\n        self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n        self.eval_aspects = eval_aspects\n        self.eval_mode = eval_mode\n        self.local = local\n        self.read_frame = read_frame\n        self.category = category\n        self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n        if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n            video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n            if len(video_roots) &gt; 1:\n                raise ValueError('The video paths should be in the same directory.')\n            else:\n                video_path = video_roots.pop()\n        elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n            video_path = data_batch['video_path'][0]\n        elif type(data_batch['video_path']) == str:\n            video_path = data_batch['video_path']\n        else:\n            raise ValueError('The video paths should be a list or a string.')\n\n\n\n        kwargs = {}\n\n        if self.category != '':\n            kwargs['category'] = self.category\n\n        kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n        result = self.vbench.evaluate(\n            videos_path = video_path,\n            name = f'results_{self.eval_mode}',\n            prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n            dimension_list = self.eval_aspects,\n            local=self.local,\n            read_frame=self.read_frame,\n            mode=self.eval_mode, **kwargs)\n\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        print('results:', results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VbenchMetric.__init__","title":"<code>__init__(collect_device=None, prefix=None, vbench_prompt_json_path=None, eval_aspects=None, eval_mode='vbench_standard', local=False, read_frame=False, category='', imaging_quality_preprocessing_mode='longer', **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>vbench_prompt_json_path</code> <code>str</code> <p>The path to the vbench prompt JSON file.</p> <code>None</code> <code>eval_aspects</code> <code>list</code> <p>the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are</p> <code>None</code> <code>eval_mode</code> <code>str</code> <p>the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']</p> <code>'vbench_standard'</code> <code>local</code> <code>bool</code> <p>whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet</p> <code>False</code> <code>read_frame</code> <code>bool</code> <p>whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video</p> <code>False</code> <code>category(str)</code> <p>The category to evaluate on, usage: --category=animal.</p> required <code>imaging_quality_preprocessing_mode(str)</code> <ol> <li>'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.</li> </ol> required <code>2.</code> <code>longer</code> <p>if the longer side is more than 512, the image is resized so that the longer side is 512.</p> required <code>3.</code> <code>shorter_centercrop</code> <p>if the shorter side is more than 512, the image is resized so that the shorter side is 512.</p> required <code>4.</code> <code>None</code> <p>no preprocessing</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n            local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n        eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n        ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n        'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n        'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n        eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n        local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n        read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n        category(str): The category to evaluate on, usage: --category=animal.\n        imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n        3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        Then the center 512 x 512 after resized is used for evaluation.\n        4. 'None': no preprocessing\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.results = []\n    self.vbench_prompt_json_path = vbench_prompt_json_path\n    self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n    self.eval_aspects = eval_aspects\n    self.eval_mode = eval_mode\n    self.local = local\n    self.read_frame = read_frame\n    self.category = category\n    self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VbenchMetric.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    print('results:', results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VbenchMetric.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n    if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n        video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n        if len(video_roots) &gt; 1:\n            raise ValueError('The video paths should be in the same directory.')\n        else:\n            video_path = video_roots.pop()\n    elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n        video_path = data_batch['video_path'][0]\n    elif type(data_batch['video_path']) == str:\n        video_path = data_batch['video_path']\n    else:\n        raise ValueError('The video paths should be a list or a string.')\n\n\n\n    kwargs = {}\n\n    if self.category != '':\n        kwargs['category'] = self.category\n\n    kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n    result = self.vbench.evaluate(\n        videos_path = video_path,\n        name = f'results_{self.eval_mode}',\n        prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n        dimension_list = self.eval_aspects,\n        local=self.local,\n        read_frame=self.read_frame,\n        mode=self.eval_mode, **kwargs)\n\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy","title":"<code>VideoPhy</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore","title":"<code>VideoScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/metrics/#organization-of-this-module","title":"Organization of this Module","text":""},{"location":"documentations/metrics/#neural-network-based-evaluation-metrics","title":"Neural Network-Based Evaluation Metrics","text":"<ul> <li>GSTVQA</li> <li>SimpleVQA</li> <li>ModularBVQA</li> </ul>"},{"location":"documentations/metrics/#distribution-based-evaluation-metricsn-metrics","title":"Distribution-Based Evaluation Metricsn Metrics","text":"<ul> <li>FID</li> <li>FVD</li> <li>IS Score </li> </ul>"},{"location":"documentations/metrics/#vision-language-similarity-based-evaluation-metrics-metrics","title":"Vision-Language Similarity-Based Evaluation Metrics Metrics","text":"<ul> <li>CLIPSim</li> <li>CLIPTemp</li> <li>BLIPSim</li> <li>Pickscore</li> </ul>"},{"location":"documentations/metrics/#vision-language-understanding-based-evaluation-metrics","title":"Vision-Language Understanding-Based Evaluation Metrics","text":"<ul> <li>VIEScore</li> <li>TIFA</li> <li>DSG</li> </ul>"},{"location":"documentations/metrics/#multi-faceted-evaluation-metrics","title":"Multi-Faceted Evaluation Metrics","text":"<ul> <li>VideoPhy</li> <li>VBench</li> <li>EvalCrafter</li> </ul>"},{"location":"documentations/metrics/blipsim/","title":"class BlipSimScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>BLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the BLIP model. Defaults to <code>Salesforce/blip-itm-base-coco</code>.</p> <code>'Salesforce/blip-itm-base-coco'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>@METRICS.register_module()\nclass BlipSimScore(BaseMetric):\n    \"\"\" Initialize the ``BLIPSimScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the BLIP model. Defaults to ``Salesforce/blip-itm-base-coco``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"Salesforce/blip-itm-base-coco\",\n                 logit_scale: bool = False,\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = BlipForImageTextRetrieval.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"BLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples  \n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n\n        # Initialize an empty tensor to store the concatenated features\n        blip_score_sum, blip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n                # if isinstance(input_prompt_frame_pair, tuple):\n                #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n                # for key, value in input_prompt_frame_pair.items():\n                #     if isinstance(value, list):\n                #         input_prompt_frame_pair[key] = value[0]\n\n                # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n                # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n                # blip_scores.append(blip_cosine_sim_score)\n                input_prompt = input_prompt.to(self.device)\n                input_frames = input_frames.to(self.device)\n                blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n                blip_cosine_sim_score *= logit_scale\n                print('current blip cosine similarity score', blip_cosine_sim_score)\n                blip_score_sum += blip_cosine_sim_score\n                blip_score_cnt += 1\n\n        # Calculate the average BLIP score across all frames\n        blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n        result['blip_sim_score'] = blip_score_frames_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        blip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            blip_score_np[i] = result['blip_sim_score']\n\n        blip_sim_mean = np.mean(blip_score_np) \n\n        print(\"Test results: blip similarity score={:.4f}\"\n              .format(blip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/blipsim/#aigve.metrics.text_video_alignment.similarity_based.blipscore.BlipSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    blip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        blip_score_np[i] = result['blip_sim_score']\n\n    blip_sim_mean = np.mean(blip_score_np) \n\n    print(\"Test results: blip similarity score={:.4f}\"\n          .format(blip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/blipsim/#aigve.metrics.text_video_alignment.similarity_based.blipscore.BlipSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>BLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"BLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples  \n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n\n    # Initialize an empty tensor to store the concatenated features\n    blip_score_sum, blip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n            # if isinstance(input_prompt_frame_pair, tuple):\n            #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n            # for key, value in input_prompt_frame_pair.items():\n            #     if isinstance(value, list):\n            #         input_prompt_frame_pair[key] = value[0]\n\n            # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n            # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n            # blip_scores.append(blip_cosine_sim_score)\n            input_prompt = input_prompt.to(self.device)\n            input_frames = input_frames.to(self.device)\n            blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n            blip_cosine_sim_score *= logit_scale\n            print('current blip cosine similarity score', blip_cosine_sim_score)\n            blip_score_sum += blip_cosine_sim_score\n            blip_score_cnt += 1\n\n    # Calculate the average BLIP score across all frames\n    blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n    result['blip_sim_score'] = blip_score_frames_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/clipsim/","title":"class CLIPSimScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                      Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n        processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                Defaults to ``openai/clip-vit-base-patch32``.\n        model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/clipsim/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/clipsim/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/cliptemp/","title":"class CLIPTempScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/cliptemp/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/cliptemp/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/dsg/","title":"class DSGScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>vqa_model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> <code>'InstructBLIP'</code> <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n        vqa_model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n        verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]]): The current evaluated frame.\n\n        Returns:\n            Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n                - 'frame_index' (int): The index of the evaluated frame.\n                - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n                - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n                - 'qid2question' (dict): Mapping of question IDs to actual questions.\n                - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n                - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n                - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n                - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n                - 'average_score_without_dependency' (float): Average score before dependency filtering.\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[int, dict, float]]</code> <p>Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys: - 'frame_index' (int): The index of the evaluated frame. - 'qid2tuple' (dict): Mapping of question IDs to tuples. - 'qid2dependency' (dict): Mapping of question IDs to dependencies. - 'qid2question' (dict): Mapping of question IDs to actual questions. - 'qid2answer' (dict): Mapping of question IDs to predicted answers. - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering. - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering. - 'average_score_with_dependency' (float): Average score considering dependency filtering. - 'average_score_without_dependency' (float): Average score before dependency filtering.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame) -&gt; Dict[str, Union[int, dict, float]]:\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n        qid_list (List[str]): The list of DSG parse question generation results.\n        frame_index (int): The index number of the currently evaluated frame.\n        frame (List[List[float]]): The current evaluated frame.\n\n    Returns:\n        Dict[str, Union[int, dict, float]]: A dictionary containing evaluation results with the following keys:\n            - 'frame_index' (int): The index of the evaluated frame.\n            - 'qid2tuple' (dict): Mapping of question IDs to tuples.\n            - 'qid2dependency' (dict): Mapping of question IDs to dependencies.\n            - 'qid2question' (dict): Mapping of question IDs to actual questions.\n            - 'qid2answer' (dict): Mapping of question IDs to predicted answers.\n            - 'qid2scores' (dict): Mapping of question IDs to scores before dependency filtering.\n            - 'qid2validity' (dict): Mapping of question IDs to boolean validity after dependency filtering.\n            - 'average_score_with_dependency' (float): Average score considering dependency filtering.\n            - 'average_score_without_dependency' (float): Average score before dependency filtering.\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/fid/","title":"class FID","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>@METRICS.register_module()\nclass FIDScore(BaseMetric):\n\n    def __init__(self, \n                 model_name: str = 'inception_v3', \n                 input_shape: tuple = (299, 299, 3), \n                 is_gpu: str = True):\n        super(FIDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.model_name = model_name\n        self.input_shape = input_shape\n        if self.model_name == \"inception_v3\":\n            self.model = models.inception_v3(pretrained=True, transform_input=False)\n            self.model.fc = nn.Identity()  # Remove classification head\n            self.model.eval().to(self.device)\n        else:\n            raise ValueError(f\"Model '{self.model_name}' is not supported for FID computation.\")\n\n        # Define preprocessing for InceptionV3\n        self.transform = transforms.Compose([\n            transforms.Resize((self.input_shape[0], self.input_shape[1])),  # InceptionV3 input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        video_tensor = self.transform(video_tensor / 255.0)\n        return video_tensor\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate activation statistics (mean and covariance) from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple of mean and covariance matrix.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Calculate FID score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FID score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n        mu2, sigma2 = self.calculate_statistics(fake)\n\n        # Compute FID score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and compute FID.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n                fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FID_Score\": fid_score\n                })\n                print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the final FID score.\"\"\"\n        scores = np.array([res[\"FID_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FID mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FID_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FID mean score saved to {json_file_path}\")\n\n        return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid_metric.FIDScore.calculate_fid","title":"<code>calculate_fid(real, fake)</code>","text":"<p>Calculate FID score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_fid(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Calculate FID score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FID score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[2048], Shape[2048, 2048]\n    mu2, sigma2 = self.calculate_statistics(fake)\n\n    # Compute FID score\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid_metric.FIDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Calculate activation statistics (mean and covariance) from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple of mean and covariance matrix.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate activation statistics (mean and covariance) from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple of mean and covariance matrix.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()  # Extract 2048-d feature vectors\n\n    mu = features.mean(axis=0)\n    sigma = np.cov(features, rowvar=False)\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid_metric.FIDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the final FID score.\"\"\"\n    scores = np.array([res[\"FID_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FID mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fid_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FID_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FID mean score saved to {json_file_path}\")\n\n    return {'FID_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid_metric.FIDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    video_tensor = self.transform(video_tensor / 255.0)\n    return video_tensor\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid_metric.FIDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and compute FID.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and compute FID.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n            fid_score = self.calculate_fid(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FID_Score\": fid_score\n            })\n            print(f\"Processed score {fid_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/fvd/","title":"class FVD","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Fr\u00e9chet Video Distance (FVD) computation using I3D model. Users can first download the pretrained I3D model from:  https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth Then put in the folder:  AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to pre-trained I3D model.</p> required <code>feature_layer</code> <code>int</code> <p>Layer to extract features from. Default is -2 (penultimate layer).</p> <code>-2</code> <code>is_gpu</code> <code>bool</code> <p>Whether to use GPU. Default is True.</p> <code>True</code> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>@METRICS.register_module()\nclass FVDScore(BaseMetric):\n    \"\"\"\n    Fr\u00e9chet Video Distance (FVD) computation using I3D model.\n    Users can first download the pretrained I3D model from: \n    https://github.com/hassony2/kinetics_i3d_pytorch/blob/master/model/model_rgb.pth\n    Then put in the folder: \n    AIGVE_Tool/aigve/metrics/video_quality_assessment/distribution_based/fvd/\n\n    Args:\n        model_path (str): Path to pre-trained I3D model.\n        feature_layer (int): Layer to extract features from. Default is -2 (penultimate layer).\n        is_gpu (bool): Whether to use GPU. Default is True.\n    \"\"\"\n    def __init__(self, \n                 model_path: str, \n                 feature_layer: int = -2, \n                 is_gpu: bool = True):\n        super(FVDScore, self).__init__()\n        self.device = torch.device(\"cuda\" if is_gpu and torch.cuda.is_available() else \"cpu\")\n        self.model = self.load_i3d_model(model_path, feature_layer)\n        self.model.eval()\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),  # I3D input size\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n        \"\"\"\n        Load a pre-trained I3D model and modify it to extract features.\n\n        Args:\n            model_path (str): Path to the I3D model checkpoint.\n            feature_layer (int): The layer index from which to extract features.\n\n        Returns:\n            torch.nn.Module: I3D feature extraction model.\n        \"\"\"\n        model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n        model.fc = nn.Identity()  # Remove classification head\n\n        if os.path.exists(model_path):\n            model.load_state_dict(torch.load(model_path, map_location=self.device))\n        else:\n            print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n        return model\n\n    def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Resize and normalize a video tensor.\n\n        Args:\n            video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n        Returns:\n            torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n        \"\"\"\n        return self.transform(video_tensor / 255.0)\n\n    def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Extract activation statistics from video frames.\n\n        Args:\n            video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n        \"\"\"\n        video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n        self.model.to(self.device)\n        # Permute to match I3D input format [B, C, T, H, W]\n        video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n        with torch.no_grad():\n            features = self.model(video_tensor).cpu().numpy()\n\n        # print('features: ', features.shape)\n        mu = features.mean(axis=0)\n        # Ensure at least 2 samples to compute covariance\n        if features.shape[0] &gt; 1:\n            sigma = np.cov(features, rowvar=False)\n        else:\n            sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n        return mu, sigma\n\n    def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n        \"\"\"\n        Compute FVD score between real and generated videos.\n\n        Args:\n            real (torch.Tensor): Real video tensor [T, C, H, W].\n            fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n        Returns:\n            float: FVD score.\n        \"\"\"\n        mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n        mu2, sigma2 = self.calculate_statistics(fake)\n        # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n        # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n        # Ensure sigma matrices are at least 2D\n        if sigma1.ndim &lt; 2:\n            sigma1 = np.expand_dims(sigma1, axis=0)\n        if sigma2.ndim &lt; 2:\n            sigma2 = np.expand_dims(sigma2, axis=0)\n\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1 @ sigma2)\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process a batch of videos and compute FVD.\n\n        Args:\n            data_batch (dict): Not used here.\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n                A list containing four tuples:\n                - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n                - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n                - A tuple of `real_video_name` (str): Ground-truth video filename.\n                - A tuple of `gen_video_name` (str): Generated video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n        batch_size = len(real_tensor_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                real_video_name = real_video_name_tuple[i]\n                gen_video_name = gen_video_name_tuple[i]\n                real_tensor = real_tensor_tuple[i]\n                gen_tensor = gen_tensor_tuple[i]\n\n                fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n                results.append({\n                    \"Real video_name\": real_video_name, \n                    \"Generated video_name\": gen_video_name, \n                    \"FVD_Score\": fvd_score\n                })\n                print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the final FVD score.\n\n        Args:\n            results (list): List of FVD scores for each batch.\n\n        Returns:\n            Dict[str, float]: Dictionary containing mean FVD score.\n        \"\"\"\n        scores = np.array([res[\"FVD_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"FVD mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n        final_results = {\n            \"video_results\": self.results, \n            \"FVD_Mean_Score\": mean_score\n        }\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"FVD mean score saved to {json_file_path}\")\n\n        return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.calculate_fvd","title":"<code>calculate_fvd(real, fake)</code>","text":"<p>Compute FVD score between real and generated videos.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real video tensor [T, C, H, W].</p> required <code>fake</code> <code>Tensor</code> <p>Generated video tensor [T, C, H, W].</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_fvd(self, real: torch.Tensor, fake: torch.Tensor) -&gt; float:\n    \"\"\"\n    Compute FVD score between real and generated videos.\n\n    Args:\n        real (torch.Tensor): Real video tensor [T, C, H, W].\n        fake (torch.Tensor): Generated video tensor [T, C, H, W].\n\n    Returns:\n        float: FVD score.\n    \"\"\"\n    mu1, sigma1 = self.calculate_statistics(real) # Shape[512], Shape[512, 512]\n    mu2, sigma2 = self.calculate_statistics(fake)\n    # print(f\"mu1 shape: {mu1.shape}, sigma1 shape: {sigma1.shape}\")\n    # print(f\"mu2 shape: {mu2.shape}, sigma2 shape: {sigma2.shape}\")\n\n    # Ensure sigma matrices are at least 2D\n    if sigma1.ndim &lt; 2:\n        sigma1 = np.expand_dims(sigma1, axis=0)\n    if sigma2.ndim &lt; 2:\n        sigma2 = np.expand_dims(sigma2, axis=0)\n\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1 @ sigma2)\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.calculate_statistics","title":"<code>calculate_statistics(video_tensor)</code>","text":"<p>Extract activation statistics from video frames.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Video tensor [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def calculate_statistics(self, video_tensor: torch.Tensor) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract activation statistics from video frames.\n\n    Args:\n        video_tensor (torch.Tensor): Video tensor [T, C, H, W].\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Mean and covariance of extracted features.\n    \"\"\"\n    video_tensor = self.preprocess_tensor(video_tensor).to(self.device)\n    self.model.to(self.device)\n    # Permute to match I3D input format [B, C, T, H, W]\n    video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 3, T, H, W]\n    with torch.no_grad():\n        features = self.model(video_tensor).cpu().numpy()\n\n    # print('features: ', features.shape)\n    mu = features.mean(axis=0)\n    # Ensure at least 2 samples to compute covariance\n    if features.shape[0] &gt; 1:\n        sigma = np.cov(features, rowvar=False)\n    else:\n        sigma = np.zeros((features.shape[1], features.shape[1])) # Identity fallback\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the final FVD score.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>List of FVD scores for each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing mean FVD score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the final FVD score.\n\n    Args:\n        results (list): List of FVD scores for each batch.\n\n    Returns:\n        Dict[str, float]: Dictionary containing mean FVD score.\n    \"\"\"\n    scores = np.array([res[\"FVD_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"FVD mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"fvd_results.json\")\n    final_results = {\n        \"video_results\": self.results, \n        \"FVD_Mean_Score\": mean_score\n    }\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"FVD mean score saved to {json_file_path}\")\n\n    return {\"FVD_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.load_i3d_model","title":"<code>load_i3d_model(model_path, feature_layer)</code>","text":"<p>Load a pre-trained I3D model and modify it to extract features.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the I3D model checkpoint.</p> required <code>feature_layer</code> <code>int</code> <p>The layer index from which to extract features.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: I3D feature extraction model.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def load_i3d_model(self, model_path: str, feature_layer: int) -&gt; torch.nn.Module:\n    \"\"\"\n    Load a pre-trained I3D model and modify it to extract features.\n\n    Args:\n        model_path (str): Path to the I3D model checkpoint.\n        feature_layer (int): The layer index from which to extract features.\n\n    Returns:\n        torch.nn.Module: I3D feature extraction model.\n    \"\"\"\n    model = models.video.r3d_18(pretrained=True)  # Using ResNet3D as an I3D alternative\n    model.fc = nn.Identity()  # Remove classification head\n\n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\n    else:\n        print(f\"Warning: Model checkpoint not found at {model_path}, using default weights.\")\n\n    return model\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.preprocess_tensor","title":"<code>preprocess_tensor(video_tensor)</code>","text":"<p>Resize and normalize a video tensor.</p> <p>Parameters:</p> Name Type Description Default <code>video_tensor</code> <code>Tensor</code> <p>Tensor of shape [T, C, H, W].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed tensor of shape [T, C, H, W].</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def preprocess_tensor(self, video_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and normalize a video tensor.\n\n    Args:\n        video_tensor (torch.Tensor): Tensor of shape [T, C, H, W].\n\n    Returns:\n        torch.Tensor: Preprocessed tensor of shape [T, C, H, W].\n    \"\"\"\n    return self.transform(video_tensor / 255.0)\n</code></pre>"},{"location":"documentations/metrics/fvd/#aigve.metrics.video_quality_assessment.distribution_based.fvd.fvd_metric.FVDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of videos and compute FVD.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>Not used here.</p> required <code>data_samples</code> <code>List[Tuple[Tensor], Tuple[Tensor], Tuple[str], Tuple[str]]</code> <p>A list containing four tuples: - A tuple of <code>real_tensor</code> (torch.Tensor): Real video tensor [T, C, H, W]. - A tuple of <code>gen_tensor</code> (torch.Tensor): Generated video tensor [T, C, H, W]. - A tuple of <code>real_video_name</code> (str): Ground-truth video filename. - A tuple of <code>gen_video_name</code> (str): Generated video filename. The len of each tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd/fvd_metric.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process a batch of videos and compute FVD.\n\n    Args:\n        data_batch (dict): Not used here.\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str], Tuple[str]]):\n            A list containing four tuples:\n            - A tuple of `real_tensor` (torch.Tensor): Real video tensor [T, C, H, W].\n            - A tuple of `gen_tensor` (torch.Tensor): Generated video tensor [T, C, H, W].\n            - A tuple of `real_video_name` (str): Ground-truth video filename.\n            - A tuple of `gen_video_name` (str): Generated video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    real_tensor_tuple, gen_tensor_tuple, real_video_name_tuple, gen_video_name_tuple = data_samples\n\n    batch_size = len(real_tensor_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            real_video_name = real_video_name_tuple[i]\n            gen_video_name = gen_video_name_tuple[i]\n            real_tensor = real_tensor_tuple[i]\n            gen_tensor = gen_tensor_tuple[i]\n\n            fvd_score = self.calculate_fvd(real_tensor, gen_tensor)\n\n            results.append({\n                \"Real video_name\": real_video_name, \n                \"Generated video_name\": gen_video_name, \n                \"FVD_Score\": fvd_score\n            })\n            print(f\"Processed FVD score {fvd_score:.4f} between {real_video_name} and {gen_video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/gstvqa/","title":"class GSTVQA","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of each three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of each three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of each three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/is_score/","title":"class IS Score","text":""},{"location":"documentations/metrics/lightvqaplus/","title":"class LightVQA+","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                A list containing five tuples:\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n                The len of each tuples are the batch size.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/lightvqaplus/#aigve.metrics.video_quality_assessment.nn_based.lightvqa_plus.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/lightvqaplus/#aigve.metrics.video_quality_assessment.nn_based.lightvqa_plus.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         A list containing five tuples:         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.         The len of each tuples are the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            A list containing five tuples:\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n            The len of each tuples are the batch size.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/pickscore/","title":"class PickScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>PickScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the PickScore model. Defaults to <code>yuvalkirstain/PickScore_v1</code>.</p> <code>'yuvalkirstain/PickScore_v1'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>@METRICS.register_module()\nclass PickScore(BaseMetric):\n    \"\"\" Initialize the ``PickScore`` evaluator.\n\n    Args:\n        model_name (str): The name of the PickScore model. Defaults to ``yuvalkirstain/PickScore_v1``.\n        logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n    \"\"\"\n    def __init__(self, \n                 model_name: str = \"yuvalkirstain/PickScore_v1\", \n                 logit_scale: bool = False) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model =AutoModel.from_pretrained(self.model_name).eval().to(self.device)\n        self.model.eval()\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"PickScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        pickscore_sum, pickscore_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt)\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_features = self.model.get_image_features(input_frames)\n                frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n                pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n                print('current pickscore', pick_score)\n                pickscore_sum += pick_score\n                pickscore_cnt += 1\n\n        # get probabilities if you have multiple images to choose from\n        # probs = torch.softmax(scores, dim=-1)\n        pickscore_total_avg = pickscore_sum/pickscore_cnt\n        result['pick_score'] = pickscore_total_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        pickscore_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            pickscore_np[i] = result['pick_score']\n\n        pickscore_sim_mean = np.mean(pickscore_np) \n\n        print(\"Test results: PickScore={:.4f}\"\n              .format(pickscore_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/pickscore/#aigve.metrics.text_video_alignment.similarity_based.pickscore.PickScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    pickscore_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        pickscore_np[i] = result['pick_score']\n\n    pickscore_sim_mean = np.mean(pickscore_np) \n\n    print(\"Test results: PickScore={:.4f}\"\n          .format(pickscore_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/pickscore/#aigve.metrics.text_video_alignment.similarity_based.pickscore.PickScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>PickScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"PickScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    pickscore_sum, pickscore_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt)\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_features = self.model.get_image_features(input_frames)\n            frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n            pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n            print('current pickscore', pick_score)\n            pickscore_sum += pick_score\n            pickscore_cnt += 1\n\n    # get probabilities if you have multiple images to choose from\n    # probs = torch.softmax(scores, dim=-1)\n    pickscore_total_avg = pickscore_sum/pickscore_cnt\n    result['pick_score'] = pickscore_total_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/simplevqa/","title":"class SimpleVQA","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/simplevqa/#aigve.metrics.video_quality_assessment.nn_based.simplevqa.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/simplevqa/#aigve.metrics.video_quality_assessment.nn_based.simplevqa.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/tifa/","title":"class TIFAScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>AIGVEModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n        openai_key (str): The user's api key of the LLM models openai provides.\n        llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n        unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n        vqa_model_name (str): The name of the ``AIGVEModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.AIGVEModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/tifa/#aigve.metrics.text_video_alignment.gpt_based.TIFA.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/tifa/#aigve.metrics.text_video_alignment.gpt_based.TIFA.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/vbench/","title":"class VBench","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>@METRICS.register_module()\nclass VbenchMetric(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n                local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n            eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n            ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n            'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n            'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n            eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n            local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n            read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n            category(str): The category to evaluate on, usage: --category=animal.\n            imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n            3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n            Then the center 512 x 512 after resized is used for evaluation.\n            4. 'None': no preprocessing\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.results = []\n        self.vbench_prompt_json_path = vbench_prompt_json_path\n        self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n        self.eval_aspects = eval_aspects\n        self.eval_mode = eval_mode\n        self.local = local\n        self.read_frame = read_frame\n        self.category = category\n        self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n        if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n            video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n            if len(video_roots) &gt; 1:\n                raise ValueError('The video paths should be in the same directory.')\n            else:\n                video_path = video_roots.pop()\n        elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n            video_path = data_batch['video_path'][0]\n        elif type(data_batch['video_path']) == str:\n            video_path = data_batch['video_path']\n        else:\n            raise ValueError('The video paths should be a list or a string.')\n\n\n\n        kwargs = {}\n\n        if self.category != '':\n            kwargs['category'] = self.category\n\n        kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n        result = self.vbench.evaluate(\n            videos_path = video_path,\n            name = f'results_{self.eval_mode}',\n            prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n            dimension_list = self.eval_aspects,\n            local=self.local,\n            read_frame=self.read_frame,\n            mode=self.eval_mode, **kwargs)\n\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        print('results:', results)\n</code></pre>"},{"location":"documentations/metrics/vbench/#aigve.metrics.multi_aspect_metrics.vbench.vbench_metric.VbenchMetric.__init__","title":"<code>__init__(collect_device=None, prefix=None, vbench_prompt_json_path=None, eval_aspects=None, eval_mode='vbench_standard', local=False, read_frame=False, category='', imaging_quality_preprocessing_mode='longer', **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>vbench_prompt_json_path</code> <code>str</code> <p>The path to the vbench prompt JSON file.</p> <code>None</code> <code>eval_aspects</code> <code>list</code> <p>the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are</p> <code>None</code> <code>eval_mode</code> <code>str</code> <p>the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']</p> <code>'vbench_standard'</code> <code>local</code> <code>bool</code> <p>whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet</p> <code>False</code> <code>read_frame</code> <code>bool</code> <p>whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video</p> <code>False</code> <code>category(str)</code> <p>The category to evaluate on, usage: --category=animal.</p> required <code>imaging_quality_preprocessing_mode(str)</code> <ol> <li>'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.</li> </ol> required <code>2.</code> <code>longer</code> <p>if the longer side is more than 512, the image is resized so that the longer side is 512.</p> required <code>3.</code> <code>shorter_centercrop</code> <p>if the shorter side is more than 512, the image is resized so that the shorter side is 512.</p> required <code>4.</code> <code>None</code> <p>no preprocessing</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            vbench_prompt_json_path: str = None, eval_aspects: List[str] = None, eval_mode: str = 'vbench_standard',\n            local: bool=False, read_frame: bool=False, category:str='', imaging_quality_preprocessing_mode:str='longer', **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        vbench_prompt_json_path (str): The path to the vbench prompt JSON file.\n        eval_aspects (list): the evaluation aspects, if the vbench_prompt_json_path is not None, the available aspects are\n        ['subject_consistency', 'background_consistency', 'temporal_flickering', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality',\n        'object_class', 'multiple_objects', 'human_action', 'color', 'spatial_relationship',\n        'scene', 'temporal_style', 'appearance_style', 'overall_consistency'] if the vbench_prompt_json_path is None, the available aspects are ['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality']\n        eval_mode (str): the evaluation mode, if the vbench_prompt_json_path is not None, the available modes are ['vbench_standard', 'vbench_category'] if the vbench_prompt_json_path is None, the available modes are ['custom_input']\n        local (bool): whether to use local mode, if True, the model will be loaded locally, if False, the model will be loaded from the internet\n        read_frame (bool): whether to read the frame from the video, if True, the model will read the frame from the video, if False, the model will not read the frame from the video\n        category(str): The category to evaluate on, usage: --category=animal.\n        imaging_quality_preprocessing_mode(str): 1. 'shorter': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        2. 'longer': if the longer side is more than 512, the image is resized so that the longer side is 512.\n        3. 'shorter_centercrop': if the shorter side is more than 512, the image is resized so that the shorter side is 512.\n        Then the center 512 x 512 after resized is used for evaluation.\n        4. 'None': no preprocessing\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.results = []\n    self.vbench_prompt_json_path = vbench_prompt_json_path\n    self.vbench = VBenchwithReturn(device=self.device, full_info_dir=self.vbench_prompt_json_path)\n    self.eval_aspects = eval_aspects\n    self.eval_mode = eval_mode\n    self.local = local\n    self.read_frame = read_frame\n    self.category = category\n    self.imaging_quality_preprocessing_mode = imaging_quality_preprocessing_mode\n</code></pre>"},{"location":"documentations/metrics/vbench/#aigve.metrics.multi_aspect_metrics.vbench.vbench_metric.VbenchMetric.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    print('results:', results)\n</code></pre>"},{"location":"documentations/metrics/vbench/#aigve.metrics.multi_aspect_metrics.vbench.vbench_metric.VbenchMetric.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/vbench/vbench_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n    if type(data_batch['video_path']) == list and len(data_batch['video_path']) &gt; 1:\n        video_roots = set([os.path.dirname(video_path) for video_path in data_batch['video_path']])\n        if len(video_roots) &gt; 1:\n            raise ValueError('The video paths should be in the same directory.')\n        else:\n            video_path = video_roots.pop()\n    elif type(data_batch['video_path']) == list and len(data_batch['video_path']) == 1:\n        video_path = data_batch['video_path'][0]\n    elif type(data_batch['video_path']) == str:\n        video_path = data_batch['video_path']\n    else:\n        raise ValueError('The video paths should be a list or a string.')\n\n\n\n    kwargs = {}\n\n    if self.category != '':\n        kwargs['category'] = self.category\n\n    kwargs['imaging_quality_preprocessing_mode'] = self.imaging_quality_preprocessing_mode\n\n    result = self.vbench.evaluate(\n        videos_path = video_path,\n        name = f'results_{self.eval_mode}',\n        prompt_list=data_batch['prompt'], # pass in [] to read prompt from filename\n        dimension_list = self.eval_aspects,\n        local=self.local,\n        read_frame=self.read_frame,\n        mode=self.eval_mode, **kwargs)\n\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/videophy/","title":"class VideoPhy","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/metrics/viescore/","title":"class VideoScore","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/utils/","title":"aigve.utils","text":""},{"location":"documentations/utils/#aigve.utils.LoadVideoFromFile","title":"<code>LoadVideoFromFile</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Load a video from file.</p> <p>Required Keys:</p> <pre><code>- video_path_pd\n</code></pre> Modified Keys <ul> <li>video_pd</li> </ul> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>int, default is -1 Desired output height of the video, unchanged if <code>-1</code> is specified.</p> <code>-1</code> <code>width</code> <code>int</code> <p>int, default is -1 Desired output width of the video, unchanged if <code>-1</code> is specified. See details in: https://github.com/dmlc/decord/blob/master/python/decord/video_reader.py#L18</p> <code>-1</code> Source code in <code>aigve/utils/loading.py</code> <pre><code>@TRANSFORMS.register_module()\nclass LoadVideoFromFile(BaseTransform):\n    \"\"\"Load a video from file.\n\n    Required Keys:\n\n        - video_path_pd\n\n    Modified Keys:\n        - video_pd\n\n    Args:\n        height: int, default is -1\n            Desired output height of the video, unchanged if `-1` is specified.\n        width: int, default is -1\n            Desired output width of the video, unchanged if `-1` is specified.\n            See details in: https://github.com/dmlc/decord/blob/master/python/decord/video_reader.py#L18\n    \"\"\"\n\n    def __init__(self, height: int = -1, width: int = -1):\n        self.height = height\n        self.width = width\n\n\n    def transform(self, results: dict) -&gt; Optional[dict]:\n        \"\"\"Functions to load video. \n        Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'\n\n        The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats.\n        Depending on the format, it processes and extracts frames accordingly.\n\n        Args:\n            results (dict): Result dict from\n                :class:`mmengine.dataset.BaseDataset`.\n\n        Returns:\n            dict: The dict contains loaded video in shape (F, C, H, W) and \n            meta information if needed. F is the number of frames, C is the \n            number of channels, H is the height, and W is the width.\n\n        Raises:\n            - NotImplementedError: If the video format is not supported.\n\n        The function first determines the format of the video file by its extension.\n        For GIFs, it iterates over each frame and converts them to RGB.\n        For PNGs, it reads the single frame, converts it to RGB.\n        For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays.\n        If a data_transform is provided, it is applied to the buffer before converting it to a tensor.\n        Finally, the tensor is permuted to match the expected (F, C, H, W) format.\n        \"\"\"\n\n        video_path = results['video_path_pd']\n        if video_path.endswith('.gif'):\n            frame_ls = []\n            img = Image.open(video_path)\n            for frame in ImageSequence.Iterator(img):\n                frame = frame.convert('RGB')\n                frame = np.array(frame).astype(np.uint8)\n                frame_ls.append(frame)\n            buffer = np.array(frame_ls).astype(np.uint8) # (F, H, W, C), np.uint8\n        elif video_path.endswith('.png'):\n            frame = Image.open(video_path)\n            frame = frame.convert('RGB')\n            frame = np.array(frame).astype(np.uint8)\n            frame_ls = [frame]\n            buffer = np.array(frame_ls) # (1, H, W, C), np.uint8\n        elif video_path.endswith('.mp4'):\n            import decord\n            decord.bridge.set_bridge('native')\n            if self.width and self.height:\n                video_reader = VideoReader(video_path, width=self.width, height=self.height, num_threads=1)\n            else:\n                video_reader = VideoReader(video_path, num_threads=1)\n            frames = video_reader.get_batch(range(len(video_reader)))  # (F, H, W, C), torch.uint8\n            buffer = frames.asnumpy().astype(np.uint8) # (F, H, W, C), np.uint8\n        else:\n            raise NotImplementedError\n\n        frames = torch.Tensor(buffer)\n        frames = frames.permute(0, 3, 1, 2) # (F, C, H, W), torch.uint8\n        results['video_pd'] = frames\n\n        return results\n\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}, '\n                    f'height={self.height}, '\n                    f'width={self.width}')\n</code></pre>"},{"location":"documentations/utils/#aigve.utils.LoadVideoFromFile.transform","title":"<code>transform(results)</code>","text":"<p>Functions to load video.  Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'</p> <p>The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats. Depending on the format, it processes and extracts frames accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Result dict from :class:<code>mmengine.dataset.BaseDataset</code>.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[dict]</code> <p>The dict contains loaded video in shape (F, C, H, W) and </p> <code>Optional[dict]</code> <p>meta information if needed. F is the number of frames, C is the </p> <code>Optional[dict]</code> <p>number of channels, H is the height, and W is the width.</p> <p>Raises:</p> Type Description <code>-NotImplementedError</code> <p>If the video format is not supported.</p> <p>The function first determines the format of the video file by its extension. For GIFs, it iterates over each frame and converts them to RGB. For PNGs, it reads the single frame, converts it to RGB. For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays. If a data_transform is provided, it is applied to the buffer before converting it to a tensor. Finally, the tensor is permuted to match the expected (F, C, H, W) format.</p> Source code in <code>aigve/utils/loading.py</code> <pre><code>def transform(self, results: dict) -&gt; Optional[dict]:\n    \"\"\"Functions to load video. \n    Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'\n\n    The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats.\n    Depending on the format, it processes and extracts frames accordingly.\n\n    Args:\n        results (dict): Result dict from\n            :class:`mmengine.dataset.BaseDataset`.\n\n    Returns:\n        dict: The dict contains loaded video in shape (F, C, H, W) and \n        meta information if needed. F is the number of frames, C is the \n        number of channels, H is the height, and W is the width.\n\n    Raises:\n        - NotImplementedError: If the video format is not supported.\n\n    The function first determines the format of the video file by its extension.\n    For GIFs, it iterates over each frame and converts them to RGB.\n    For PNGs, it reads the single frame, converts it to RGB.\n    For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays.\n    If a data_transform is provided, it is applied to the buffer before converting it to a tensor.\n    Finally, the tensor is permuted to match the expected (F, C, H, W) format.\n    \"\"\"\n\n    video_path = results['video_path_pd']\n    if video_path.endswith('.gif'):\n        frame_ls = []\n        img = Image.open(video_path)\n        for frame in ImageSequence.Iterator(img):\n            frame = frame.convert('RGB')\n            frame = np.array(frame).astype(np.uint8)\n            frame_ls.append(frame)\n        buffer = np.array(frame_ls).astype(np.uint8) # (F, H, W, C), np.uint8\n    elif video_path.endswith('.png'):\n        frame = Image.open(video_path)\n        frame = frame.convert('RGB')\n        frame = np.array(frame).astype(np.uint8)\n        frame_ls = [frame]\n        buffer = np.array(frame_ls) # (1, H, W, C), np.uint8\n    elif video_path.endswith('.mp4'):\n        import decord\n        decord.bridge.set_bridge('native')\n        if self.width and self.height:\n            video_reader = VideoReader(video_path, width=self.width, height=self.height, num_threads=1)\n        else:\n            video_reader = VideoReader(video_path, num_threads=1)\n        frames = video_reader.get_batch(range(len(video_reader)))  # (F, H, W, C), torch.uint8\n        buffer = frames.asnumpy().astype(np.uint8) # (F, H, W, C), np.uint8\n    else:\n        raise NotImplementedError\n\n    frames = torch.Tensor(buffer)\n    frames = frames.permute(0, 3, 1, 2) # (F, C, H, W), torch.uint8\n    results['video_pd'] = frames\n\n    return results\n</code></pre>"},{"location":"documentations/utils/#aigve.utils.read_image_detectron2","title":"<code>read_image_detectron2(file_name, format=None)</code>","text":"<p>Read an image into the given format. Will apply rotation and flipping if the image has such exif information.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>image file path</p> required <code>format</code> <code>str</code> <p>one of the supported image modes in PIL, or \"BGR\" or \"YUV-BT.601\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>an HWC image in the given format, which is 0-255, uint8 for supported image modes in PIL or \"BGR\"; float (0-1 for Y) for YUV-BT.601.</p> Source code in <code>aigve/utils/image_reading.py</code> <pre><code>def read_image_detectron2(file_name, format=None):\n    \"\"\"\n    Read an image into the given format.\n    Will apply rotation and flipping if the image has such exif information.\n\n    Args:\n        file_name (str): image file path\n        format (str): one of the supported image modes in PIL, or \"BGR\" or \"YUV-BT.601\".\n\n    Returns:\n        image (np.ndarray):\n            an HWC image in the given format, which is 0-255, uint8 for\n            supported image modes in PIL or \"BGR\"; float (0-1 for Y) for YUV-BT.601.\n    \"\"\"\n    try:\n        import detectron2\n    except ImportError:\n        print(\"detectron2 is not installed. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"detectron2\"])\n\n        return detectron2.data.detection_utils.read_image(img_src, format=\"BGR\")\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"guides/","title":"Get Started with AIGVE","text":"<ul> <li> <p> About us</p> <p>Get more information about <code>AIGVE</code> and use it in your function learning projects.</p> <p> More Information</p> </li> <li> <p> Installation</p> <p>Install <code>AIGVE</code> with <code>pip</code> and set up the dependency in just minutes.</p> <p> How to Install</p> </li> <li> <p> Quickstart</p> <p>A quickstart tutorial to help you use <code>AIGVE</code> APIs for AIGVE model training.</p> <p> Get Started</p> </li> </ul>"},{"location":"guides/about_us/","title":"About us","text":"<p>AIGVE is a website hosting the documentations, tutorials, examples and the latest updates about the <code>AIGVE</code> library.</p>"},{"location":"guides/about_us/#what-is-aigve","title":"\ud83d\ude80 What is <code>AIGVE</code>?","text":"<p><code>AIGVE</code> (AI Generated Video Evaluation Toolkit) provides a comprehensive and structured evaluation framework for assessing AI-generated video quality developed by the IFM Lab. It integrates multiple evaluation metrics, covering diverse aspects of video evaluation, including neural-network-based assessment, distribution comparison, vision-language alignment, and multi-faceted analysis.</p> <ul> <li>Official Website: https://www.aigve.org/</li> <li>Github Repository: https://github.com/ShaneXiangH/AIGVE_Tool</li> <li>PyPI Package: https://pypi.org/project/aigve/</li> <li>AIGVE-Bench Full Dataset https://huggingface.co/datasets/xiaoliux/AIGVE-Bench</li> <li>IFM Lab https://www.ifmlab.org/</li> </ul> <p></p>"},{"location":"guides/about_us/#citing-us","title":"Citing Us","text":"<p><code>aigve</code> is developed based on the AIGVE-Tool paper from IFM Lab, which can be downloaded via the following links:</p> <ul> <li>AIGVE-Tool Paper (2025): https://arxiv.org/abs/2503.14064</li> </ul> <p>If you find <code>AIGVE</code> library and the AIGVE-Tool papers useful in your work, please cite the papers as follows: <pre><code>@article{xiang2025aigvetoolaigeneratedvideoevaluation,\n      title={AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark}, \n      author={Xinhao Xiang and Xiao Liu and Zizhong Li and Zhuosheng Liu and Jiawei Zhang},\n      year={2025},\n      journal={arXiv preprint arXiv:2503.14064},\n      eprint={2503.14064},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.14064}, \n}\n</code></pre></p>"},{"location":"guides/about_us/#library-organization","title":"Library Organization","text":"Components Descriptions <code>aigve</code> The library for assessing AI-generated video quality <code>aigve.configs</code> a library for parameter configuration and management <code>aigve.core</code> a library for video evaluation process design <code>aigve.datasets</code> a library for dataset loading design <code>aigve.metrics</code> a library for video evaluation metrics design and building <code>aigve.utils</code> a library for utility function definition"},{"location":"guides/about_us/#evaluation-metrics-zoo","title":"Evaluation Metrics Zoo","text":""},{"location":"guides/about_us/#distribution-comparison-based-evaluation-metrics","title":"Distribution Comparison-Based Evaluation Metrics","text":"<p>These metrics assess the quality of generated videos by comparing the distribution of real and generated samples.</p> <ul> <li>\u2705 FID: Frechet Inception Distance (FID) quantifies the similarity between real and generated video feature distributions by measuring the Wasserstein-2 distance.</li> <li>\u2705 FVD: Frechet Video Distance (FVD) extends the FID approach to video domain by leveraging spatio-temporal features extracted from action recognition networks.</li> <li>\u2705 IS: Inception Score (IS) evaluates both the quality and diversity of generated content by analyzing conditional label distributions.</li> </ul>"},{"location":"guides/about_us/#video-only-neural-network-based-evaluation-metrics","title":"Video-only Neural Network-Based Evaluation Metrics","text":"<p>These metrics leverage deep learning models to assess AI-generated video quality based on learned representations.</p> <ul> <li>\u2705 GSTVQA: Generalized Spatio-Temporal VQA (GSTVQA) employs graph-based spatio-temporal analysis to assess video quality.</li> <li>\u2705 SimpleVQA: Simple Video Quality Assessment (Simple-VQA) utilizes deep learning features for no-reference video quality assessment.</li> <li>\u2705 LightVQA+: Light Video Quality Assessment Plus (Light-VQA+) incorporates exposure quality guidance to evaluate video quality.</li> </ul>"},{"location":"guides/about_us/#vision-language-similarity-based-evaluation-metrics","title":"Vision-Language Similarity-Based Evaluation Metrics","text":"<p>These metrics evaluate alignment, similarity, and coherence between visual and textual representations, often using embeddings from models like CLIP and BLIP.</p> <ul> <li>\u2705 CLIPSim: CLIP Similarity (CLIPSim) leverages CLIP embeddings to measure semantic similarity between videos and text.</li> <li>\u2705 CLIPTemp: CLIP Temporal (CLIPTemp) extends CLIPSim by incorporating temporal consistency assessment.</li> <li>\u2705 BLIPSim: Bootstrapped Language-Image Pre-training Similarity (BLIPSim) uses advanced pre-training techniques to improve video-text alignment evaluation.</li> <li>\u2705 Pickscore: PickScore incorporates human preference data to provide more perceptually aligned measurement of video-text matching.</li> </ul>"},{"location":"guides/about_us/#vision-language-understanding-based-evaluation-metrics","title":"Vision-Language Understanding-Based Evaluation Metrics","text":"<p>These metrics assess higher-level understanding, reasoning, and factual consistency in vision-language models.</p> <ul> <li>\u2705 VIEScore: Video Information Evaluation Score (VIEScore) provides explainable assessments of conditional image synthesis.</li> <li>\u2705 TIFA: Text-Image Faithfulness Assessment (TIFA) employs question-answering techniques to evaluate text-to-image alignment.</li> <li>\u2705 DSG: Davidsonian Scene Graph (DSG) improves fine-grained evaluation reliability through advanced scene graph representations.</li> </ul>"},{"location":"guides/about_us/#multi-faceted-evaluation-metrics","title":"Multi-Faceted Evaluation Metrics","text":"<p>These metrics integrate structured, multi-dimensional assessments to provide a holistic benchmarking framework for AI-generated videos.</p> <ul> <li>\u2705 VideoPhy: Video Physics Evaluation (VideoPhy) specifically assesses the physical plausibility of generated videos.</li> <li>\u2705 VideoScore: Video Score (VideoScore) simulates fine-grained human feedback across multiple evaluation dimensions.</li> <li>\u2705 VBench: VBench provides a comprehensive benchmark by combining multiple aspects such as consistency, realism, and alignment into a unified scoring system.</li> </ul>"},{"location":"guides/about_us/#key-features","title":"Key Features","text":"<ul> <li>Multi-Dimensional Evaluation: Covers video coherence, physics, and benchmarking.</li> <li>Open-Source &amp; Customizable: Designed for easy integration.</li> <li>Cutting-Edge AI Assessment: Supports various AI-generated video tasks.</li> </ul>"},{"location":"guides/about_us/#license-copyright","title":"License &amp; Copyright","text":"<p>Copyright \u00a9 2025 IFM Lab. All rights reserved.</p> <ul> <li><code>AIGVE</code> source code is published under the terms of the MIT License. </li> <li><code>AIGVE</code> documentation and the <code>AIGVE-Tool</code> papers are licensed under a Creative Commons Attribution-Share Alike 4.0 Unported License (CC BY-SA 4.0). </li> </ul>"},{"location":"guides/installation/","title":"Installation of the AIGVE Library","text":""},{"location":"guides/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"guides/installation/#python","title":"Python","text":"<p>It is recommended that you use Python 3.10+. You can download and install the latest Python  from the python official website.</p>"},{"location":"guides/installation/#package-manager","title":"Package Manager","text":"<p>To install the <code>aigve</code> binaries, you will need to use pip. </p>"},{"location":"guides/installation/#pip","title":"pip","text":"<p>If you installed Python via Homebrew or the Python website, pip (or pip3) was installed with it.</p> <p>To install pip, you can refer to the pip official website.</p> <p>To upgrade your pip, you can use the following command: <pre><code>python -m pip install --upgrade pip\n</code></pre></p>"},{"location":"guides/installation/#installation","title":"Installation","text":"<p>The <code>aigve</code> library has been published at both PyPI and the project github repository.</p>"},{"location":"guides/installation/#install-from-pypi","title":"Install from PyPI","text":"<p>To install <code>aigve</code> from PyPI, use the following command:</p> <pre><code>pip install aigve\n</code></pre>"},{"location":"guides/installation/#install-from-source-code","title":"Install from Source Code","text":"<p>You can also install <code>aigve</code> from the source code, which has been released at the  project github repository. </p> <p>You can download the public repository either from the project github webpage or via the following command: <pre><code>git clone https://github.com/ShaneXiangH/AIGVE_Tool.git\n</code></pre></p> <p>After entering the downloaded source code directory, <code>aigve</code> can be installed with the following command:</p> <pre><code>python setup.py install\n</code></pre> <p>If you don't have <code>setuptools</code> installed locally, please consider to first install <code>setuptools</code>: <pre><code>pip install setuptools \n</code></pre></p>"},{"location":"guides/installation/#dependency-packages","title":"Dependency Packages","text":"<p>The <code>aigve</code> library is developed based on several dependency packages. </p> <p>Option 1 (recommended): Create a specific conda environment for <code>AIGVE-Tool</code>: <pre><code>conda env create -f environment.yml\nconda activate aigve\nconda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre></p> <p>Option 2: Install dependency packages from requirement.txt:</p> <p>The updated dependency requirement.txt of <code>aigve</code> can be downloaded from the project github repository. After downloading the requirement.txt, you can install all these dependencies with the pip command:</p> install commandrequirement.txt <pre><code>pip install -r requirements.txt\n</code></pre> <pre><code>mmcv==2.2.0\nmmdet==3.3.0\nmmengine==0.10.6\ntransformers==4.46.3\nnumpy==1.23.5\npandas==2.2.3\nsympy==1.13.3\neinops==0.8.1\nsentencepiece==0.2.0\nh5py==3.12.1\nfvcore\nopenai\ngit+https://github.com/openai/CLIP.git\ntqdm\npytorch==2.1.0 \ntorchvision==0.16.0 \ntorchaudio==2.1.0\npytorchvideo\ngdown==5.2.0\ngit+https://github.com/TIGER-AI-Lab/Mantis.git\nscipy==1.14.1\ndecord==0.6.0\n</code></pre>"},{"location":"guides/quick_start/","title":"Quickstart Tutorial","text":"<p>This quickstart tutorial walks you through running a ready-to-use evaluation in AIGVE Toolkit using existing configurations, datasets, and metrics. No customization is needed!</p>"},{"location":"guides/quick_start/#run-existing-aigve-pipelines","title":"Run Existing AIGVE Pipelines","text":"<p>AIGVE provides preconfigured examples so you can get started instantly. You can navigate to the <code>aigve/</code> folder and run:</p> <pre><code>python main_aigve.py {metric_config_file}.py --work-dir {working_dir_path}\n</code></pre> <p>For example, to run the GSTVQA metric under its configuration file:  <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/gstvqa.py --work-dir ./output\n</code></pre></p> <p>There are some other examples:</p> <p>For SimpleVQA: <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/simplevqa.py --work-dir ./output\n</code></pre></p> <p>For LightVQAPlus: <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/lightvqa_plus.py --work-dir ./output\n</code></pre> Note: Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the config (e.g., <code>model_path</code>).</p> <p>Under our AIGVE Toolkit design, these running processes will:</p> <ul> <li> <p>Load the dataset and dataloader</p> </li> <li> <p>Initialize the metric evaluator</p> </li> <li> <p>Run the evaluation loop</p> </li> <li> <p>Save the evaluation results</p> </li> </ul>"},{"location":"guides/quick_start/#check-your-outputs","title":"Check Your Outputs","text":"<p>After the run, check your working directory (e.g., <code>./output</code>) for:</p> <ul> <li> <p>Log files with evaluation summaries</p> </li> <li> <p>Output JSON files containing per-video or aggregate metric results</p> </li> </ul>"},{"location":"guides/quick_start/#tips-for-quickstart","title":"Tips for Quickstart","text":"<ul> <li> <p>Make sure your environment is installed correctly. Follow instructions in installation.</p> </li> <li> <p>Check paths: ensure configuration file path and working directory path are correct.</p> </li> <li> <p>Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files.</p> </li> </ul>"},{"location":"news/","title":"Recent News","text":"<p>Library Release</p> 2025-Mar-20<p>The AIGVE library v0.0.1 was released.</p> <p>Project Launch</p> 2024-Nov<p>The AIGVE project was launched.</p>"},{"location":"news/research/","title":"Latest Research","text":"<p>AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark (March 2025)</p><p>This paper introduces AIGVE-Tool, a unified and modular framework for evaluating AI-generated videos (AIGV) across diverse quality dimensions. As illustrated in Figure 1, AIGVE-Tool features a configuration-driven architecture built upon a novel five-category taxonomy of evaluation metrics, enabling standardized and extensible benchmarking across datasets, models, and modalities. It decouples data handling from metric computation through customizable dataloaders and modular metrics, facilitating seamless integration of new components. Additionally, the authors present AIGVE_Bench, a large-scale benchmark dataset with 2,430 videos and over 21,000 human-annotated scores across nine critical aspects. AIGVE-Tool provides a scalable foundation for advancing fair and comprehensive AIGV evaluation. </p>"},{"location":"news/toolkit/","title":"Library Updates","text":"<ul> <li>2024-11-03: <code>aigve</code> project launched.</li> <li>2025-03-20: <code>aigve</code> v0.0.1 released.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#before-you-begin","title":"Before You Begin","text":"<p>Before reading tutorials in this tab, you may need to first check the Get Started tab to follow the  Installation guidance to install the latest <code>tinybig</code> and its dependency packages. </p> <p>Also you are recommended to read the Quickstart Tutorial to get familiar  with the basic modules and models provided in the AIGVE toolkit before reading tutorials in this tab.</p> <p>The tutorials in this tab are primarily prepared based on  the AIGVE-Tool paper.  We recommend reading that paper first for detailed technical information about the AIGVE toolkit. </p>"},{"location":"tutorials/#tutorial-organization","title":"Tutorial Organization","text":"<p>The tutorials are organized into two sections.</p> <ul> <li> <p> Beginner's Tutorials</p> <p>The Beginner's Tutorials focus on the fundamental concepts, components, modules,  and tasks about the AIGVE model, which are organized as follows: <ol> <li>Tutorials about configuration files.</li> <li>Tutorials about dataset preparation.</li> <li>Tutorials about customizable dataloaders.</li> <li>Tutorials about modular metrics.</li> <li>Tutorial about Running AIGV Evaluations</li> </ol></p> <p> Get started</p> </li> <li> <p> Advanced Tutorials</p> <p>The Advanced Tutorials focus on more advanced topics about AIGVE related to the  design pattern and philosophy of the framework. <ol> <li>Tutorials about the AIGVE Loop.</li> </ol></p> <p> More information...</p> </li> </ul>"},{"location":"tutorials/advanced/","title":"Advanced Tutorials","text":"<p>This page will be updated as new tutorials are posted, so please check back regularly.</p>"},{"location":"tutorials/advanced/#tutorial-organization","title":"Tutorial Organization","text":"Advanced Tutorial ID Tutorial Title Released Date Advanced Tutorial 1 The AIGVE Loop TBD"},{"location":"tutorials/advanced/loop/","title":"Tutorial on the AIGVE Loop","text":""},{"location":"tutorials/beginner/","title":"Beginner's Tutorials","text":"<p>This series of beginner-friendly tutorials will guide you through using the AIGVE Toolkit to evaluate AI-generated videos in a modular, reproducible, and efficient manner.</p> <p>You'll learn how to:</p> <ul> <li> <p>Understand and work with Configuration Files</p> </li> <li> <p>Prepare your own datasets in AIGVE format </p> </li> <li> <p>Customize dataloaders for your own datasets</p> </li> <li> <p>Implement or modify modular evaluation metrics </p> </li> <li> <p>Run evaluations from both predefined and custom pipelines</p> </li> </ul> <p>Whether you're using AIGVE out-of-the-box or tailoring it for your own research, these tutorials will help you get started with confidence.</p> <p>We assume you have correctly installed the latest <code>aigve</code> and its dependency packages already. If you haven't installed them yet, please refer to the installation page for the guidance.</p> <p>This tutorial is prepared based on the  AIGVE-Tool paper.  We also recommend reading that paper first for detailed technical information about <code>aigve</code> toolkit.</p>"},{"location":"tutorials/beginner/#tutorial-organization","title":"Tutorial Organization","text":"Tutorial ID Tutorial Title Released Date Tutorial 0 Quickstart Tutorial March 29, 2025 Tutorial 1 Configuration Files March 25, 2025 Tutorial 2 Dataset Preparation March 26, 2025 Tutorial 3 Customizable Dataloaders March 27, 2025 Tutorial 4 Modular Metrics March 29, 2025 Tutorial 5 Running AIGV Evaluations March 29, 2025"},{"location":"tutorials/beginner/config/","title":"Tutorial on Configuration Files","text":"<p>This tutorial introduces how to configure and organize the modular components of AIGVE from configuration files. AIGVE uses MMEngine's Python-style configuration system. It follows a modular and inheritance-based design, which is convenient to conduct various evaluation experiments. It allows you to define all parameters in one centralized location, enabling easy corresponding data access just like getting values from Python <code>dict</code>. AIGVE's configuration system also supports inheritance, enabling better organization and management of complex configurations.</p>"},{"location":"tutorials/beginner/config/#config-file-content","title":"Config file content","text":"<p>AIGVE uses a modular design, all modules with different functions can be configured through the config. Taking <code>gstqva</code> as an example, we will introduce each field in the config according to different function modules.</p>"},{"location":"tutorials/beginner/config/#default-base-configuration","title":"Default Base configuration","text":"<p>These configurations define the default setup of the MMEngine runner system. This includes specifying the evaluation loop class, the model wrapper, logging levels, and hook behavior. Since these configuration are not specific to a particular evaluation metric, they are defined in the base configuration for better modularity and reusability.</p> <pre><code>from mmengine.config import read_base\nwith read_base():\n    from ._base_.default import *\n</code></pre> <p>where the <code>default</code> config is defined as: </p> <pre><code>from core import AIGVELoop, AIGVEModel\n\ndefault_scope = None\n\nlog_level = 'INFO'\n\nmodel = dict(type=AIGVEModel)\n\ndefault_hooks = None # Execute default hook actions as https://github.com/open-mmlab/mmengine/blob/85c83ba61689907fb1775713622b1b146d82277b/mmengine/runner/runner.py#L1896\n\nval_cfg = dict(type=AIGVELoop)\n</code></pre> <ul> <li> <p><code>default_scope</code> and <code>log_level</code> control default scoping and logging behaviors.</p> </li> <li> <p><code>model</code> is a required component of MMEngine's runner pipeline. Since AIGVE is an evaluation-only framework, it is by default set to <code>AIGVEModel</code>, allowing loaded data batch to flow directly into the evaluator without model-specific logic.</p> </li> <li> <p><code>default_hooks</code> is by default set to <code>None</code> to disable default behaviors.</p> </li> <li> <p><code>val_cfg</code> defines the whole evaluation pipeline behaviors. It is set to <code>AIGVELoop</code>, the core loop class in AIGVE for running evaluations from datasets loading to metric evaluation. For more details about <code>AIGVELoop</code>, please refer to Tutorial on the AIGVE Loop. </p> </li> </ul>"},{"location":"tutorials/beginner/config/#dataset-loader-configuration","title":"Dataset &amp; Loader configuration","text":"<p>Following <code>AIGVELoop</code>, <code>val_dataloader</code> is required for batch data loading. Different AIGVE metrics may require distinct data sources, data loading manners, data sampling strategies, or data preprocessing pipelines. As such, they may use different dataloader classes or set different parameters to build it. All AIGVE customizable dataLoaders are inherited from PyTorch's <code>Dataset</code> class. In <code>gstvqa</code>, its <code>val_dataloader</code> is configured as: </p> <pre><code>from mmengine.dataset import DefaultSampler\nfrom datasets import GSTVQADataset\n\nval_dataloader = dict(\n    batch_size=1,\n    num_workers=4,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type=DefaultSampler, shuffle=False),\n    dataset=dict(\n        type=GSTVQADataset,\n        # video_dir='AIGVE_Tool/aigve/data/toy/evaluate/', # it has 16 frames for each video, each frame is [512, 512, 3]\n        # prompt_dir='AIGVE_Tool/aigve/data/toy/annotations/evaluate.json',\n        video_dir='AIGVE_Tool/aigve/data/AIGVE_Bench/videos_3frame/', # it has 81 frames for each video, each frame is [768, 1360, 3]\n        prompt_dir='AIGVE_Tool/aigve/data/AIGVE_Bench/annotations/test.json',\n        model_name='vgg16',  # User can choose 'vgg16' or 'resnet18'\n        max_len=3,\n    )\n)\n</code></pre> <ul> <li> <p><code>batch_size</code>, <code>num_workers</code>, <code>persistent_workers</code>, <code>drop_last</code>, <code>sampler</code> and <code>dataset</code> are parameters for <code>torch.utils.data.DataLoader</code>. Specifically for <code>dataset</code> parameter, the <code>gstvqa</code> is configured to use <code>GSTVQADataset</code>. </p> </li> <li> <p><code>video_dir</code>, <code>prompt_dir</code>, <code>model_name</code> and <code>max_len</code> are four parameters defined in the <code>GSTVQADataset</code>. </p> </li> <li> <p>For more details about customizing such dataloader, please refer to Tutorial on Customizable Dataloaders. Our <code>AIGVE</code> support loading any dataset formatted using MMFormat JSON annotation file. For more details about preparing dataset into this format, please refer to Tutorial on Dataset Preparation. </p> </li> </ul>"},{"location":"tutorials/beginner/config/#evaluator-configuration","title":"Evaluator configuration","text":"<p>Following <code>AIGVELoop</code>, <code>val_evaluator</code> is required for batch data processing, evaluation, and final metric score computing. Different AIGVE metrics generally will define their own evaluation class, to perform their unique evaluation-related operations, such as model loading, dynamic feature extraction, per-sample evaluation, score aggregation, or flexible computational resource management. All our modular evaluator metrics are inherited from MMEngine's <code>BaseMetric</code> class. In <code>gstvqa</code>, its <code>val_evaluator</code> is configured as: </p> <pre><code>from metrics.video_quality_assessment.nn_based.gstvqa.gstvqa_metric import GstVqa\n\nval_evaluator = dict(\n    type=GstVqa,\n    model_path=\"metrics/video_quality_assessment/nn_based/gstvqa/GSTVQA/TCSVT_Release/GVQA_Release/GVQA_Cross/models/training-all-data-GSTVQA-konvid-EXP0-best\",\n)\n</code></pre> <ul> <li> <p>the <code>gstvqa</code> is configured to use <code>GstVqa</code>. </p> </li> <li> <p>Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files. For example here, please make sure your <code>model_path</code> contains the path of pretrained model you downloaded.</p> </li> <li> <p>For more details about customizing such metric evaluator, please refer to Tutorial on Modular Metrics. </p> </li> </ul>"},{"location":"tutorials/beginner/config/#tips-for-configuration-files","title":"Tips for Configuration Files","text":"<ul> <li> <p>Place shared/default settings in <code>_base_</code> folders and reuse them across multiple configs to avoid duplication and keep configs clean.</p> </li> <li> <p>Ensure paths configurations such as <code>video_dir</code>, <code>prompt_dir</code>, and <code>model_path</code> are correct.</p> </li> <li> <p>Keep each config modular. Separate different functional blocks into logical sections (e.g., dataloader, evaluator, etc.)</p> </li> <li> <p>Use toy-version dataset to test new configs and logics before scaling up.</p> </li> <li> <p>Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files. </p> </li> </ul>"},{"location":"tutorials/beginner/config/#whats-next","title":"What's Next?","text":"<p>After configuring the evaluation pipeline, you can proceed to:</p> <ul> <li> <p>Prepare datasets in MMFormat</p> </li> <li> <p>Customize dataloaders</p> </li> <li> <p>Customize evaluation metrics</p> </li> <li> <p>Run the AIGVE loop on your own metrics or datasets</p> </li> <li> <p>Have a deeper view of the ALGVE Loop</p> </li> </ul>"},{"location":"tutorials/beginner/dataloader/","title":"Tutorial on Customizable Dataloaders","text":"<p>AIGVE supports flexible dataloader design to handle diverse datasets, video formats, and evaluation settings. Each dataloader inherits from PyTorch's Dataset class, and can be easily customized to load videos, extract features, and return evaluation-ready inputs.  This tutorial introduces how to implement and customize dataloaders in AIGVE. Taking <code>GSTVQADataset</code> as an example, we will introduce how to customize a dataloader in AIGVE to support various data-related tasks such as reads video inputs, parses prompts, extracts features, and feeds standardized tensors to the evaluator.</p>"},{"location":"tutorials/beginner/dataloader/#design-overview","title":"Design Overview","text":"<p>Each dataloader in AIGVE follows a modular structure and is designed to support evaluation-only workflows. The core responsibilities of a custom dataloader include:</p> <ul> <li> <p>Loading raw videos or frame sequences</p> </li> <li> <p>Parsing annotations</p> </li> <li> <p>Returning each sample as a Python <code>dict</code> containing all necessary fields for downstream evaluation (e.g., prompt, video tensor, metadata)</p> </li> </ul> <p>AIGVE decouples dataloaders from models and metrics, allowing seamless plug-and-play usage with different evaluation modules.</p>"},{"location":"tutorials/beginner/dataloader/#dataset-base-class","title":"Dataset Base Class","text":"<p>All custom datasets in AIGVE inherit from <code>torch.utils.data.Dataset</code> and need to implement two essential methods:</p> <ul> <li> <p><code>__len__(self)</code>: returns the number of samples.</p> </li> <li> <p><code>__getitem__(self, index)</code>: returns one sample at the given index.</p> </li> </ul> <p>Each dataset class defines its own logic for reading videos, parsing annotations, feature extraction, and returning evaluation-ready outputs.  While AIGVE natively supports datasets formatted using MMFormat-style JSON annotations (see Tutorial on Dataset Preparation), it is compatible with any custom format as long as the dataloader returns the expected sample format for evaluation.</p> <p>A minimal dataloader example that loading from standard AIGVE JSON annotations looks like this:</p> <pre><code>from torch.utils.data import Dataset\nimport torch\nimport os\nimport cv2\nimport json\n\n@DATASETS.register_module()\nclass CustomVideoDataset(Dataset):\n    def __init__(self, video_dir, prompt_dir, max_len=30):\n        super().__init__()\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.max_len = max_len\n\n        # Load annotations\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n        self.video_names = [item['video_path_pd'] for item in read_data['data_list']]\n        self.prompts = [item['prompt_gt'] for item in read_data['data_list']]\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def __getitem__(self, index):\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        # Load video frames as tensor\n        cap = cv2.VideoCapture(video_path)\n        input_frames = []\n        while cap.isOpened() and len(input_frames) &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            input_frames.append(torch.tensor(frame).float())\n        cap.release()\n\n        # Pad to fixed length\n        if len(input_frames) &lt; self.max_len:\n            pad_frames = torch.zeros((self.max_len - len(input_frames), *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        else:\n            input_frames_tensor = torch.stack(input_frames[:self.max_len])\n\n        # Permute shape to [T, C, H, W]\n        input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2)\n\n        return input_frames_tensor, self.prompts[index], video_name\n</code></pre> <p>This structure is highly adaptable and can be extended to support a wide range of dataset types and evaluation scenarios. You can build additional logic into your custom dataset class to support:</p> <ul> <li> <p>Feature extraction using pre-trained backbones</p> </li> <li> <p>Multimodal inputs, such as language prompts, audio tracks, reference videos, or scene metadata</p> </li> <li> <p>Sample-wise metadata returns, including model name, subject, dynamic type, quality tags, etc.</p> </li> <li> <p>Flexible temporal control, such as dynamic frame sampling or resolution normalization</p> </li> <li> <p>Input padding and format conversion, ensuring consistent tensors for evaluators</p> </li> </ul> <p>By modifying only the <code>__getitem__()</code> method and how the annotations are parsed, developers can customize new data modalities and processing pipelines.</p>"},{"location":"tutorials/beginner/dataloader/#returned-outputs","title":"Returned Outputs","text":"<p>Each sample returned by the <code>__getitem__()</code> method should typically include:</p> <ul> <li> <p><code>video</code> (Tensor): a video tensor (e.g., shape <code>[T, C, H, W]</code>)</p> </li> <li> <p><code>prompt</code> (str): text field, often <code>prompt_gt</code> from the annotation file</p> </li> <li> <p>Some metadata: additional information fields such as model_name, subject, dynamic_type, category, etc.</p> </li> <li> <p>Other additional fields added depending on the needs of the downstream evaluator.</p> </li> </ul> <p>With AIGVE's modular design, all outputs returned from the dataloader will be passed into the <code>data_samples</code> argument of the <code>process()</code> function in the metric evaluator. This ensures seamless integration between your dataloader and the evaluation pipeline. For more details on implementing or customizing the <code>process()</code> method, please refer to, please refer to Tutorial on customize evaluation metrics</p>"},{"location":"tutorials/beginner/dataloader/#example-gstvqadataset","title":"Example: GSTVQADataset","text":"<p><code>GSTVQADataset</code> supports dynamic frame selection, flexible video backend loading, and annotation-based control. You can check the implementation from here.</p>"},{"location":"tutorials/beginner/dataloader/#key-features-of-gstvqadataset","title":"Key Features of <code>GSTVQADataset</code>:","text":"<p>The <code>GSTVQADataset</code> showcases how to build a robust dataloader with integrated feature extraction and dynamic preprocessing. Key capabilities include:</p> <ul> <li> <p>Video loading via OpenCV backend, with optional support for frame sampling</p> </li> <li> <p>Frame preprocessing and conversion to PyTorch tensor format</p> </li> <li> <p>Parsing annotations and mapping prompts and video paths accordingly</p> </li> <li> <p>Mean and standard deviation feature extraction using either VGG16 or ResNet18</p> </li> <li> <p>Temporal alignment and zero-padding to a fixed length for batch consistency</p> </li> </ul> <p>After implemented the <code>GSTVQADataset</code>, you could configure it in the configuration file:  <pre><code>from datasets import GSTVQADataset\n\nval_dataloader = dict(\n    batch_size=1,\n    num_workers=4,\n    dataset=dict(\n        type=GSTVQADataset,\n        video_dir='aigve/data/AIGVE_Bench/videos_3frame/',\n        prompt_dir='aigve/data/AIGVE_Bench/annotations/test.json',\n        model_name='vgg16',\n        max_len=3,\n    )\n)\n</code></pre> Please make sure your <code>video_dir</code> contains the visual data, and <code>prompt_dir</code> points to your JSON annotations. </p>"},{"location":"tutorials/beginner/dataloader/#tips-for-customizing-datasets","title":"Tips for Customizing Datasets","text":"<ul> <li> <p>Ensure <code>__getitem__()</code> returns all required fields for the evaluator.</p> </li> <li> <p>Normalize tensor shapes using padding or format conversion.</p> </li> <li> <p>Add support for frame sampling or resizing if needed.</p> </li> <li> <p>Test your dataloader with a toy-version dataset before large-scale use.</p> </li> <li> <p>Some dataloaders may require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files.</p> </li> </ul>"},{"location":"tutorials/beginner/dataloader/#whats-next","title":"What's Next?","text":"<p>After customizing the dataloader under a dataset, you can proceed to:</p> <ul> <li> <p>Customize evaluation metrics</p> </li> <li> <p>Run the AIGVE loop on your own metrics or datasets</p> </li> </ul>"},{"location":"tutorials/beginner/dataset/","title":"Tutorial on Dataset Preparation","text":"<p>AIGVE supports flexible and modular dataset loading, designed for a wide range of evaluation scenarios.  While AIGVE natively supports datasets formatted using the MMFormat annotation schema, it is not limited to this format \u2014 any custom data format can be supported as long as the dataloader returns the fields expected by the evaluator. This tutorial introduces how to prepare and organize datasets for use with AIGVE.  We will use AIGVE-Bench toy dataset as an example to introduce the complete dataset preparation process, including directory structure, annotation format, and data organization following the AIGVE specification.</p>"},{"location":"tutorials/beginner/dataset/#dataset-structure","title":"Dataset Structure","text":"<p>AIGVE expects datasets to be organized into two major components:</p> <ol> <li> <p>Raw Media Files (e.g., videos or images)    These files contain the visual content to be evaluated. <code>AIGVE</code> supoorts both raw videos (.mp4, .avi, etc.) and frame sequences (pre-extracted frames as .mp4, .avi, etc.).</p> </li> <li> <p>Annotation File    A JSON file containing the prompts, ground truth annotations and metadata necessary for evaluation. Depending on the specific metric, additional fields like reference videos or multi-modal inputs may be required.</p> </li> </ol>"},{"location":"tutorials/beginner/dataset/#mmformat-style-json-annotation","title":"MMFormat-Style JSON Annotation","text":"<p>AIGVE standardizes how datasets are defined and used across different evaluation tasks, allowing for flexible training or evaluation within the same framework. The annotation file must be in <code>.json</code> format and must contain two fields:</p> <ul> <li> <p><code>metainfo</code>: a dictionary describing meta information about the dataset.  </p> </li> <li> <p><code>data_list</code>: a list of dictionaries where each item defines a sample and its associated groud truth annotations.</p> </li> </ul> <p>Here is a part of annotation file used in AIGVE-Bench toy dataset:</p> <pre><code>{\n  \"metainfo\": {\n    \"length\": 10,\n    \"split\": \"train\",\n    \"generators\": [\n      \"cogvideox\"\n    ]\n  },\n  \"data_list\": [\n    {\n      \"id\": 0,\n      \"prompt_gt\": \"A vast desert expanse, with golden sands swirling under the shimmering heat of midday sun...\",\n      \"technical_quality\": 4,\n      \"dynamic\": 1,\n      \"consistency\": 5,\n      \"physics\": 5,\n      \"element_presentence\": 5,\n      \"element_quality\": 5,\n      \"action_presentence\": 0,\n      \"action_quality\": 0,\n      \"overall\": 2,\n      \"model\": \"cogvideox\",\n      \"subject\": [\"desert\"],\n      \"dynamic_type\": [\"daylight transition\"],\n      \"detailed_dynamic\": [\"daylight transition\"],\n      \"category\": \"global\",\n      \"video_path_pd\": \"cogvideox_0.mp4\"\n    },\n    {\n      \"id\": 2,\n      \"prompt_gt\": \"An aerial view of a modern city with sleek skyscrapers...\",\n      \"technical_quality\": 4,\n      \"dynamic\": 1,\n      \"consistency\": 5,\n      \"physics\": 3,\n      \"element_presentence\": 5,\n      \"element_quality\": 2,\n      \"action_presentence\": 0,\n      \"action_quality\": 0,\n      \"overall\": 1,\n      \"model\": \"cogvideox\",\n      \"subject\": [\"city\", \"architecture\"],\n      \"dynamic_type\": [\"season transition\", \"camera movement\"],\n      \"detailed_dynamic\": [\"season transition\", \"camera movement\"],\n      \"category\": \"global\",\n      \"video_path_pd\": \"cogvideox_2.mp4\"\n    }\n  ]\n}\n</code></pre> <ul> <li> <p><code>metainfo</code>: general information about the dataset (e.g., total length, split type, video generators used).</p> </li> <li> <p><code>data_list</code>: a list of video samples and their corresponding ground truth annotations.</p> </li> <li> <p>The keys in each entry of <code>data_list</code> could be different for different dataset. <code>AIGVE-Bench</code> is a large-scale, multifaceted benchmark dataset designed to systematically evaluate different video generation models across nine quality dimensions. Its <code>data_list</code> keys entry includes:</p> <ul> <li> <p><code>id</code>: unique identification number of this sample.</p> </li> <li> <p><code>prompt_gt</code>: ground-truth prompt used to generate the video.</p> </li> <li> <p>Multifaceted human annotated performance scores. Such as <code>technical_quality</code>, <code>dynamic</code>, <code>consistency</code>, <code>physics</code>, <code>overall</code>, etc.</p> </li> <li> <p>Classification fields of the video. such as <code>subject</code>, <code>dynamic_type</code>, <code>detailed_dynamic</code>, <code>category</code>, etc.</p> </li> <li> <p><code>video_path_pd</code>: relative path to the generated video file from video generator.</p> </li> </ul> </li> </ul> <p>For more information about <code>AIGVE-Bench</code> dataset, please refer to Section 4 &amp; 5 of the AIGVE-Tool paper.</p>"},{"location":"tutorials/beginner/dataset/#directory-layout","title":"Directory Layout","text":"<p>Your dataset directory should be organized as follows: <pre><code>AIGVE_Tool/\n\u251c\u2500\u2500 aigve/\n\u2502   \u2514\u2500\u2500 data/\n\u2502       \u2514\u2500\u2500 DATASET_NAME/\n\u2502           \u251c\u2500\u2500 DATASET_SPLIT/  # Folder of raw video files or extracted frames\n\u2502           \u2502   \u251c\u2500\u2500 video_001.mp4\n\u2502           \u2502   \u2514\u2500\u2500 video_002.mp4\n\u2502           \u2514\u2500\u2500 annotations/\n\u2502               \u2514\u2500\u2500 ANNOTATION_NAME.json   # JSON annotation file\n</code></pre></p> <p>You could custmoize these folder names:</p> <ul> <li> <p><code>DATASET_NAME</code>, the name of the dataset you are evaluating (e.g., <code>AIGVE_Bench</code>, <code>toy</code>, etc.)</p> </li> <li> <p><code>DATASET_SPLIT</code>, the name of the subfolder containing video clips or frame sequences used in evaluation. You could create multiple splits of the dataset and put them in different subfolders.  </p> </li> <li> <p><code>ANNOTATION_NAME</code>, the name of the annotation file. You could either create multiple annotation file corresponding to different data splits, or a single file including all annotations for various data splits.</p> </li> </ul> <p>Taking an example, AIGVE-Bench toy dataset is orginaized as: <pre><code>AIGVE_Tool/\n\u251c\u2500\u2500 aigve/\n\u2502   \u2514\u2500\u2500 data/\n\u2502       \u2514\u2500\u2500 AIGVE_Bench_toy/\n|           \u251c\u2500\u2500 annotations/\n\u2502           \u2502   \u2514\u2500\u2500 train.json   # JSON annotation file\n|           \u251c\u2500\u2500 videos/  # Folder of raw video files \n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_0.mp4\n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_2.mp4\n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_3.mp4\n\u2502           \u2502   \u2514\u2500\u2500 ...  \n\u2502           \u251c\u2500\u2500 videos_3frame/  # Folder of raw video files extracted in 3 frames for each\n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_0.mp4\n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_2.mp4\n\u2502           \u2502   \u251c\u2500\u2500 cogvideox_3.mp4\n\u2502           \u2502   \u2514\u2500\u2500 ...\n\u2502           \u2514\u2500\u2500 ...   \n</code></pre></p>"},{"location":"tutorials/beginner/dataset/#customizing-annotations-for-your-dataset","title":"Customizing Annotations for Your Dataset","text":"<p>In summary, when you are preparing a new datset, you can consider customizing the following fields:</p> <ul> <li> <p>Prompts. (question or task-specific instruction)</p> </li> <li> <p>Model-specific inputs. (e.g., prompt-based generated videos, precomputed features or conditions)</p> </li> <li> <p>Metadata. (resolution, duration, FPS, modality infomation, etc.)</p> </li> <li> <p>Ground truth scores. (if applicable, for supervised evaluation)</p> </li> <li> <p>The video and annotation folder path. As long as the paths match what is configured (such as <code>video_dir</code> and <code>prompt_dir</code> parameters) in your dataloader configuration. </p> </li> </ul> <p>Your custom dataloader can then parse and use these fields accordingly. For more information about customizing your dataloaders based on your prepared dataset, please refer to Tutorial on Customizable Dataloaders.</p>"},{"location":"tutorials/beginner/dataset/#tips-for-preparing-datasets","title":"Tips for Preparing Datasets","text":"<ul> <li> <p>Ensure video names match those in the annotation file.</p> </li> <li> <p>Validate your JSON file.</p> </li> <li> <p>Keep all paths relative to your project root for portability.</p> </li> <li> <p>Use smaller frame samples or lower resolution data for debugging.</p> </li> </ul>"},{"location":"tutorials/beginner/dataset/#whats-next","title":"What's Next?","text":"<p>After prepared a new dataset, you can proceed to:</p> <ul> <li> <p>Customize dataloaders</p> </li> <li> <p>Customize evaluation metrics</p> </li> </ul>"},{"location":"tutorials/beginner/evaluator/","title":"Tutorial on Modular Metrics","text":"<p>AIGVE provides a modular and extensible metric design to support evaluation across diverse video quality dimensions and tasks. Each evaluation metric inherits from MMEngine's BaseMetric, and is automatically called in the evaluation loop.</p> <p>This tutorial introduces how to implement and integrate custom evaluation metrics into the AIGVE framework. Taking <code>GstVqa</code> as an example, we show how metrics can be plugged into the pipeline and reused across datasets.</p>"},{"location":"tutorials/beginner/evaluator/#design-overview","title":"Design Overview","text":"<p>All metric classes in AIGVE inherit from <code>BaseMetric</code> and implement the following methods:</p> <ul> <li> <p><code>process(self, data_batch, data_samples)</code>: called during evaluation, processes each mini-batch.</p> </li> <li> <p><code>compute_metrics(self, results)</code>: computes final aggregated metrics after all samples are processed.</p> </li> </ul> <p>The <code>AIGVELoop</code> will calls these methods automatically in the evaluation stage.</p> <p>A minimal example of a custom metric class looks like this:</p> <pre><code>from mmengine.evaluator import BaseMetric\nfrom core.registry import METRICS\n\n@METRICS.register_module()\nclass MyMetric(BaseMetric):\n    def __init__(self):\n        super().__init__()\n        self.results = []\n\n    def process(self, data_batch, data_samples):\n        # Extract needed fields from the sample\n        for sample in data_samples:\n            video = sample['video']\n            prompt = sample['prompt']\n            # Do custom processing and scoring\n            score = 1.0  # Placeholder score\n            self.results.append(dict(video_name=sample['video_name'], score=score))\n\n    def compute_metrics(self, results):\n        # Aggregate final scores\n        avg_score = sum([item['score'] for item in self.results]) / len(self.results)\n        return dict(MyScore=avg_score)\n</code></pre> <p>Note: The format of <code>data_samples</code> passed to <code>process()</code> must match the output structure of the dataset's <code>__getitem__()</code> method. For example, if your dataset returns: </p> <p><pre><code>def __getitem__(self, index) -&gt; tuple[torch.Tensor, int, str]:\n    return deep_features, num_frames, video_name\n</code></pre> For the help of our AIGVE Loop, the <code>data_samples</code> in <code>process()</code> will be a list of three tuples (i.e. List[[torch.Tensor], Tuple[int], Tuple[str]]): <pre><code>deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n</code></pre> Each tuple has length equal to the batch size.</p>"},{"location":"tutorials/beginner/evaluator/#example-gstvqa","title":"Example: GstVqa","text":"<p><code>GstVqa</code> is a video-only neural network-based metric which uses a pretrained model to compute quality scores. In its <code>process()</code>, features are loaded and passed through the model to produce scores.  In its <code>compute_metrics()</code>, average scores are reported. You can find full implementation here.</p> <p>After implemented the <code>GstVqa</code>, you could configure it in the configuration file:  <pre><code>from metrics.video_quality_assessment.nn_based.gstvqa.gstvqa_metric import GstVqa\n\nval_evaluator = dict(\n    type=GstVqa,\n    model_path='path/to/gst_vqa_model.pth'\n)\n</code></pre> Note: Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files. For example here, please make sure your <code>model_path</code> contains the path of pretrained model you downloaded.</p>"},{"location":"tutorials/beginner/evaluator/#tips-for-customizing-evaluation-metrics","title":"Tips for Customizing Evaluation Metrics","text":"<ul> <li> <p>Register your metric using <code>@METRICS.register_module()</code>.</p> </li> <li> <p>Implement <code>process()</code> to process each batch and store per-sample results in <code>self.results</code>.</p> </li> <li> <p>Make sure <code>data_samples</code> format corresponds to your dataset's <code>__getitem__()</code> output.</p> </li> <li> <p>You may log or save results in <code>compute_metrics()</code> if needed.</p> </li> <li> <p>Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files.</p> </li> </ul>"},{"location":"tutorials/beginner/evaluator/#whats-next","title":"What's Next?","text":"<p>After customizing the modular metrics, you can proceed to:</p> <ul> <li>Run the AIGVE loop on your own metrics or datasets</li> </ul>"},{"location":"tutorials/beginner/running/","title":"Tutorial on Running AIGV Evaluations","text":"<p>This tutorial explains how to run evaluations using the <code>AIGVE toolkit</code>. Once you have prepared your dataset, customized the dataloader, and implemented your metric module, you can configure and launch the AIGVE evaluation loop with ease.</p>"},{"location":"tutorials/beginner/running/#prerequisites","title":"Prerequisites","text":"<p>Before running an evaluation, make sure:</p> <ul> <li> <p>Your dataset is formatted and organized correctly (Dataset Preparation)</p> </li> <li> <p>Your dataloader is defined and included in a config file (Customizable Dataloaders)</p> </li> <li> <p>Your metric is implemented and registered (Modular Metrics)</p> </li> <li> <p>Your configuration is ready (Configuration Files)</p> </li> </ul>"},{"location":"tutorials/beginner/running/#running-an-evaluation-from-a-configuration-file","title":"Running an Evaluation from a Configuration file","text":"<p>You can navigate to the AIGVE main directory and run your evaluation using the <code>main_aigve.py</code> script with your desired config:</p> <pre><code>python main_aigve.py {metric_config_file}.py --work-dir {working_dir_path}\n</code></pre> <p>For example, to run the GSTVQA metric under its configuration file:  <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/gstvqa.py --work-dir ./output\n</code></pre></p> <p>There are some other examples:</p> <p>For SimpleVQA: <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/simplevqa.py --work-dir ./output\n</code></pre></p> <p>For LightVQAPlus: <pre><code>cd AIGVE_Tool/aigve\npython main_aigve.py AIGVE_Tool/aigve/configs/lightvqa_plus.py --work-dir ./output\n</code></pre></p> <p>Note: Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the config (e.g., <code>model_path</code>).</p> <p>Under our AIGVE Toolkit design, these running processes will:</p> <ul> <li> <p>Load the dataset and dataloader</p> </li> <li> <p>Initialize the metric evaluator</p> </li> <li> <p>Run the evaluation loop</p> </li> <li> <p>Save the evaluation results</p> </li> </ul>"},{"location":"tutorials/beginner/running/#evaluation-output","title":"Evaluation Output","text":"<p>The evaluation output depends on your metric class. Common output includes:</p> <ul> <li> <p>Console logs (e.g., mean scores, per-sample results)</p> </li> <li> <p>Saved JSON file with results (e.g., <code>output_results.json</code>)</p> </li> </ul>"},{"location":"tutorials/beginner/running/#tips-for-running-aigv-evaluations","title":"Tips for Running AIGV Evaluations","text":"<ul> <li> <p>Start small: use toy version data and small configs to verify your setup.</p> </li> <li> <p>Check paths: ensure configuration file path and working directory path are correct.</p> </li> <li> <p>Some metrics require downloading pretrained models manually. Make sure they are downloaded correctly and placed in the correct paths as specified in the configuration files.</p> </li> </ul>"},{"location":"tutorials/beginner/running/#whats-next","title":"What's Next?","text":"<p>Once your evaluation runs successfully, you can:</p> <ul> <li> <p>Try running different existing metrics (you can choose from here)</p> </li> <li> <p>Benchmark on our AIGVE-Bench dataset (toy version is in here, the full version will publish soon!)</p> </li> <li> <p>Create your new AIGVE metrics following Tutorial on customize evaluation metrics</p> </li> </ul> <p>Enjoy using AIGVE Toolkit for robust, modular, and reproducible AIGV evaluation!</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/","title":"Index","text":""},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_1","title":"\u6587\u7ae0\u6807\u9898","text":""},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#aigve-tool-ai-generated-video-evaluation-toolkit-with-multifaceted-benchmark","title":"AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark","text":""},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_2","title":"\u6587\u7ae0, \u7f51\u7ad9, \u548c\u6e90\u7801\u94fe\u63a5","text":"<ul> <li>Official Website: https://www.aigve.org/</li> <li>Github Repository: https://github.com/ShaneXiangH/AIGVE_Tool</li> <li>PyPI Package: https://pypi.org/project/aigve/</li> <li>AIGVE-Bench Full Dataset https://huggingface.co/datasets/xiaoliux/AIGVE-Bench</li> <li>IFM Lab https://www.ifmlab.org/</li> </ul>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_3","title":"\u80cc\u666f\u4ecb\u7ecd","text":"<p>\u8fd1\u5e74\u6765\uff0c\u968f\u7740 Sora\u3001CogVideoX\u3001Hunyuan \u7b49\u6587\u672c\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u6301\u7eed\u7a81\u7834\uff0cAI \u751f\u6210\u89c6\u9891\uff08AIGV\uff09\u6b63\u8fc5\u901f\u6210\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u524d\u6cbf\u65b9\u5411\u3002\u6b64\u7c7b\u6a21\u578b\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u65f6\u5e8f\u8fde\u7eed\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u865a\u62df\u5185\u5bb9\u521b\u4f5c\u3001\u6559\u80b2\u52a8\u753b\u3001\u79d1\u5b66\u53ef\u89c6\u5316\u7b49\u591a\u4e2a\u573a\u666f\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u3002</p> <p>\u5c3d\u7ba1 AIGV \u6a21\u578b\u672c\u8eab\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5982\u4f55\u5bf9\u5176\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u8fdb\u884c\u7cfb\u7edf\u3001\u91cf\u5316\u7684\u8d28\u91cf\u8bc4\u4f30\uff08AIGV Evaluation, \u5373 AIGVE\uff09\u4ecd\u662f\u5f53\u524d\u4e9f\u5f85\u89e3\u51b3\u7684\u6838\u5fc3\u95ee\u9898\u3002 \u5df2\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u8bf8\u5982 FID\u3001IS\u3001CLIPScore\u3001TIFA\u3001VIEScore \u7b49\u591a\u79cd AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u522b\u4ece\u56fe\u6587\u4e00\u81f4\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u3001\u8bed\u4e49\u5339\u914d\u3001\u7269\u7406\u5408\u7406\u6027\u7b49\u4e0d\u540c\u89d2\u5ea6\u5bf9\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u6253\u5206\u3002 \u8fd9\u4e9b\u65b9\u6cd5\u5728\u5404\u81ea\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5e94\u7528\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d AIGV \u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u9762\u4e34\u4ee5\u4e0b\u4e09\u65b9\u9762\u6311\u6218\uff1a</p> <ol> <li> <p>\u7406\u8bba\u4f53\u7cfb\u7f3a\u5931\uff1a\u73b0\u6709 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u662f\u4e3a\u7279\u5b9a\u6a21\u578b\u6216\u8bc4\u4f30\u76ee\u6807\u5355\u72ec\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5f52\u7eb3\u4e0e\u7ed3\u6784\u6027\u6574\u7406\u3002\u5f53\u524d\u9886\u57df\u5c1a\u672a\u5f62\u6210\u5bf9 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5b8c\u6574\u5206\u7c7b\u4f53\u7cfb\uff0c\u7f3a\u5c11\u5bf9\u8bc4\u4f30\u76ee\u6807\u3001\u8f93\u5165\u6a21\u6001\u3001\u8bed\u4e49\u5c42\u7ea7\u7b49\u6838\u5fc3\u5c5e\u6027\u7684\u7cfb\u7edf\u6027\u68b3\u7406\u3002\u8fd9\u5bfc\u81f4\u7814\u7a76\u8005\u5728\u9009\u7528 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u65f6\u7f3a\u4e4f\u660e\u786e\u7684\u8bed\u4e49\u6307\u5f15\uff0c\u4e5f\u96be\u4ee5\u8fdb\u884c\u591a\u65b9\u6cd5\u7ec4\u5408\u3001\u8de8\u6a21\u578b\u6216\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u8fc1\u79fb\u3002</p> </li> <li> <p>\u5de5\u7a0b\u5b9e\u73b0\u788e\u7247\u5316\uff1a\u73b0\u6709\u5404\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f5c\u4e3a\u72ec\u7acb\u4ee3\u7801\u5b9e\u73b0\u3002\u5b83\u4eec\u5f80\u5f80\u5b9e\u73b0\u5206\u6563\u3001\u63a5\u53e3\u4e0d\u7edf\u4e00\u3001\u9884\u5904\u7406\u6b65\u9aa4\u4e0d\u4e00\u81f4\uff0c\u5b58\u5728\u5927\u91cf\u91cd\u590d\u4ee3\u7801\u4e0e\u4f9d\u8d56\u51b2\u7a81\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u6d41\u7a0b\u7684\u6807\u51c6\u5316\u590d\u73b0\u4e0e\u8de8\u7814\u7a76\u6a2a\u5411\u5bf9\u6bd4\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u9650\u5236\u4e86 AIGV \u7684\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u7684\u6807\u51c6\u5316\u53d1\u5c55\u4e0e\u5927\u89c4\u6a21 benchmark \u6784\u5efa\u3002</p> </li> <li> <p>\u7f3a\u4e4f\u6807\u51c6\u6570\u636e\u4e0e\u5bf9\u9f50\u673a\u5236\uff1a\u76ee\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u7ef4\u5ea6\u3001\u8de8\u6a21\u578b\u3001\u771f\u5b9e\u4eba\u7c7b\u8bc4\u5206\u7684\u7edf\u4e00\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u4f5c\u4e3a\u4e3b\u89c2\u8bc4\u4ef7\u53c2\u7167\uff0c\u96be\u4ee5\u9a8c\u8bc1\u8bc4\u4f30\u65b9\u6cd5\u6709\u6548\u6027\u4e0e\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6cd5\u95f4\u7684\u8868\u73b0\uff0c\u9650\u5236\u4e86 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5b9a\u91cf\u5206\u6790\u4e0e\u4f18\u5316\u7a7a\u95f4\u3002</p> </li> </ol> <p>\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa AIGVE-Tool \u2014\u2014 \u4e00\u4e2a\u7edf\u4e00\u3001\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u5305\u3002</p> <p>\u9996\u5148\uff0cAIGVE-Tool \u63d0\u51fa\u4e86\u9996\u4e2a\u7ed3\u6784\u5316\u7684\"\u4e94\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u5206\u7c7b\u6cd5\"\u3002AIGVE-Tool \u4ece\"\u8f93\u5165\u6a21\u6001\"\u3001\"\u5efa\u6a21\u65b9\u5f0f\"\u4e0e\"\u8bed\u4e49\u6df1\u5ea6\"\u7b49\u89d2\u5ea6\u5bf9\u5df2\u6709\u4e3b\u6d41 AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u5f52\u7c7b\uff0c\u603b\u7ed3\u51fa\u4e86\u5206\u5e03\u5bf9\u6bd4\u7c7b\u3001\u89c6\u9891\u611f\u77e5\u7c7b\u3001\u56fe\u6587\u76f8\u4f3c\u6027\u7c7b\u3001\u56fe\u6587\u7406\u89e3\u7c7b\u4e0e\u591a\u7ef4\u7efc\u5408\u7c7b\u4e94\u5927\u7c7b\u522b\u3002 \u8be5\u5206\u7c7b\u4f53\u7cfb\u5f25\u8865\u4e86\u5f53\u524d AIGV \u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u91cc\u957f\u671f\u7f3a\u5931\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u8bc4\u4f30\u65b9\u6cd5\u7684\u5206\u7c7b\u6807\u51c6\u4e0e\u7ec4\u7ec7\u51c6\u5219\u3002\u4e0d\u4ec5\u5e2e\u52a9\u7528\u6237\u4ece\u529f\u80fd\u89d2\u5ea6\u7406\u89e3\u4e0d\u540c AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\u4e0e\u5dee\u5f02\uff0c\u4e5f\u4e3a\u540e\u7eed\u4efb\u52a1\u9002\u914d\u3001\u8bc4\u4f30\u65b9\u6cd5\u6269\u5c55\u4e0e\u878d\u5408\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002</p> <p>\u5176\u6b21\uff0c\u8be5 AIGVE-Tool \u5de5\u5177\u5305\u6784\u5efa\u4e86\u6e05\u6670\u3001\u53ef\u6269\u5c55\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u6267\u884c\u6846\u67b6\uff0c\u7edf\u4e00\u96c6\u6210\u4e86\u8fd120\u4e2a\u4e3b\u6d41 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002 AIGVE-Tool \u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u9a71\u52a8\u3001\u7ec4\u4ef6\u89e3\u8026\u7684\u7406\u5ff5\uff0c\u91cd\u65b0\u7ec4\u7ec7\u4e86 AIGV \u8d28\u91cf\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u6838\u5fc3\u73af\u8282\uff0c\u6784\u5efa\u4e86\u6db5\u76d6\u6570\u636e\u52a0\u8f7d\u3001\u6279\u91cf\u8bc4\u4f30\u3001\u6574\u4f53\u6d41\u7a0b\u63a7\u5236\u7684\u7edf\u4e00\u6267\u884c\u67b6\u6784\uff0c AIGVE-Tool \u652f\u6301\u5feb\u901f\u63a5\u5165\u73b0\u6709\u6216\u81ea\u5b9a\u4e49\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u6237\u53ef\u4ee5\u65b9\u4fbf\u5730\u8fd0\u884c\u3001\u590d\u7528\u3001\u6269\u5c55\u5404\u79cd AIGV \u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u4e0e\u5176\u4ed6 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u6807\u51c6\u5316\u5bf9\u6bd4\u3002 AIGVE-Tool \u663e\u8457\u63d0\u5347\u4e86 AIGV \u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u5b9e\u73b0\u903b\u8f91\u7684\u590d\u7528\u6027\u4e0e\u53ef\u7ef4\u62a4\u6027\uff0c\u63a8\u52a8 AIGV \u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u7684\u6a21\u5757\u5316\u4e0e\u4f53\u7cfb\u5316\u3002</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u914d\u5957\u5730\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4eba\u7c7b\u6253\u5206\u6807\u6ce8\u7684\u591a\u7ef4\u5ea6\u57fa\u51c6\u6570\u636e\u96c6 AIGVE-Bench\u3002AIGVE-Bench\u6db5\u76d6\u4e94\u4e2a\u4ee3\u8868\u6027 AIGV \u6a21\u578b\u30012430 \u4e2a\u89c6\u9891\u6837\u672c\u4e0e\u4e5d\u4e2a\u6838\u5fc3\u8d28\u91cf\u7ef4\u5ea6\u7684\u4eba\u7c7b\u6253\u5206\uff08\u5171\u8ba1 21870 \u6761\u8bc4\u5206\uff09\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86 AIGV \u8d28\u91cf\u8bc4\u4f30\u4e2d\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4e3b\u89c2\u8bc4\u4ef7\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5bf9\u9f50\u5206\u6790\u3002 AIGVE-Bench \u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4ef7\u7ef4\u5ea6\u5b9a\u4e49\u3001\u8de8\u6a21\u578b\u6027\u80fd\u5bf9\u6bd4\u4e0e\u4e00\u81f4\u6027\u7edf\u8ba1\u56fe\u8868\uff0c\u662f\u5f53\u524d\u7ed3\u6784\u6700\u5b8c\u5907\u3001\u7ef4\u5ea6\u6700\u5168\u9762\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e4b\u4e00\u3002</p> <p>\u7efc\u5408\u800c\u8a00\uff0c\"\u4e94\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u5206\u7c7b\u6cd5\"\u4e3a AIGV \u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491, AIGVE-Tool \u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u8bbe\u8ba1\uff0cAIGVE-Bench \u5219\u6784\u5efa\u4e86\u53ef\u9760\u7684\u591a\u7ef4\u5ea6\u4e3b\u89c2\u8bc4\u4ef7\u6570\u636e\u96c6\u3002\u4e09\u8005\u534f\u540c\u4e3a AIGV \u8d28\u91cf\u8bc4\u4f30 \u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\"\u7406\u8bba-\u67b6\u6784-\u6570\u636e\"\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u8fc8\u5411\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u3001\u53ef\u6bd4\u8f83\u7684\u5168\u65b0\u7814\u7a76\u8303\u5f0f\u65f6\u4ee3\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#01","title":"01. \u4e94\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u5206\u7c7b\u6cd5","text":"<p>\u968f\u7740\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u4e0d\u65ad\u6f14\u5316\uff0cAIGV \u7684\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u9010\u6b65\u4ece\"\u56fe\u50cf\u7ea7\u6253\u5206\"\u8fc8\u5411\"\u89c6\u9891\u7ea7\u3001\u591a\u7ef4\u5ea6\u3001\u8de8\u6a21\u6001\"\u7684\u5168\u65b0\u9636\u6bb5\u3002\u4e3a\u4e86\u5e2e\u52a9\u7528\u6237\u7cfb\u7edf\u7406\u89e3\u8fd9\u4e9b\u8bc4\u4f30\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u4e0e\u80fd\u529b\u5dee\u5f02\uff0cAIGVE-Tool \u5728\u8bba\u6587\u4e2d\u9996\u6b21\u63d0\u51fa\u4e86\"\u4e94\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\"\uff0c\u8986\u76d6\u4e86\u4ece\u4f4e\u5c42\u611f\u77e5\u5230\u9ad8\u5c42\u8bed\u4e49\u3001\u4ece\u5355\u6a21\u6001\u7279\u5f81\u5230\u8de8\u6a21\u6001\u7406\u89e3\u7684\u4e0d\u540c\u8bc4\u4ef7\u89c6\u89d2\u3002</p> <p>\u6211\u4eec\u5c06\u76ee\u524d\u4e3b\u6d41\u7684 AIGV \u8bc4\u4f30\u65b9\u6cd5\u6839\u636e\u5176\u8bbe\u8ba1\u76ee\u6807\u4e0e\u6570\u636e\u6a21\u6001\uff0c\u5212\u5206\u4e3a\u4ee5\u4e0b\u4e94\u5927\u7c7b\uff1a</p> <ol> <li> <p>\u5206\u5e03\u5bf9\u6bd4\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff08Distribution Comparison-Based\uff09\uff1a\u8861\u91cf\u751f\u6210\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u4e4b\u95f4\u5728\u5206\u5e03\u5c42\u9762\u7684\u76f8\u4f3c\u5ea6\u3002\u4ee3\u8868\u65b9\u6cd5\u5305\u62ec\uff1aFID\u3001FVD\u3001IS\u3002</p> </li> <li> <p>\u89c6\u9891\u611f\u77e5\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff08Video-Only Neural Network-Based\uff09\uff1a\u5173\u6ce8\u89c6\u9891\u7684\u6e05\u6670\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u52a8\u6001\u6d41\u7545\u6027\u7b49\u4e3b\u89c2\u753b\u9762\u8d28\u91cf\u3002\u4ee3\u8868\u65b9\u6cd5\u5305\u62ec\uff1aGSTVQA\u3001SimpleVQA\u3001LightVQA+\u3002</p> </li> <li> <p>\u56fe\u6587\u76f8\u4f3c\u6027\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff08Vision-Language Similarity-Based\uff09\uff1a\u4f7f\u7528 CLIP/BLIP \u7b49\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b,\u5ea6\u91cf\u89c6\u9891 video \u4e0e\u6587\u672c\u63d0\u793a prompt \u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u4ee3\u8868\u65b9\u6cd5\u5305\u62ec\uff1a CLIPScore\u3001BLIPSim\u3001PickScore\u3002</p> </li> <li> <p>\u56fe\u6587\u7406\u89e3\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff08Vision-Language Understanding-Based\uff09\uff1a\u5173\u6ce8\u89c6\u9891 video \u4e0e\u6587\u672c\u63d0\u793a prompt \u4e4b\u95f4\u7684\u6df1\u5c42\u7406\u89e3\u5173\u7cfb,\u805a\u7126\u4e8b\u4ef6\u4e00\u81f4\u6027\u3001\u95ee\u7b54\u51c6\u786e\u6027\u3001\u52a8\u4f5c\u63a8\u7406\u7b49\u66f4\u590d\u6742\u8bed\u4e49\u5c42\u9762\u3002\u4ee3\u8868\u65b9\u6cd5\u5305\u62ec\uff1aTIFA\u3001VIEScore\u3001DSG\u3002</p> </li> <li> <p>\u591a\u7ef4\u7efc\u5408\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff08Multi-Faceted Evaluation\uff09\uff1a\u7efc\u5408\u591a\u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u52a0\u6743\u878d\u5408\u8bc4\u4f30\uff0c\u8ffd\u6c42\u5168\u9762\u53cd\u6620\u89c6\u9891\u8868\u73b0\u3002\u4ee3\u8868\u65b9\u6cd5\u5305\u62ec\uff1aVideoPhy\u3001VideoScore\u3002</p> </li> </ol> <p>\u8be5\u5206\u7c7b\u6cd5\u9996\u6b21\u7cfb\u7edf\u6027\u6574\u5408\u4e86 AIGVE \u9886\u57df\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6e05\u6670\u5b9a\u4e49\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3001\u4ee3\u8868\u65b9\u6cd5\u4e0e\u529f\u80fd\u7279\u6027,\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u8fdb\u884c\u573a\u666f\u9009\u578b\u4e0e\u7ec4\u5408\u4f7f\u7528\u3002</p> <p>\u4e3a\u4e86\u76f4\u89c2\u7406\u89e3\u4e94\u5927\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u7684\u8986\u76d6\u8303\u56f4\u4e0e\u5dee\u5f02\uff0c\u4e0b\u8868\u5bf9 AIGVE-Tool \u5f53\u524d\u6536\u5f55\u7684AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u6c47\u603b\uff0c\u5e76\u7b80\u8981\u4ecb\u7ecd\u5176\u9002\u7528\u529f\u80fd\uff1a</p> <p></p> <p>\u901a\u8fc7\u8be5\u7ed3\u6784\u5316\u5206\u7c7b\uff0c\u7528\u6237\u53ef\u4ee5\u5feb\u901f\u5b9a\u4f4d\u5404\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u7684\u8bc4\u4f30\u76ee\u6807\u3001\u4f7f\u7528\u65b9\u5f0f\u4e0e\u8f93\u5165\u8981\u6c42\u3002\u6211\u4eec\u7684 AIGVE-Tool \u6839\u636e\u8fd9\u4e2a\u5206\u7c7b\u6765\u8fdb\u884c\u7ec4\u7ec7\uff0c\u4e3a\u6784\u5efa\u7528\u6237\u81ea\u5b9a\u4e49 AIGV \u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u4e0e\u5de5\u7a0b\u4fbf\u5229\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#02-aigve-tool","title":"02. AIGVE-Tool\uff1a\u901a\u7528,\u6a21\u5757\u5316,\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u67b6\u6784","text":"<p>AIGVE-Tool\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u9a71\u52a8\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u6838\u5fc3\u529f\u80fd\u5305\u62ec:</p> <ul> <li> <p>\u7edf\u4e00\u7684\u4e3b\u5faa\u73af AIGVELoop\uff1a\u8d1f\u8d23\u6807\u51c6\u6d41\u7a0b\u7684\u8c03\u5ea6\uff0c\u4ece\u6570\u636e\u52a0\u8f7d\u3001\u7279\u5f81\u63d0\u53d6\u3001\u8bc4\u4f30\u65b9\u6cd5\u7684\u8ba1\u7b97\u5230\u7ed3\u679c\u4fdd\u5b58\uff0c\u5168\u90e8\u81ea\u52a8\u6267\u884c\u3002</p> </li> <li> <p>\u7075\u6d3b\u7684 DataLoader \u6784\u5efa\u673a\u5236\uff1a\u652f\u6301\u4e0d\u540c\u89c6\u9891\u6570\u636e\u683c\u5f0f\u3001\u4efb\u610f\u5e27\u6570\u91c7\u6837\u3001\u89c6\u9891\u89e3\u7801\u5668\u9009\u62e9\u3001OpenCV / torchvision\u591a\u540e\u7aef\u652f\u6301\u3001\u590d\u6742\u6570\u636e\u5b57\u6bb5\u5904\u7406\uff08\u5982 video+prompt\uff09\u7b49\u3002\u7528\u6237\u53ef\u8f7b\u677e\u6269\u5c55\u81ea\u5df1\u7684 dataset\u3002</p> </li> <li> <p>\u6a21\u5757\u5316\u7684 Evaluator \u63a5\u53e3\uff1a\u6211\u4eec\u5c06\u6240\u6709\u5df2\u6709\u7684\u548c\u65b0\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8bbe\u8ba1\u4e3a\u72ec\u7acb\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u5b9e\u73b0\u6807\u51c6\u63a5\u53e3\u5373\u53ef\u63a5\u5165\u81ea\u5b9a\u4e49\u7684\u8bc4\u4f30\u65b9\u6cd5\u65b0\u903b\u8f91\u3002</p> </li> <li> <p>\u914d\u7f6e\u6587\u4ef6\u9a71\u52a8\u6267\u884c\uff1a\u4f7f\u7528\u7c7b\u4f3c MMEngine \u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u6240\u6709\u8bbe\u7f6e\u5747\u901a\u8fc7 Python \u914d\u7f6e\u6587\u4ef6\u5b8c\u6210\uff0c\u6db5\u76d6\u6a21\u578b\u8def\u5f84\u3001\u89c6\u9891\u8f93\u5165\u3001\u8bc4\u4f30\u65b9\u6cd5\u7b49\u3002\u907f\u514d\u786c\u7f16\u7801\uff0c\u65b9\u4fbf\u7ec4\u5408\u3001\u7ee7\u627f\u548c\u5feb\u901f\u590d\u7528\u3002</p> </li> </ul> <p>\u4e0b\u56fe\u5c55\u793a\u4e86 AIGVE-Tool \u7684\u7cfb\u7edf\u7ed3\u6784\uff1a</p> <p></p> <p>\u5982\u4e0a\u56fe\u6240\u793a\uff0cAIGVE-Tool \u6846\u67b6\u7531\u4e09\u5927\u6838\u5fc3\u7ec4\u4ef6\u6784\u6210\uff1a\u914d\u7f6e\u6587\u4ef6\uff08Configuration Files\uff09\u3001\u53ef\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff08Customizable DataLoaders\uff09\u4e0e\u6a21\u5757\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff08Modular Metrics\uff09\u3002\u5b83\u4eec\u901a\u8fc7\u7edf\u4e00\u4e3b\u5faa\u73af AIGVELoop \u4e32\u8054\u5728\u4e00\u8d77\uff0c\u5b8c\u6210\u4ece\u6570\u636e\u52a0\u8f7d\uff0c\u8d28\u91cf\u8bc4\u4f30\uff0c\u5230\u7ed3\u679c\u8f93\u51fa\u7684\u5168\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u6269\u5c55\u4e0e\u914d\u7f6e\u66ff\u6362\u3002\u8be5\u8bbe\u8ba1\u57fa\u4e8e MMEngine \u5b9e\u73b0\uff0c\u5f3a\u8c03\u4ee3\u7801\u7ed3\u6784\u6e05\u6670\u4e0e\u6613\u7528\u6027\uff0c\u5f00\u53d1\u8005\u53ea\u9700\u5173\u6ce8\u7ec4\u4ef6\u5b9e\u73b0\uff0c\u65e0\u9700\u66f4\u6539\u4e3b\u6d41\u7a0b\u903b\u8f91\u3002</p> <p>\u57fa\u4e8e\u6b64\u67b6\u6784\uff0c\u5f53\u524d\u7248\u672c\u5df2\u539f\u751f\u652f\u6301\u5305\u62ec FID\u3001IS\u3001FVD\u3001CLIPScore\u3001SimpleVQA\u3001GSTVQA\u3001TIFA\u3001VideoScore \u7b49\u5728\u5185\u7684\u8fd1 20 \u4e2a\u4ee3\u8868\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6db5\u76d6\u4ece\u89c6\u89c9\u8d28\u91cf\u5230\u8bed\u4e49\u7406\u89e3\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4efb\u52a1\u3002\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u5206\u522b\u4ecb\u7ecd\u8be5\u67b6\u6784\u7684\u6bcf\u4e2a\u6a21\u5757\uff1a</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#1-aigveloop","title":"1. \u4e3b\u5faa\u73af AIGVELoop","text":"<p>AIGVELoop \u662f\u6574\u4e2a AIGVE-Tool \u6846\u67b6\u7684\u6267\u884c\u5165\u53e3\uff0c\u7ee7\u627f\u4e8e MMEngine \u7684 BaseLoop \u6784\u5efa\uff0c\u5177\u5907\u9ad8\u5ea6\u7075\u6d3b\u4e0e\u901a\u7528\u6027\u3002\u5b83\u8d1f\u8d23\u4e32\u8054\u6570\u636e\u52a0\u8f7d\u5668(DataLoader)\u4e0e\u8bc4\u4f30\u5668\uff08Evaluator\uff09\uff0c\u81ea\u52a8\u8c03\u5ea6\u5b8c\u6574\u7684\u63a8\u7406-\u8bc4\u4f30\u6d41\u7a0b\u3002\u7528\u6237\u65e0\u9700\u624b\u52a8\u63a7\u5236\u6bcf\u4e00\u8f6e\u8bc4\u4f30\u903b\u8f91\uff0c\u53ea\u9700\u914d\u7f6e\u597d\u6570\u636e\u52a0\u8f7d\u4e0e\u65b9\u6cd5\u8bc4\u4f30\u6a21\u5757\u5373\u53ef\u5feb\u901f\u5f00\u59cb\u5b9e\u9a8c\u3002AIGVELoop \u652f\u6301\u81ea\u5b9a\u4e49 hooks\u3001FP16 \u63a8\u7406\u3001\u5143\u4fe1\u606f\u6ce8\u5165\u7b49\u529f\u80fd\uff0c\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#2-customizable-dataloaders","title":"2. \u53ef\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff08Customizable DataLoaders\uff09","text":"<p>\u4e3a\u9002\u5e94\u4e0d\u540c AIGV \u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u5728\u683c\u5f0f\u3001\u5206\u8fa8\u7387\u3001\u65f6\u957f\u7b49\u65b9\u9762\u7684\u5de8\u5927\u5dee\u5f02\uff0cAIGVE-Tool \u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u52a0\u8f7d\u6a21\u5757\u3002\u8be5\u52a0\u8f7d\u6a21\u5757\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u5e76\u517c\u5bb9\u591a\u6a21\u6001\u8f93\u5165\uff0c\u652f\u6301\u591a\u79cd\u8bfb\u53d6\u540e\u7aef\uff08\u5982 OpenCV\u3001torchvision\uff09\u3002\u7528\u6237\u53ef\u52a0\u8f7d (video, prompt, meta) \u7b49\u591a\u5b57\u6bb5\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u590d\u6742\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3002\u5728AIGVE-Tool\u6846\u67b6\u4e0b\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u7684\u5bf9\u89c6\u9891\u5e27\u62bd\u6837\u3001\u5e27\u7ec4\u5408\u65b9\u5f0f\u3001\u89c6\u9891\u683c\u5f0f\u89e3\u7801\u3001\u5e27\u6570\u5f52\u4e00\u5316\u7b49\u9884\u5904\u7406\u6b65\u9aa4\u8fdb\u884c\u4e2a\u6027\u5316\u914d\u7f6e\uff0c\u4ece\u800c\u5b9e\u73b0\u7075\u6d3b\u7684\u6570\u636e\u52a0\u8f7d\u3002\u901a\u8fc7\u7edf\u4e00\u5c01\u88c5\uff0c\u914d\u7f6e\u597d\u7684 DataLoader \u53ef\u5728\u4e0d\u540c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u590d\u7528\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u91cd\u590d\u4ee3\u7801\u7f16\u5199\u6210\u672c\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#3-modular-metrics","title":"3. \u6a21\u5757\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff08Modular Metrics\uff09","text":"<p>AIGVE-Tool \u7684\u8bc4\u4f30\u65b9\u6cd5\u90e8\u5206\u88ab\u8bbe\u8ba1\u4e3a\u5b8c\u5168\u6a21\u5757\u5316\u7ed3\u6784\u3002\u6bcf\u4e2a\u8bc4\u4f30\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u6a21\u5757\u6ce8\u518c\uff0c\u5747\u7ee7\u627f\u81ea MMEngine \u7684 BaseMetric\u3002 \u5b83\u4eec\u7edf\u4e00\u9075\u5faa process()\u548c compute_metrics()\u4e24\u4e2a\u63a5\u53e3\u3002process() \u7528\u4e8e\u5904\u7406\u5355\u6279\u6837\u672c\uff0ccompute_metrics() \u7528\u4e8e\u8ba1\u7b97\u7edf\u8ba1\u6027\u5206\u6570\u3002 \u65e0\u8bba\u662f\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982 FID/FVD\uff09\uff0c\u8fd8\u662f\u5927\u6a21\u578b\u9a71\u52a8\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982 TIFA\u3001CLIPScore\uff09\uff0c\u90fd\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f\u62bd\u8c61\u57fa\u7c7b\u5feb\u901f\u96c6\u6210\u3002 \u6b64\u8bbe\u8ba1\u652f\u6301\u6279\u5904\u7406\u8ba1\u7b97\u3001\u8bc4\u4f30\u7ef4\u5ea6\u81ea\u5b9a\u4e49\u3001\u591a\u8bc4\u4f30\u65b9\u6cd5\u7ec4\u5408\u7b49\u9ad8\u7ea7\u529f\u80fd\uff0c\u662f\u6574\u4e2a\u6846\u67b6\u7684\u62d3\u5c55\u6838\u5fc3\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#4-configuration-driven-execution","title":"4. \u914d\u7f6e\u9a71\u52a8\u6267\u884c\uff08Configuration-Driven Execution\uff09","text":"<p>AIGVE-Tool \u91c7\u7528\u57fa\u4e8e MMEngine \u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u6240\u6709\u7ec4\u4ef6\u4e0e\u53c2\u6570\u5747\u901a\u8fc7 .py \u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u3002\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u6307\u5b9a\u6a21\u578b\u8def\u5f84\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u6570\u636e\u8def\u5f84\u3001batch size \u7b49\u8fd0\u884c\u53c2\u6570\u3002\u914d\u7f6e\u7ed3\u6784\u6e05\u6670\uff0c\u652f\u6301\u6a21\u5757\u7ee7\u627f\u4e0e\u53c2\u6570\u8986\u76d6\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5b9e\u9a8c\u7ba1\u7406\u3002\u7528\u6237\u65e0\u9700\u4fee\u6539\u4e3b\u903b\u8f91\uff0c\u5373\u53ef\u5feb\u901f\u66f4\u6362\u8bc4\u4f30\u65b9\u6cd5\u6a21\u5757\uff0c\u5927\u5e45\u63d0\u5347\u5b9e\u9a8c\u6548\u7387\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#03-aigve-bench","title":"03. AIGVE-Bench\uff1a\u591a\u7ef4\u591a\u6a21\u578b\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6","text":"<p>AIGVE-Bench \u662f\u6211\u4eec\u5728 AIGVE-Tool \u6846\u67b6\u4e0b\u8bbe\u8ba1\u6784\u5efa\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u5f53\u4e0b\u53ca\u672a\u6765 AIGV \u591a\u7ef4\u5ea6\u3001\u591a\u4efb\u52a1\u7684\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u3002 \u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u6587\u672c\u63d0\u793a\u3001\u6a21\u578b\u8f93\u51fa\u89c6\u9891\u3001\u4eba\u7c7b\u591a\u7ef4\u8bc4\u5206\u4e09\u5927\u8981\u7d20\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u7edf\u4e00\u683c\u5f0f\u7684\u4eba\u7c7b\u6807\u6ce8\uff0c\u8fd8\u5c06\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7ed3\u679c\u4e0e\u4eba\u7c7b\u4e3b\u89c2\u8bc4\u5206\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u5bf9\u4e0e\u76f8\u5173\u6027\u5bf9\u9f50\u5206\u6790\uff0c\u662f\u5f53\u524d\u8bc4\u4ef7\u7ef4\u5ea6\u6700\u5168\u9762\u3001\u7ec4\u7ec7\u7ed3\u6784\u5316\u7a0b\u5ea6\u6700\u9ad8\u7684 AIGV \u8bc4\u4f30\u57fa\u51c6\u4e4b\u4e00\u3002 \u5177\u4f53\u6765\u8bf4\uff0cAIGVE-Bench \u5177\u5907\u4ee5\u4e0b\u7279\u70b9\uff1a</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#1","title":"1. \u6807\u6ce8\u89c4\u6a21\u5e9e\u5927","text":"<p>\u4e3a\u4e86\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u4e0e\u6cdb\u5316\u6027\uff0cAIGVE-Bench \u91c7\u7528\u4e86\u5927\u89c4\u6a21\u4eba\u7c7b\u6807\u6ce8\u673a\u5236\u3002\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c\u5171\u8ba1\u6536\u96c6 21870 \u6761\u9ad8\u8d28\u91cf\u8bc4\u5206\u6837\u672c\uff0c\u6db5\u76d6 2430 \u4e2a\u89c6\u9891\uff0c\u6bcf\u6761\u89c6\u9891\u90fd\u5728 9 \u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u4e0a\u7531\u4eba\u5de5\u6253\u5206\u5b8c\u6210\uff0c\u6784\u5efa\u4e86\u591a\u7ef4\u5ea6\u3001\u7ec6\u7c92\u5ea6\u7684\u4e3b\u89c2\u8d28\u91cf\u53c2\u8003\u6807\u51c6\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#2","title":"2. \u6587\u672c\u6307\u4ee4\u8bbe\u8ba1\u4e30\u5bcc","text":"<p>AIGVE-Bench \u62e5\u6709 500 \u6761\u9ad8\u8d28\u91cf\u6587\u672c\u63d0\u793a\uff08prompt\uff09\uff0c\u5e7f\u6cdb\u6db5\u76d6\u9759\u6001\u573a\u666f\u3001\u52a8\u4f5c\u884c\u4e3a\u3001\u7269\u7406\u77e5\u8bc6\u3001\u4ea4\u4e92\u5173\u7cfb\u7684\u591a\u4e2a\u65b9\u9762\u3002</p> <p>\u4e0b\u8868\u603b\u7ed3\u4e86 AIGVE-Bench \u6240\u6db5\u76d6\u7684\u6587\u672c\u63d0\u793a\u7c7b\u522b\uff08Instruction Categories\uff09\uff0c\u6211\u4eec\u4ece\u62cd\u6444\u89c6\u89d2\uff08\u5982\u5168\u666f vs. \u8fd1\u666f\uff09\u4e0e\u5185\u5bb9\u7c7b\u578b\uff08\u9759\u6001\u5bf9\u8c61 vs. \u52a8\u6001\u7c7b\u578b\uff09\u4e24\u4e2a\u65b9\u9762\u5bf9\u6240\u6709\u6587\u672c\u63d0\u793a\uff08prompt\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\uff1a</p> <p></p> <ul> <li> <p>Global View \u7c7b\u6587\u672c\u63d0\u793a\u5173\u6ce8\u7684\u662f\u5927\u573a\u666f\u4e0e\u81ea\u7136\u5730\u8c8c\uff0c\u5982\u57ce\u5e02\u5efa\u7b51\u3001\u6e56\u6cca\u5c71\u5ddd\u7b49\uff0c\u642d\u914d\u5982\u5929\u6c14\u53d8\u5316\u3001\u5149\u7167\u5207\u6362\u7b49\u81ea\u7136\u52a8\u6001\u3002</p> </li> <li> <p>Close Shot \u7c7b\u6587\u672c\u63d0\u793a\u5219\u805a\u7126\u4e8e\u5355\u4e2a\u6216\u591a\u4e2a\u5bf9\u8c61\u672c\u8eab\uff08\u5982\u4eba\u7269\u3001\u52a8\u7269\u3001\u690d\u7269\uff09\u7684\u5c40\u90e8\u884c\u4e3a\u4e0e\u4ea4\u4e92\u52a8\u4f5c\u3002</p> </li> </ul> <p>\u4e0b\u56fe\u4e3a AIGVE-Bench \u7684\u6587\u672c\u63d0\u793a\uff08prompt\uff09\u4e2d\u6bcf\u4e00\u79cd\"\u9759\u6001\u5bf9\u8c61\"\uff08Subjects\uff09\u4e0e\"\u52a8\u6001\u7c7b\u578b\"\uff08Dynamics\uff09\u5206\u5e03\u60c5\u51b5\uff1a</p> <p></p> <p>\u53ef\u89c1 AIGVE-Bench \u7684\u6587\u672c\u63d0\u793a\u5728\u9759\u6001\u5bf9\u8c61\u4e0e\u52a8\u6001\u7c7b\u578b\u4e2d\u5747\u5177\u5907\u826f\u597d\u591a\u6837\u6027\uff0c\u786e\u4fdd\u4e86 AIGVE-Bench \u6570\u636e\u96c6\u5728\u573a\u666f\u590d\u6742\u5ea6\u3001\u8fd0\u52a8\u53d8\u5316\u3001\u4ea4\u4e92\u6027\u7b49\u65b9\u9762\u7684\u5747\u8861\u6027\uff0c\u4e3a\u540e\u7eed AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u6027\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#3","title":"3. \u6570\u636e\u6765\u6e90\u591a\u6837","text":"<p>AIGVE-Bench \u7cbe\u9009\u4e86\u4e94\u4e2a\u4ee3\u8868\u6027\u7684 SOTA AI\u89c6\u9891\u751f\u6210\u6a21\u578b: CogVideoX\u3001Genmo\u3001Hunyuan\u3001Pyramid \u548c Sora\u3002\u6211\u4eec\u8bbe\u7f6e\u7edf\u4e00\u63d0\u793a\u8bcd\u3001\u91c7\u6837\u7b56\u7565\uff0c\u5bf9\u6bcf\u4e2a\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u8fdb\u884c\u91c7\u96c6\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u5305\u542b 2430 \u4e2a\u89c6\u9891\u6837\u672c \u7684\u6570\u636e\u96c6\u3002</p> <p>\u4e0b\u8868\u5217\u51fa\u4e86\u7531\u4e0d\u540c\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u7684\u5173\u952e\u53c2\u6570\uff0c\u5305\u62ec\u5206\u8fa8\u7387\u3001\u5e27\u7387\u4e0e\u65f6\u957f\u7b49\uff0c\u8fd9\u4e9b\u53c2\u6570\u5dee\u5f02\u4f53\u73b0\u4e86\u5404\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u3001\u98ce\u683c\u4ee5\u53ca\u6027\u80fd\u5c42\u9762\u7684\u591a\u6837\u6027\uff1a</p> <p></p> <p>\u8fd9\u4e9b\u5dee\u5f02\u4e3a AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u2014\u2014\u5b83\u4eec\u4e0d\u4ec5\u8981\u9002\u914d\u4e0d\u540c\u8f93\u51fa\u683c\u5f0f\uff0c\u8fd8\u9700\u5177\u5907\u8db3\u591f\u7684\u9c81\u68d2\u6027\u4e0e\u5e7f\u6cdb\u6027\uff0c\u4ee5\u5b9e\u73b0\u8de8\u6a21\u578b\u7684\u516c\u5e73\u5bf9\u6bd4\u3002</p> <p>\u4e0b\u8868\u5c55\u793a\u4e86\u4f7f\u7528 AIGVE-Tool \u4e2d\u5df2\u96c6\u6210\u7684 AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bf9\u4e94\u4e2a\u5f53\u524d SOTA \u89c6\u9891\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u7684\u5b8c\u6574\u6253\u5206\u7ed3\u679c\u3002\u8868\u4e2d\u7c97\u4f53\u8868\u793a\u8be5\u8bc4\u4f30\u65b9\u6cd5\u4e0b\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff0c\u65b9\u62ec\u53f7\u5185\u4e3a\u6bcf\u4e2a\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d6\u503c\u8303\u56f4\uff1a</p> <p></p> <p>\u4ece\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0c\u4e0d\u540c\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e0d\u540c\u6765\u6e90\u7684 AIGV \u4e0a\u5404\u5177\u4f18\u52bf\uff0c\u8bf4\u660e\u5355\u4e00\u7684 AIGV \u8d28\u91cf\u8bc4\u4ef7\u7ef4\u5ea6\u96be\u4ee5\u5168\u9762\u53cd\u6620 AI \u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u7efc\u5408\u80fd\u529b\u3002\u8fd9\u4e5f\u8fdb\u4e00\u6b65\u5370\u8bc1\u4e86 AIGVE-Bench \u591a\u7ef4\u8d28\u91cf\u8bc4\u4ef7\u4f53\u7cfb\u7684\u91cd\u8981\u6027\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#4","title":"4. \u8bc4\u4ef7\u7ef4\u5ea6\u5168\u9762","text":"<p>\u4e3a\u4e86\u6784\u5efa\u5177\u6709\u89e3\u91ca\u6027\u4e0e\u901a\u7528\u6027\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u6211\u4eec\u5728 AIGVE-Bench \u4e2d\u8bbe\u5b9a\u4e86 9 \u4e2a\u7ec6\u5206\u7684\u8bc4\u4ef7\u7ef4\u5ea6\uff0c\u8986\u76d6\u4ece\u753b\u8d28\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u7269\u7406\u5408\u7406\u6027\u5230\u4ea4\u4e92\u4e0e\u573a\u666f\u590d\u6742\u5ea6\u7b49\u591a\u4e2a\u5c42\u9762\u3002\u8fd9\u4e9b\u7ef4\u5ea6\u4e0d\u4ec5\u8986\u76d6\u4e86\u4f20\u7edf\u56fe\u50cf/\u89c6\u9891\u8d28\u91cf\u8bc4\u4ef7\u4e2d\u7684\u57fa\u7840\u7ef4\u5ea6\uff0c\u4e5f\u6269\u5c55\u81f3 AIGV \u72ec\u6709\u7684\u8de8\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u3002\u6bcf\u4e2a\u89c6\u9891\u90fd\u5728\u8fd9 9 \u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u4e0a\u9762\u8fdb\u884c\u4e86\u4eba\u5de5\u6807\u6ce8\u8bc4\u5206\uff0c\u6784\u5efa\u4e86\u6807\u51c6\u7684\u4e3b\u89c2\u8bc4\u4ef7\u6807\u51c6\u3002</p> <p>\u4e0b\u8868\u5bf9\u6bcf\u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u7684\u5177\u4f53\u5b9a\u4e49\u4e0e\u8bc4\u4f30\u8981\u70b9\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u7406\u89e3\u8bc4\u5206\u6807\u51c6\uff0c\u540c\u65f6\u4e5f\u4e3a\u540e\u7eed\u8bbe\u8ba1\u65b0\u7684 AIGV \u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u8bed\u4e49\u652f\u6301\uff1a</p> <p></p> <p>\u8fd9\u4e9b\u8bc4\u4ef7\u7ef4\u5ea6\u4e0d\u4ec5\u53ef\u72ec\u7acb\u4f7f\u7528\uff0c\u4e5f\u652f\u6301\u52a0\u6743\u878d\u5408\uff0c\u652f\u6301\u7814\u7a76\u8005\u6839\u636e\u4efb\u52a1\u9700\u6c42\u7075\u6d3b\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u8bc4\u4f30\u65b9\u6cd5\u3002</p> <p>\u4e3a\u4e86\u66f4\u76f4\u89c2\u5730\u5c55\u793a\u4e0d\u540c AIGV \u6a21\u578b\u5728\u5404\u8bc4\u4ef7\u7ef4\u5ea6\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u6211\u4eec\u7edf\u8ba1\u4e86\u7531\u4e94\u4e2a\u4e3b\u6d41\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u5728\u4e0d\u540c\u9759\u6001\u5bf9\u8c61\u7c7b\u522b\uff08\u4e0a\u534a\u90e8\u5206\uff09\u548c\u8d28\u91cf\u8bc4\u4ef7\u7ef4\u5ea6\uff08\u4e0b\u534a\u90e8\u5206\uff09\u4e0a\u7684\u4eba\u7c7b\u8bc4\u5206\u5206\u5e03\u60c5\u51b5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> <p>\u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u751f\u6210\u7279\u5b9a\u7c7b\u578b\u8bed\u4e49\u5185\u5bb9\uff08\u5982\u81ea\u7136\u573a\u666f vs. \u57ce\u5e02\u5efa\u7b51\uff09\u65f6\u8868\u73b0\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u4e00\u4e9b\u6a21\u578b\u5728\"\u81ea\u7136\u7c7b\u5bf9\u8c61\"\uff08\u5982\u5c71\u8109\u3001\u52a8\u7269\uff09\u4e0a\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u4e3b\u89c2\u8bc4\u5206\uff0c\u800c\u5728\"\u4eba\u7c7b\u6216\u57ce\u5e02\u7c7b\u5bf9\u8c61\"\u4e0a\u8bc4\u5206\u76f8\u5bf9\u8f83\u4f4e\uff0c\u8868\u660e\u5176\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u8868\u8fbe\u65b9\u9762\u4ecd\u5b58\u5728\u77ed\u677f\u3002</p> <p>\u4e0b\u534a\u90e8\u5206\u8fdb\u4e00\u6b65\u4ece\u4e5d\u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\uff08\u5982\u6280\u672f\u8d28\u91cf\u3001\u7269\u4f53\u4fdd\u771f\u5ea6\u3001\u4ea4\u4e92\u5408\u7406\u6027\u7b49\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u7ec6\u81f4\u5bf9\u6bd4\u3002\u53ef\u4ee5\u89c2\u5bdf\u5230\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u5404\u7ef4\u5ea6\u4e0a\u5448\u73b0\u51fa\u4e0d\u540c\u7684\u5f3a\u9879\u4e0e\u5f31\u9879\u3002\u4f8b\u5982\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u7269\u7406\u89c4\u5219\u7ef4\u5ea6\u4e0a\u5b58\u5728\u8f83\u5927\u4e0d\u8db3\u3002</p> <p>\u6a2a\u8f74\u8868\u793a\u5bf9\u8c61\u7c7b\u522b\u6216\u8bc4\u4ef7\u7ef4\u5ea6\uff0c\u7eb5\u8f74\u8868\u793a\u5bf9\u5e94\u6a21\u578b\u7684\u5e73\u5747\u4eba\u7c7b\u8bc4\u5206\uff0c\u56fe\u4e2d \u03bc \u8868\u793a\u5747\u503c\u3002\u6574\u4f53\u6765\u770b\uff0c\u8be5\u56fe\u6e05\u6670\u5c55\u793a\u4e86\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u4eba\u7c7b\u4e3b\u89c2\u8bc4\u4ef7\u4e0b\u7684\u591a\u7ef4\u5ea6\u6027\u80fd\u7279\u5f81\uff0c\u662f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u4e0e\u80fd\u529b\u8bca\u65ad\u7684\u91cd\u8981\u4f9d\u636e\u3002</p> <p>\u8fd9\u8fdb\u4e00\u6b65\u8bf4\u660e\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u7ef4\u8bc4\u4ef7\u4f53\u7cfb\u4e0d\u4ec5\u662f\u5de5\u7a0b\u5b9e\u73b0\u7684\u4f18\u5316\u624b\u6bb5\uff0c\u66f4\u662f\u63d0\u5347\u6a21\u578b\u7406\u89e3\u4e0e\u5206\u6790\u6df1\u5ea6\u7684\u7406\u8bba\u5fc5\u9700\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#5","title":"5. \u4eba\u7c7b\u8bc4\u5206\u4e0e\u81ea\u52a8\u5316\u8bc4\u4f30\u4e00\u81f4\u6027\u5206\u6790","text":"<p>\u4e3a\u4e86\u5e2e\u52a9\u7814\u7a76\u8005\u5728\u591a\u4e2a\u7ef4\u5ea6\u95f4\u9009\u62e9\u6700\u5408\u9002\u7684 AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0cAIGVE-Bench \u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u5404\u8bc4\u4ef7\u7ef4\u5ea6\u4e0b\u7684\u4eba\u7c7b\u8bc4\u5206\u4e0e\u5404 AIGV \u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u7ed3\u679c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6211\u4eec\u4ee5 Spearman \u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\uff08SRCC\uff09\u4e3a\u8861\u91cf\u6307\u6807\uff0c\u5206\u522b\u8ba1\u7b97\u4e86\u4ee5\u4e0b\u4e09\u79cd\u76f8\u5173\u6027\u8868\u73b0\uff1a</p> <ul> <li> <p>SRCC\u208drand\u208e\uff1a\u968f\u673a\u751f\u6210\u7684\u5206\u6570\u4e0e\u4eba\u5de5\u6807\u7b7e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff08\u7528\u4e8e\u5efa\u7acb\u65e0\u4fe1\u606f\u57fa\u7ebf\uff09\uff1b</p> </li> <li> <p>SRCC\uff1a\u5355\u4e00\u81ea\u52a8\u5316 AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6807\u7b7e\u7684\u76f8\u5173\u6027\uff1b</p> </li> <li> <p>SRCC\u208dreg\u208e\uff1a\u901a\u8fc7\u7ebf\u6027\u56de\u5f52\u878d\u5408\u591a\u79cd AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u540e\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u5de5\u6807\u7b7e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002</p> </li> </ul> <p>\u4e0b\u8868\u5217\u51fa\u4e86\u6bcf\u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u4e0b\u63a8\u8350\u7684\u6700\u4f18\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u6027\u6307\u6807\u7ed3\u679c\uff1a </p> <p>\u4ece\u7ed3\u679c\u6765\u770b\uff0c\u90e8\u5206\u5355\u4e00\u65b9\u6cd5\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u4e00\u81f4\u6027\uff0c\u800c\u591a\u65b9\u6cd5\u878d\u5408\u7b56\u7565\uff08SRCC\u208dreg\u208e\uff09\u901a\u5e38\u5728\u6574\u4f53\u7ef4\u5ea6\u4e0a\u5177\u6709\u66f4\u5f3a\u7684\u62df\u5408\u80fd\u529b\u3002 \u8fd9\u4e00\u53d1\u73b0\u9996\u6b21\u7cfb\u7edf\u6027\u9a8c\u8bc1\u4e86\u591a\u7ef4\u878d\u5408\u7b56\u7565\u5728\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684 AIGV \u81ea\u52a8\u8bc4\u4f30\u4f53\u7cfb\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_4","title":"\u603b\u7ed3\u4e0e\u5c55\u671b","text":"<p>AIGVE-Tool \u662f\u9996\u4e2a\u8986\u76d6\u4e3b\u6d41 AIGV \u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3001\u652f\u6301\u81ea\u5b9a\u4e49\u6269\u5c55\u3001\u642d\u914d\u9ad8\u8d28\u91cf\u591a\u7ef4\u57fa\u51c6\u6570\u636e\u7684\u5b8c\u6574\u5de5\u5177\u5305\u3002\u5176\u6838\u5fc3\u8d21\u732e\u4f53\u73b0\u5728\u4ee5\u4e0b\u4e09\u65b9\u9762\uff1a</p> <ul> <li> <p>\u5b8c\u5584\u7406\u8bba\uff1a\u63d0\u51fa\u4e86\u9996\u4e2a AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u957f\u671f\u7f3a\u4e4f\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u7684\u7a7a\u767d\uff1b</p> </li> <li> <p>\u7edf\u4e00\u67b6\u6784\uff1a\u6784\u5efa\u4e86\u57fa\u4e8e\u914d\u7f6e\u9a71\u52a8\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u89e3\u8026\u4e0e\u7075\u6d3b\u7ec4\u5408\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u4efb\u52a1\u7684\u53ef\u590d\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\uff1b</p> </li> <li> <p>\u6784\u5efa\u6570\u636e\uff1a\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u591a\u7ef4\u5ea6\u4e3b\u89c2\u8bc4\u4ef7\u57fa\u51c6 AIGVE-Bench\uff0c\u9996\u6b21\u5b9e\u73b0\u4e3b\u89c2\u8bc4\u5206\u4e0e\u4e3b\u6d41\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u4e4b\u95f4\u7684\u7cfb\u7edf\u5bf9\u9f50\u4e0e\u91cf\u5316\u5206\u6790\uff0c\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u9a8c\u8bc1\u63d0\u4f9b\u53ef\u9760\u652f\u6491\u3002</p> </li> </ul> <p>\u6211\u4eec\u671f\u671b AIGVE-Tool \u80fd\u591f\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5feb\u901f\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u4e5f\u4e3a\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u7edf\u4e00\u3001\u516c\u5e73\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u52a9\u529b AIGV \u7814\u7a76\u8fc8\u5411\u66f4\u9ad8\u5c42\u6b21\u7684\u7cfb\u7edf\u5316\u4e0e\u6807\u51c6\u5316\u3002</p> <p>\u5c55\u671b\u672a\u6765\uff0c\u6211\u4eec\u5c06\u6301\u7eed\u8fed\u4ee3\u66f4\u65b0\u96c6\u6210\u66f4\u591a AIGV \u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u8bc4\u4ef7\u7ef4\u5ea6\uff0c\u63a8\u52a8\u6784\u5efa\u66f4\u5177\u666e\u9002\u6027\u4e0e\u8986\u76d6\u6027\u7684 AIGV \u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb\u3002\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u8bda\u631a\u6b22\u8fce\u5e7f\u5927\u7814\u7a76\u8005\u3001\u5de5\u7a0b\u5e08\u3001\u5f00\u53d1\u8005\u4e0e\u5b66\u751f\u52a0\u5165\uff0c\u5171\u5efa\u5171\u4eab AIGVE \u751f\u6001\uff0c\u5171\u540c\u63a8\u52a8\u591a\u6a21\u6001\u751f\u6210\u8d28\u91cf\u8bc4\u4f30\u8fc8\u5411\u65b0\u9636\u6bb5\uff01</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#aigve","title":"<code>aigve</code>\u5de5\u5177\u5305\u4e0e\u9879\u76ee\u7f51\u7ad9\u4ecb\u7ecd","text":"<p>\u4e3a\u4e86\u4fbf\u4e8e\u7528\u6237\u5feb\u901f\u4e0a\u624b\u4f7f\u7528 AIGVE-Tool\uff0c\u6211\u4eec\u540c\u6b65\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u4ed3\u5e93\u3001\u5b89\u88c5\u5305\u53d1\u5e03\u5e73\u53f0\uff0c\u4ee5\u53ca\u914d\u5957\u7684\u9879\u76ee\u5b98\u7f51\u4e0e\u4f7f\u7528\u6587\u6863\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#pypi","title":"\u6e90\u7801\u4ed3\u5e93\u4e0e PyPI \u5b89\u88c5\u5730\u5740","text":"<p><code>aigve</code> \u5de5\u5177\u5305\u5df2\u7ecf\u5728 GitHub \u4e0e PyPI \u5e73\u53f0\u4e0a\u7ebf\uff0c\u652f\u6301\u4e00\u952e\u5b89\u88c5\u4e0e\u672c\u5730\u90e8\u7f72\uff1a</p> <ul> <li>Github Repository: https://github.com/ShaneXiangH/AIGVE_Tool , \u5305\u542b\u5b8c\u6574\u7684\u6846\u67b6\u6e90\u7801\u3001\u8bc4\u4f30\u65b9\u6cd5\u5b9e\u73b0\u3001\u914d\u7f6e\u793a\u4f8b\u4e0e\u6570\u636e\u9884\u5904\u7406\u811a\u672c\u3002</li> <li>PyPI Package: https://pypi.org/project/aigve/ , \u652f\u6301\u4f7f\u7528 <code>pip install aigve</code> \u5feb\u901f\u5b89\u88c5\u3002</li> </ul> <p>\u6211\u4eec\u5efa\u8bae\u5f00\u53d1\u8005\u901a\u8fc7 GitHub \u83b7\u53d6\u6700\u65b0\u6e90\u7801\u7248\u672c\uff0c\u65b9\u4fbf\u67e5\u9605\u8bc4\u4f30\u65b9\u6cd5\u5b9e\u73b0\u4e0e\u63d0\u4ea4 Issue\u3002</p>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_5","title":"\u5b98\u65b9\u9879\u76ee\u7f51\u7ad9","text":"<p>\u4e3a\u4e86\u63d0\u4f9b\u66f4\u53cb\u597d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0c\u6211\u4eec\u4e13\u95e8\u642d\u5efa\u4e86 AIGVE \u9879\u76ee\u5b98\u7f51\uff0c\u7f51\u7ad9\u4e2d\u5305\u542b\u4e86\u5b8c\u6574\u7684\u7ec4\u4ef6\u8bf4\u660e\u3001\u914d\u7f6e\u793a\u4f8b\u3001\u5e38\u89c1\u95ee\u9898\u7b54\u7591\u4e0e\u4f7f\u7528\u6559\u7a0b\u7b49\u5185\u5bb9\u3002\u7f51\u7ad9\u94fe\u63a5\u548c\u90e8\u5206\u7f51\u7ad9\u9875\u9762\u5982\u4e0b\u6240\u793a\uff1a</p> <ul> <li>Official Website: https://www.aigve.org/</li> </ul>"},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_6","title":"\u7f51\u7ad9\u4e3b\u9875","text":""},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#_7","title":"\u6587\u6863\u76ee\u5f55","text":""},{"location":"tutorials/%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/#tutorial","title":"Tutorial \u5217\u8868","text":"<p>\u5982\u6709\u95ee\u9898\uff0c\u6b22\u8fce\u901a\u8fc7 GitHub Issue \u7559\u8a00\u4ea4\u6d41\uff0c\u6216\u53c2\u4e0e\u8d21\u732e\u66f4\u591a\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u6570\u636e\u96c6\u652f\u6301\u3002\u6211\u4eec\u5c06\u6301\u7eed\u8fed\u4ee3\u6587\u6863\u4e0e\u529f\u80fd\uff0c\u63d0\u5347 AIGV \u8d28\u91cf\u8bc4\u4f30\u7684\u5de5\u7a0b\u4fbf\u5229\u6027\u4e0e\u793e\u533a\u53cb\u597d\u5ea6\u3002</p>"}]}